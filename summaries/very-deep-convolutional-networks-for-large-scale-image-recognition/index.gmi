# A summary of Very Deept Cnvolutional Networks for Large-Scale Image Recognition by Karen Simonyan and Andrew Zisserman

Wednesday, September 28, 2022 Â· 3 minute read


# A summary of *Very Deept Cnvolutional Networks for Large-Scale Image Recognition* by Karen Simonyan and Andrew Zisserman

> Author names
> [https://doi.org/10.48550/arXiv.1409.1556](https://doi.org/10.48550/arXiv.1409.1556)

For the summary of the paper, go to the [Summary](#summary) section of this
article.

## Table of Contents

- [A summary of *Very Deept Cnvolutional Networks for Large-Scale Image Recognition* by Karen Simonyan and Andrew Zisserman](#a-summary-of-very-deept-cnvolutional-networks-for-large-scale-image-recognition-by-karen-simonyan-and-andrew-zisserman)
  - [Table of Contents](#table-of-contents)
  - [First Pass](#first-pass)
    - [Category](#category)
    - [Context](#context)
    - [Contributions](#contributions)
  - [Second Pass](#second-pass)
  - [Background Work](#background-work)
  - [Motivation](#motivation)
    - [Figures, Diagrams, Illustrations, and Graphs](#figures-diagrams-illustrations-and-graphs)
    - [Clarity](#clarity)
    - [Relevant Work](#relevant-work)
  - [Third Pass](#third-pass)
    - [Author Assumptions](#author-assumptions)
    - [Correctness](#correctness)
    - [Discussion of the Proofs](#discussion-of-the-proofs)
    - [How Would I Present the Idea(s)](#how-would-i-present-the-ideas)
    - [Future Directions](#future-directions)
  - [Summary](#summary)
  - [Summarization Technique](#summarization-technique)
  - [Citations](#citations)

______________________________________________________________________

## First Pass

> Discussion about the title, abstract, introduction, section and sub-section
> headings, and conclusion

The paper *Very Deep Convolutional Networks for Large Scale Image Recognition*
by Karen Simonyan and ndrew Zissernman discusses the SOTA performance of their
model in the 2014 ImageNet Challenge on localisation and classification tasks.
They discuss that be extending the depth of convolutional neural networks to 16
up to 19 layers, with a 3x3 filter size, SOTA performance is possible without
redeveloping the architecture of existing convolutional neural networks. This is
in contrast to [Szegedy's work](going-deeper-with-convolutions.md) who proposes
the Inception architecture for classification and object detection; with which
the reference implementation also came first in the 2014 ImageNet Challenge in
its respective tasks. Simoyan et al. discuss the architecture and training that
went into their model (VGG) and how to architect future models to perform as
well or better.

### Category

> What type of paper is this work?

This is both a computer vision model evaluation and architecture paper.

### Context

> What other *types* of papers is the work related to?

This paper is most closely related to others who publish work regarding SOTA
performance on CV architecture and models.

### Contributions

> What are the author's main contributions?

Their main contributions is an exploration of depth in traditional convulational
neural networks to achieve SOTA performance.

______________________________________________________________________

## Second Pass

## Background Work

> What has been done prior to this paper?

Prior work has gone into optimizing the width and intial convulations of
convultional neural networks.

[Szegedy et al.](going-deeper-with-convolutions.md) proposed a new architecture
(Inception) that achieved SOTA performance in the 2014 ImageNet Challenge. Else,
Krizhevsky et al. \[2\] and others have proposed improvments to the
convulational neural network architecture.

## Motivation

> Why should we care about this paper?

We should care about the authors work as increasing the depth of a neural
network by their proposed architecture allows for easy expansion of existing
convulational neural networks without redesigning the libraries used to create
them.

### Figures, Diagrams, Illustrations, and Graphs

> Are the axes properly labeled? Are results shown with error bars, so that
> conclusions are statistically significant?

The tables that are presented are easy to read, but can be improved upon. Often,
multiple rows will correspond with a single model configuration. This is fine,
however, it is difficult to make out what configuration each row corresponds to.

### Clarity

> Is the paper well written?

### Relevant Work

> Mark relevant work for review

The following relevant work can be found in the [Citations](#citations) section
of this article.

______________________________________________________________________

## Third Pass

> This section can only be complete after a virtual re-implementation of the
> paper

### Author Assumptions

> What assumptions does the author(s) make? Are they justified assumptions?

### Correctness

> Do the assumptions seem valid?

### Discussion of the Proofs

### How Would I Present the Idea(s)

### Future Directions

> My own proposed future directions for the work

______________________________________________________________________

## Summary

> A summary of the paper

______________________________________________________________________

## Summarization Technique

This paper was summarized using the technique proposed by S. Keshav in his work
*How to Read a Paper* \[0\].

## Citations


=> https://doi.org/10.1145/1273445.1273458	0. https://doi.org/10.1145/1273445.1273458

=> https://doi.org/10.48550/arXiv.1409.1556	1. https://doi.org/10.48550/arXiv.1409.1556

=> https://doi.org/10.1145/3065386	2. https://doi.org/10.1145/3065386

