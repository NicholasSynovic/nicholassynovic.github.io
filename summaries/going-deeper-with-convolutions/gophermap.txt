!A summary of Going deeper with convolutions by Christian Szegedy etal.

Wednesday, September 28, 2022 Â· 8 minute read

A summary of Going deeper with convolutions Christian Szegedy etal; INSERT DOI
For the summary of the paper, go to the Summary section of this article.
Table of Contents A summary of Going deeper with convolutions Table of Contents First Pass Category Context Contributions Second Pass Figures, Diagrams, Illustrations, and Graphs Clarity Relevant Work Third Pass Author Assumptions Correctness Future Directions Summary Summarization Technique Citations First Pass Discussion about the title, abstract, introduction, section and sub-section headings, and conclusion
The paper Going deeper with convolutions by Christian Szegedy etal [1] describes a 2014 state of the art computer vision model (on the ImageNet Large-Scale Visual Recognition Challenge) called GoogLeNet architected based on Hebbian principles (i.e. neruons that fire together, are wired together)and a constant computational budget. Their approach relies on creative algorithms and neuroscience principles and aims to be a more power effiecient model for mobile devices by limiting the computations during infrencing. Additionally, their model is deep but not wide and is considered &ldquo;sparse&rdquo; by the authors. In other words, there are as few nodes as possible within the neural network.
Szegedy etal&rsquo;s contributions are a state of the art computer vision model that provides experimental evidence that, &ldquo;&hellip; Approximating the expected optimal sparse structure by readily avilible dense building blocks is a viable method for improving neural networks for computer vision&rdquo;. This means that this model proves that dense neural networks for computer vision are not necessary in the author&rsquo;s viewpoint.
Category What type of paper is this work?
This is a computer vision paper describing both a machine learning architecture and refernce model.
Context What other types of papers is the work related to?
This paper is related to other computer vision papers that achieve state of the art performance values based on the ImageNet Large-Scale Visual Recognition Challenge.
Contributions What are the author&rsquo;s main contributions?
Szegedy etal&rsquo;s contributions are:
A computer vision model architecture (Inception) that is both sparse and aims to be computationally efficient on mobile (non-server) devices, A reference model of the aforementioned computer vision model architecture A comparison of previous state of the art work to justify their claims that sparser networks are the future of computer vision models. Second Pass Figures, Diagrams, Illustrations, and Graphs Are the axes properly labeled? Are results shown with error bars, so that conclusions are statistically significant?
All of the tables have proper column labels. However, Table 1 does not provide default values for blank cells. This is most likely due to the layer type not performaing a specific operation (as described in the column label). Regardless, the remaining tables look good.
Additionally, Figure 3 is very clear to read, if a little dense. However, as it describes all of the layers of GoogLeNet and how they are connected, I find the size to be appropriate.
Clarity Is the paper well written?
The paper is fairly well written. The only complaints that I have are minor grammatical mistakes that the author&rsquo;s left in (by accident I assume). Additionally, that the authors didn&rsquo;t optimize their tables and figures to better fit on the pages. As tables and figures are stacked on top of one another, it would be possible to reclaim paper space by rearranging multiple tables and figures to be next to one another, with the exception of Figure 3 due to the sheer size of it.
Relevant Work Mark relevant work for review
The following relevant work can be found in the Citations section of this article.
Contrast Normalization Max Pooling Average Pooling Softmax Activation Dropout - [2] Localization Task - [3] Gabor Filters - [4] Network in Network - [5] Rectified Linear Activation - [6] Regions with Convolutional Neural Networks - [7] Multi-Box Prediction - [8] Arora proof - [9] LeNet 5 - [10] Fisher vectors Polyak Averaging - [11] Jaccard index Selective Search - [12] Photometric Distortions - [13] Third Pass This section can only be complete after a virtual re-implementation of the paper
Author Assumptions What assumptions does the author(s) make? Are they justified assumptions?
One assumption that the authors make is that overfitting is more prone to occur in large models. Additionally, overfitting can occur when there is not enough labeled examples in a dataset when a large model is training. Furthermore, increasing the size of a model increases the number of computations that must be done between layers (e.g. chaining two convulational layers results in computation cost quadratically increasing) Their solutions relies on moving from fully connected to sparsely connected architectures including within convolutional layers. Also, their model architecutre is based on the idea that computers are inefficent when, &ldquo;&hellip; Computing numerical calculations on non-uniform sparse data structures&rdquo;.
They assume that 1x1, 3x3, and 5x5 filters are the proper filters to use, but did not test other size of filters. They also assume that using, &ldquo;Inception modules&rdquo; is only useful at higher levels, whereas the initial levels are standard convultational levels. However, this was not tested either and was due to, &ldquo;infrastructural inefficiences&rdquo; in the implementation.
Finally, that the model that achieved state of the art performance was the best model. The authors had been training and testing other models for months prior, however, it is unclear what the testing methodology was and why a particular model was choosen to compete in the ImageNet competition.
Correctness Do the assumptions seem valid?
The first paragraph of assumptions seems reasonable and correct. However, the remaining two paragraphs seem unreasonable. This is due to the lack of testing that the author&rsquo;s put in when optimizing their model with respect to the filter sizes and choosing models. Furthermore, if testing did occur to address these issues, it is not addressed in this paper, thus leaving the reader to wonder why testing wasn&rsquo;t performed.
Future Directions My own proposed future directions for the work
Based off of Section 3 (Motiviation and High Level Considerations), one promising area of study would be to perform a network architecture search utilizing the principles and reasoning of their approach to other machine learning and computer vision domains.
An enhancement to their work is possible by analyzing what filter sizes most optimal improve performance. Currently the author&rsquo;s are restricting GoogLeNet to 1x1, 3x3, and 5x5 filter sizes, but this was due to convience and no data was given to support this.
Summary A summary of the paper
The paper Going deeper with convolutions by Szegedy etal [1] introduces a computer vision model architecture called Inception and a reference model called GoogLeNet.
Inception is a model architecuture that is both sparse and (attempts to be) computationally efficent during inferencing with only 1.5 billion multiply-add operations allowed. Inception models are composed of multiple Inception modules that are stacked on top of each other. Each Inception module takes in data from the previous layer and passes it into small convultional filters (i.e. 1x1 typically). There are three of these small filters that are wired to the input of the Inception module, with one of them connected directly to the output. The outputs of two of these filters are then passed into larger filters (i.e. 5x5) to which it is then passed into a DepthConcat function. Additionally, a 3x3 filter is wired to the input of the module and the output of which goes into a 1x1 filter to be passed into the DepthConcat function as well. From there, it is passed into another Inception module and the process repeats.
Note: Depth when referring to two dimensional images refers to the color channel of the image. As images typically have three color channels (i.e. red, green, blue), an image would have a depth of 3.
Example: A 200 pixel by 200 pixel full color spectrum image would be represented as 200x200x3.
It is possible for Inception modules to have an additional connection to the input of the module to perform average pooling for softmax activation.
GoogLeNet achieved SOTA performance in the ImageNet Large-Scale Visual Recognition Challenge image classification task by having a top-5 error of 6.67% on both the validation and testing data. This is an improvment of 56.5% in comparison to 2012&rsquo;s SOTA performer (SuperVision) and 2013&rsquo;s SOTA performer (Clarifai). Additionally, they achieved SOTA perfomance for the ImageNet Large-Scale Visual Recognition Challenge detection task with a mean average precision of 43.9% utilizing an ensemble inference approach. This model was architected using the Inception architecture with 22 layers. However, not every layer was an Inception module; the first few layers were standard convulational layers.
The author&rsquo;s contributions were as follows;
The Inception computer vision architecture, The GoogLeNet SOTA computer vision model for classification and object detection. My opinion on this paper is that while it is well written, the author&rsquo;s make numerous assumptions about the optimal performance of their model&rsquo;s architecture. They don&rsquo;t test optimal sizes for filters as well as resolving bugs such as the usage of standard convulational layers early in the model. Both of which can be solved by performing a neural architecture search.
Future work for this paper would involve optimizing the model architecture via a neural architecture search. As well as evaluating the performance of the model by both increasing and decreasing the depth of the model.
Summarization Technique This paper was summarized using the technique proposed by S. Keshav in his work How to Read a Paper [0].
Citations 

h0. https://doi.org/10.1145/1273445.1273458	URL:https://doi.org/10.1145/1273445.1273458

h1. https://doi.org/10.48550/arXiv.1409.4842	URL:https://doi.org/10.48550/arXiv.1409.4842

h2. https://doi.org/10.48550/arXiv.1207.0580	URL:https://doi.org/10.48550/arXiv.1207.0580

h3. https://doi.org/10.48550/arXiv.1312.6229	URL:https://doi.org/10.48550/arXiv.1312.6229

h4. https://doi.org/10.1109/TPAMI.2007.56	URL:https://doi.org/10.1109/TPAMI.2007.56

h5. https://doi.org/10.48550/arXiv.1312.4400	URL:https://doi.org/10.48550/arXiv.1312.4400

h6. https://doi.org/10.1145/3065386	URL:https://doi.org/10.1145/3065386

h7. https://openaccess.thecvf.com/content_cvpr_2014/html/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html	URL:https://openaccess.thecvf.com/content_cvpr_2014/html/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html

h8. https://openaccess.thecvf.com/content_cvpr_2014/html/Erhan_Scalable_Object_Detection_2014_CVPR_paper.html	URL:https://openaccess.thecvf.com/content_cvpr_2014/html/Erhan_Scalable_Object_Detection_2014_CVPR_paper.html

h9. http://proceedings.mlr.press/v32/arora14.pdf	URL:http://proceedings.mlr.press/v32/arora14.pdf

h10. https://doi.org/10.1162/neco.1989.1.4.541	URL:https://doi.org/10.1162/neco.1989.1.4.541

h11. https://doi.org/10.1137/0330046	URL:https://doi.org/10.1137/0330046

h12. https://doi.org/10.1109/ICCV.2011.6126456	URL:https://doi.org/10.1109/ICCV.2011.6126456

h13. https://doi.org/10.48550/arXiv.1312.5402	URL:https://doi.org/10.48550/arXiv.1312.5402

.
