:blogpost: true
:date: August 20, 2024
:category: Paper
:tags: ESE, ESE 29, 2024, Case Study, 08-20-2024
:nocomments:

##################################################################################################
 *Challenges and practices of deep learning model reengineering: A case study on computer vision*
##################################################################################################

:bdg-primary:`Journal Article` :bdg-primary-line:`ESE 2024`
:bdg-primary-line:`Case Study`

**********
 Authors
**********

.. grid:: 3

   .. grid-item-card:: Wenxin Jiang
      :text-align: center

   .. grid-item-card:: Vishnu Banna
      :text-align: center

   .. grid-item-card:: Rohan Sethi
      :text-align: centerProcess Metrics

   .. grid-item-card:: Naveen Vivek
      :margin: 3 0 0 0
      :text-align: center

   .. grid-item-card:: Abhinav Goel
      :text-align: center
      :margin: 3 0 0 0

   .. grid-item-card:: Nicholas M. Synovic
      :text-align: center
      :margin: 3 0 0 0

   .. grid-item-card:: George K. Thiruvathukal
      :text-align: center
      :margin: 3 0 0 0

   .. grid-item-card:: James C. Davis
      :text-align: center
      :margin: 3 0 0 0

**********
 Abstract
**********

Context: Many engineering organizations are reimplementing and extending deep
neural networks from the research community. We describe this process as deep
learning model reengineering. Deep learning model reengineering — reusing,
replicating, adapting, and enhancing state-of-the-art deep learning approaches —
is challenging for reasons including under-documented reference models, changing
requirements, and the cost of implementation and testing.

Objective: Prior work has characterized the challenges of deep learning model
development, but as yet we know little about the deep learning model
reengineering process and its common challenges. Prior work has examined DL
systems from a “product” view, examining defects from projects regardless of the
engineers' purpose. Our study is focused on reengineering activities from a
"process" view, and focuses on engineers specifically engaged in the
reengineering process.

Method: Our goal is to understand the characteristics and challenges of deep
learning model reengineering. We conducted a mixed-methods case study of this
phenomenon, focusing on the context of computer vision. Our results draw from
two data sources: defects reported in open-source reeengineering projects, and
interviews conducted with practitioners and the leaders of a reengineering team.
From the defect data source, we analyzed 348 defects from 27 open-source deep
learning projects. Meanwhile, our reengineering team replicated 7 deep learning
models over two years; we interviewed 2 open-source contributors, 4
practitioners, and 6 reengineering team leaders to understand their experiences.

Results: Our results describe how deep learning-based computer vision techniques
are reengineered, quantitatively analyze the distribution of defects in this
process, and qualitatively discuss challenges and practices. We found that most
defects (58%) are reported by re-users, and that reproducibility-related defects
tend to be discovered during training (68% of them are). Our analysis shows that
most environment defects (88%) are interface defects, and most environment
defects (46%) are caused by API defects. We found that training defects have
diverse symptoms and root causes. We identified four main challenges in the DL
reengineering process: model operationalization, performance debugging,
portability of DL operations, and customized data pipeline. Integrating our
quantitative and qualitative data, we propose a novel reengineering workflow.

Conclusions: Our findings inform several conclusion, including: standardizing
model reengineering practices, developing validation tools to support model
reengineering, automated support beyond manual model reengineering, and
measuring additional unknown aspects of model reengineering.

***********
 Artifacts
***********

.. todo::

   - Add the paper preprint
   - Add the poster
   - Add link to the source code
   - Update the bibtex

.. grid:: 2

   .. grid-item-card:: Paper Preprint
      :text-align: center

      .. button-link:: blog_posts/index.html
         :click-parent:
         :color: primary
         :expand:

         Download

   .. grid-item-card:: Published Paper
      :text-align: center

      .. button-link:: https://doi.org/10.1145/3551349.3559517
         :click-parent:
         :color: primary
         :expand:

         View

   .. grid-item-card:: Poster
      :text-align: center
      :margin: 3 0 0 0

      .. button-link:: https://doi.org/10.1145/3551349.3559517
         :click-parent:
         :color: primary
         :expand:

         Download

   .. grid-item-card:: Source Code
      :text-align: center
      :margin: 3 0 0 0

      .. button-link:: https://doi.org/10.1145/3551349.3559517
         :click-parent:
         :color: primary
         :expand:

         View

   .. grid-item-card::
      :margin: 3 0 0 0
      :text-align: left
      :columns: 12

      .. dropdown:: BibTex

         .. code:: BibTex

            @inproceedings{synovic_snapshot_2023,
               address = {New York, NY, USA},
               series = {{ASE} '22},
               title = {Snapshot {Metrics} {Are} {Not} {Enough}: {Analyzing} {Software} {Repositories} with {Longitudinal} {Metrics}},
               isbn = {978-1-4503-9475-8},
               shorttitle = {Snapshot {Metrics} {Are} {Not} {Enough}},
               url = {https://dl.acm.org/doi/10.1145/3551349.3559517},
               doi = {10.1145/3551349.3559517},
               abstract = {Software metrics capture information about software development processes and products. These metrics support decision-making, e.g., in team management or dependency selection. However, existing metrics tools measure only a snapshot of a software project. Little attention has been given to enabling engineers to reason about metric trends over time—longitudinal metrics that give insight about process, not just product. In this work, we present PRIME (PRocess MEtrics), a tool to compute and visualize process metrics. The currently-supported metrics include productivity, issue density, issue spoilage, and bus factor. We illustrate the value of longitudinal data and conclude with a research agenda. The tool’s demo video can be watched at https://bit.ly/ase2022-prime. Source code can be found at https://github.com/SoftwareSystemsLaboratory/prime.},
               urldate = {2023-09-06},
               booktitle = {Proceedings of the 37th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
               publisher = {Association for Computing Machinery},
               author = {Synovic, Nicholas M. and Hyatt, Matt and Sethi, Rohan and Thota, Sohini and Shilpika and Miller, Allan J. and Jiang, Wenxin and Amobi, Emmanuel S. and Pinderski, Austin and Läufer, Konstantin and Hayward, Nicholas J. and Klingensmith, Neil and Davis, James C. and Thiruvathukal, George K.},
               month = jan,
               year = {2023},
               keywords = {Empirical software engineering, Software metrics},
               pages = {1--4},
            }

.. note::

   I have continued to iterate upon the ``PRIME`` tool in a new repository. If
   want to learn more, visit `<https://github.com/NicholasSynovic/prime>`_.

*******
 Video
*******

.. youtube:: YigEHy3_JCo
