<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Nicholas M. Synovic</title><link>https://nsynovic.dev/</link><description>Recent content on Nicholas M. Synovic</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Nicholas M. Synovic</copyright><lastBuildDate>Sun, 26 Jun 2022 21:04:02 -0500</lastBuildDate><atom:link href="https://nsynovic.dev/feed.xml" rel="self" type="application/rss+xml"/><item><title>A summary of ImageNet Classification with Deep Convolutional Neural Networks by Krizhevsky et al.</title><link>https://nsynovic.dev/summaries/imagenet-classification-with-deep-convolutional-neural-networks/</link><pubDate>Thu, 29 Sep 2022 14:33:01 -0500</pubDate><guid>https://nsynovic.dev/summaries/imagenet-classification-with-deep-convolutional-neural-networks/</guid><description>&lt;h1 id="a-summary-of-imagenet-classification-with-deep-convolutional-neural-networks">A summary of &lt;em>ImageNet Classification with Deep Convolutional Neural Networks&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Krizhevsky et al.;
&lt;a href="https://doi.org/10.1145/3065386">https://doi.org/10.1145/3065386&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-imagenet-classification-with-deep-convolutional-neural-networks">A summary of &lt;em>ImageNet Classification with Deep Convolutional Neural Networks&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>&lt;/li>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#third-pass">Third Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;li>&lt;a href="#discussion-of-the-proofs">Discussion of the Proofs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#how-would-i-present-the-ideas">How Would I Present the Idea(s)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>ImageNet Classification with Deep Convolutional Neural Networks&lt;/em> by
Krizhevsky et al. discusses the AlexNet model and its architecture as well as
its SOTA achievements in the 2012 ImageNet Challenge. The difference between
AlexNet and other contestants was that the model relies on GPU training to train
the convulational neural network model. By utilizing the GPU, training time can
be accelerated significatnly more than what was previously possible. Their major
contributions is that a large, deep convulational neural network is capable of
achieving record-breaking resuls via supervised learning. They did not utilize
unsupervised pre-training, but the authors suspect that it would improve the
accuracy of the model.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is a computer vision model evaluation and architecture paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is similar to others that have published about SOTA results from the
ImageNet Challenge.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contributions were that training on GPUs allows for accelerated
training, that large and deep convulutional neural networks are effective at
clasifying images, and that removing layers does decrease the performance of
models. Therefore, a larger, deeper model is applicable.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h2 id="background-work">Background Work&lt;/h2>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Previous work on designing convulational neural networks and architectures.
However, they were bounded by not being particularly deep.&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Because it is one of the key papers that demonstrates that large, deep,
convulational neural networks are effective for image classification. As well as
providing evidence that training on GPUs is not only effective but recommended
for optimal performance.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>Nearly all of the figures are designed well, with the exception of Figure 2.
Figure 2 is the model architecture of AlexNet. This figure suffers from
information density and a three dimensional design which makes it hard to
determin what is going on and in what dimension are images being manipulated.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;hr>
&lt;h2 id="third-pass">Third Pass&lt;/h2>
&lt;blockquote>
&lt;p>This section can only be complete after a virtual re-implementation of the
paper&lt;/p>
&lt;/blockquote>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;h3 id="correctness">Correctness&lt;/h3>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;h3 id="discussion-of-the-proofs">Discussion of the Proofs&lt;/h3>
&lt;h3 id="how-would-i-present-the-ideas">How Would I Present the Idea(s)&lt;/h3>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using the technique proposed by S. Keshav in his work
&lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/imagenet-classification-with-deep-convolutional-neural-networks/</description></item><item><title>A summary of Very Deept Cnvolutional Networks for Large-Scale Image Recognition by Karen Simonyan and Andrew Zisserman</title><link>https://nsynovic.dev/summaries/very-deep-convolutional-networks-for-large-scale-image-recognition/</link><pubDate>Wed, 28 Sep 2022 22:40:46 -0500</pubDate><guid>https://nsynovic.dev/summaries/very-deep-convolutional-networks-for-large-scale-image-recognition/</guid><description>&lt;h1 id="a-summary-of-very-deep-convolutional-networks-for-large-scale-image-recognition-by-karen-simonyan-and-andrew-zisserman">A summary of &lt;em>Very Deep Convolutional Networks for Large-Scale Image Recognition&lt;/em> by Karen Simonyan and Andrew Zisserman&lt;/h1>
&lt;blockquote>
&lt;p>Karen Simonyan and Andrew Zisserman;
&lt;a href="https://doi.org/10.48550/arXiv.1409.1556">https://doi.org/10.48550/arXiv.1409.1556&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-very-deept-cnvolutional-networks-for-large-scale-image-recognition-by-karen-simonyan-and-andrew-zisserman">A summary of &lt;em>Very Deep Convolutional Networks for Large-Scale Image Recognition&lt;/em> by Karen Simonyan and Andrew Zisserman&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>&lt;/li>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#third-pass">Third Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Very Deep Convolutional Networks for Large Scale Image Recognition&lt;/em>
by Karen Simonyan and Andrew Zissernman discusses the SOTA performance of their
model in the 2014 ImageNet Challenge on localisation and classification tasks.
They discuss that be extending the depth of convolutional neural networks to 16
up to 19 layers, with a 3x3 filter size, SOTA performance is possible without
redeveloping the architecture of existing convolutional neural networks. This is
in contrast to &lt;a href="going-deeper-with-convolutions.md">Szegedy&amp;rsquo;s work&lt;/a> who proposes
the Inception architecture for classification and object detection; with which
the reference implementation also came first in the 2014 ImageNet Challenge in
its respective tasks. Simoyan et al. discuss the architecture and training that
went into their model (VGG) and how to architect future models to perform as
well or better.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is both a computer vision model evaluation and architecture paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is most closely related to others who publish work regarding SOTA
performance on CV architecture and models.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contributions is an exploration of depth in traditional convulational
neural networks to achieve SOTA performance.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h2 id="background-work">Background Work&lt;/h2>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Prior work has gone into optimizing the width and intial convulations of
convultional neural networks.&lt;/p>
&lt;p>&lt;a href="going-deeper-with-convolutions.md">Szegedy et al.&lt;/a> proposed a new architecture
(Inception) that achieved SOTA performance in the 2014 ImageNet Challenge. Else,
Krizhevsky et al. [2] and others have proposed improvments to the
convulational neural network architecture.&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about the authors work as increasing the depth of a neural
network by their proposed architecture allows for easy expansion of existing
convulational neural networks without redesigning the libraries used to create
them.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The tables that are presented are easy to read, but can be improved upon. Often,
multiple rows will correspond with a single model configuration. This is fine,
however, it is difficult to make out what configuration each row corresponds to.
Additionally, the tables make comparing error percentages easy across model
configurations.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is well written and can be understood.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>Classical convultional neural network architecture - [3]&lt;/li>
&lt;li>GoogLeNet - [2]&lt;/li>
&lt;li>Clarifai&lt;/li>
&lt;li>ImageNet classification with deep convolutional neural net- works [4]&lt;/li>
&lt;li>Isotropically-rescaled training image&lt;/li>
&lt;li>ImageNet 2013 submissions - [5], [6] Localization and Detection using
Convolutional Networks&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="third-pass">Third Pass&lt;/h2>
&lt;blockquote>
&lt;p>This section can only be complete after a virtual re-implementation of the
paper&lt;/p>
&lt;/blockquote>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors assume that the performance improvements that convulational neural
networks are achieving are based off of larger datasets and better compute
optimization.&lt;/p>
&lt;h3 id="correctness">Correctness&lt;/h3>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>I agree with their assumption. However, [2] created a SOTA model utilizing a
new architecture, rather than improving upon an existing one.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I would love to try and optimize the input layer of convulational neural
networks by having a computation that not only looks at the color space, but
also the opacity of an image. This would allow for images to have their
background removed for the purposes of classification by making the background
less opaque than the foreground.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Very Deep Convolutional Networks for Large Scale Image Recognition&lt;/em>
by Karen Simonyan and Andrew Zissernman discusses the SOTA performance of their
model in the 2014 ImageNet Challenge on localisation and classification tasks.
They discuss that be extending the depth of convolutional neural networks to 16
up to 19 layers, with a 3x3 filter size, SOTA performance is possible without
redeveloping the architecture of existing convolutional neural networks. Their
work builds of previous efforts of improving convulational neural network
performance by optimizing the filter size and intial layer, but contrasts
contemporaries [2] by not developing a new architecture. Their work has
importance as it shows that the existing convulational neural network
architecture is capable of SOTA performance by increasing the depth of the
model. They justify this by trying six different model configurations, and
finding that models with 16 to 19 layers performed best on the 2014 ImageNet
Challenge classification and localisation challenges.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using the technique proposed by S. Keshav in his work
&lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/very-deep-convolutional-networks-for-large-scale-image-recognition/</description></item><item><title>A summary of Going deeper with convolutions by Christian Szegedy et al..</title><link>https://nsynovic.dev/summaries/going-deeper-with-convolutions/</link><pubDate>Wed, 28 Sep 2022 20:07:40 -0500</pubDate><guid>https://nsynovic.dev/summaries/going-deeper-with-convolutions/</guid><description>&lt;h1 id="a-summary-of-going-deeper-with-convolutions">A summary of &lt;em>Going deeper with convolutions&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Christian Szegedy et al.;
&lt;a href="https://doi.org/10.48550/arXiv.1409.4842">https://doi.org/10.48550/arXiv.1409.4842&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-going-deeper-with-convolutions">A summary of &lt;em>Going deeper with convolutions&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#third-pass">Third Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Going deeper with convolutions&lt;/em> by Christian Szegedy et al. [1]
describes a 2014 state of the art computer vision model (on the ImageNet
Large-Scale Visual Recognition Challenge) called GoogLeNet architected based on
Hebbian principles (i.e. neruons that fire together, are wired together)and a
constant computational budget. Their approach relies on creative algorithms and
neuroscience principles and aims to be a more power effiecient model for mobile
devices by limiting the computations during infrencing. Additionally, their
model is deep but not wide and is considered &amp;ldquo;sparse&amp;rdquo; by the authors. In other
words, there are as few nodes as possible within the neural network.&lt;/p>
&lt;p>Szegedy et al.&amp;rsquo;s contributions are a state of the art computer vision model that
provides experimental evidence that, &amp;ldquo;&amp;hellip; Approximating the expected optimal
sparse structure by readily avilible dense building blocks is a viable method
for improving neural networks for computer vision&amp;rdquo;. This means that this model
proves that dense neural networks for computer vision are not necessary in the
author&amp;rsquo;s viewpoint.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is a computer vision paper describing both a machine learning architecture
and refernce model.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is related to other computer vision papers that achieve state of the
art performance values based on the ImageNet Large-Scale Visual Recognition
Challenge.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Szegedy et al.&amp;rsquo;s contributions are:&lt;/p>
&lt;ul>
&lt;li>A computer vision model architecture (Inception) that is both sparse and aims
to be computationally efficient on mobile (non-server) devices,&lt;/li>
&lt;li>A reference model of the aforementioned computer vision model architecture&lt;/li>
&lt;li>A comparison of previous state of the art work to justify their claims that
sparser networks are the future of computer vision models.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the tables have proper column labels. However, Table 1 does not provide
default values for blank cells. This is most likely due to the layer type not
performaing a specific operation (as described in the column label). Regardless,
the remaining tables look good.&lt;/p>
&lt;p>Additionally, Figure 3 is very clear to read, if a little dense. However, as it
describes all of the layers of GoogLeNet and how they are connected, I find the
size to be appropriate.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is fairly well written. The only complaints that I have are minor
grammatical mistakes that the author&amp;rsquo;s left in (by accident I assume).
Additionally, that the authors didn&amp;rsquo;t optimize their tables and figures to
better fit on the pages. As tables and figures are stacked on top of one
another, it would be possible to reclaim paper space by rearranging multiple
tables and figures to be next to one another, with the exception of Figure 3 due
to the sheer size of it.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>Contrast Normalization&lt;/li>
&lt;li>Max Pooling&lt;/li>
&lt;li>Average Pooling&lt;/li>
&lt;li>Softmax Activation&lt;/li>
&lt;li>Dropout - [2]&lt;/li>
&lt;li>Localization Task - [3]&lt;/li>
&lt;li>Gabor Filters - [4]&lt;/li>
&lt;li>Network in Network - [5]&lt;/li>
&lt;li>Rectified Linear Activation - [6]&lt;/li>
&lt;li>Regions with Convolutional Neural Networks - [7]&lt;/li>
&lt;li>Multi-Box Prediction - [8]&lt;/li>
&lt;li>Arora proof - [9]&lt;/li>
&lt;li>LeNet 5 - [10]&lt;/li>
&lt;li>Fisher vectors&lt;/li>
&lt;li>Polyak Averaging - [11]&lt;/li>
&lt;li>Jaccard index&lt;/li>
&lt;li>Selective Search - [12]&lt;/li>
&lt;li>Photometric Distortions - [13]&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="third-pass">Third Pass&lt;/h2>
&lt;blockquote>
&lt;p>This section can only be complete after a virtual re-implementation of the
paper&lt;/p>
&lt;/blockquote>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>One assumption that the authors make is that overfitting is more prone to occur
in large models. Additionally, overfitting can occur when there is not enough
labeled examples in a dataset when a large model is training. Furthermore,
increasing the size of a model increases the number of computations that must be
done between layers (e.g. chaining two convulational layers results in
computation cost quadratically increasing) Their solutions relies on moving from
fully connected to sparsely connected architectures including within
convolutional layers. Also, their model architecutre is based on the idea that
computers are inefficent when, &amp;ldquo;&amp;hellip; Computing numerical calculations on
non-uniform sparse data structures&amp;rdquo;.&lt;/p>
&lt;p>They assume that 1x1, 3x3, and 5x5 filters are the proper filters to use, but
did not test other size of filters. They also assume that using, &amp;ldquo;Inception
modules&amp;rdquo; is only useful at higher levels, whereas the initial levels are
standard convultational levels. However, this was not tested either and was due
to, &amp;ldquo;infrastructural inefficiences&amp;rdquo; in the implementation.&lt;/p>
&lt;p>Finally, that the model that achieved state of the art performance was the best
model. The authors had been training and testing other models for months prior,
however, it is unclear what the testing methodology was and why a particular
model was choosen to compete in the ImageNet competition.&lt;/p>
&lt;h3 id="correctness">Correctness&lt;/h3>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>The first paragraph of assumptions seems reasonable and correct. However, the
remaining two paragraphs seem unreasonable. This is due to the lack of testing
that the author&amp;rsquo;s put in when optimizing their model with respect to the filter
sizes and choosing models. Furthermore, if testing did occur to address these
issues, it is not addressed in this paper, thus leaving the reader to wonder why
testing wasn&amp;rsquo;t performed.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>Based off of Section 3 (Motiviation and High Level Considerations), one
promising area of study would be to perform a network architecture search
utilizing the principles and reasoning of their approach to other machine
learning and computer vision domains.&lt;/p>
&lt;p>An enhancement to their work is possible by analyzing what filter sizes most
optimal improve performance. Currently the author&amp;rsquo;s are restricting GoogLeNet to
1x1, 3x3, and 5x5 filter sizes, but this was due to convience and no data was
given to support this.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Going deeper with convolutions&lt;/em> by Szegedy et al. [1] introduces a
computer vision model architecture called Inception and a reference model called
GoogLeNet.&lt;/p>
&lt;p>Inception is a model architecuture that is both sparse and (attempts to be)
computationally efficent during inferencing with only 1.5 billion multiply-add
operations allowed. Inception models are composed of multiple Inception modules
that are stacked on top of each other. Each Inception module takes in data from
the previous layer and passes it into small convultional filters (i.e. 1x1
typically). There are three of these small filters that are wired to the input
of the Inception module, with one of them connected directly to the output. The
outputs of two of these filters are then passed into larger filters (i.e. 5x5)
to which it is then passed into a DepthConcat function. Additionally, a 3x3
filter is wired to the input of the module and the output of which goes into a
1x1 filter to be passed into the DepthConcat function as well. From there, it is
passed into another Inception module and the process repeats.&lt;/p>
&lt;p>&lt;strong>Note:&lt;/strong> Depth when referring to two dimensional images refers to the color
channel of the image. As images typically have three color channels (i.e. red,
green, blue), an image would have a depth of 3.&lt;/p>
&lt;p>&lt;em>Example:&lt;/em> A 200 pixel by 200 pixel full color spectrum image would be
represented as 200x200x3.&lt;/p>
&lt;p>It is possible for Inception modules to have an additional connection to the
input of the module to perform average pooling for softmax activation.&lt;/p>
&lt;p>GoogLeNet achieved SOTA performance in the ImageNet Large-Scale Visual
Recognition Challenge image classification task by having a top-5 error of 6.67%
on both the validation and testing data. This is an improvment of 56.5% in
comparison to 2012&amp;rsquo;s SOTA performer (SuperVision) and 2013&amp;rsquo;s SOTA performer
(Clarifai). Additionally, they achieved SOTA perfomance for the ImageNet
Large-Scale Visual Recognition Challenge detection task with a mean average
precision of 43.9% utilizing an ensemble inference approach. This model was
architected using the Inception architecture with 22 layers. However, not every
layer was an Inception module; the first few layers were standard convulational
layers.&lt;/p>
&lt;p>The author&amp;rsquo;s contributions were as follows;&lt;/p>
&lt;ol>
&lt;li>The Inception computer vision architecture,&lt;/li>
&lt;li>The GoogLeNet SOTA computer vision model for classification and object
detection.&lt;/li>
&lt;/ol>
&lt;p>My opinion on this paper is that while it is well written, the author&amp;rsquo;s make
numerous assumptions about the optimal performance of their model&amp;rsquo;s
architecture. They don&amp;rsquo;t test optimal sizes for filters as well as resolving
bugs such as the usage of standard convulational layers early in the model. Both
of which can be solved by performing a neural architecture search.&lt;/p>
&lt;p>Future work for this paper would involve optimizing the model architecture via a
neural architecture search. As well as evaluating the performance of the model
by both increasing and decreasing the depth of the model.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using the technique proposed by S. Keshav in his work
&lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/going-deeper-with-convolutions/</description></item><item><title>A summary of How to Read a Paper by S. Keshav</title><link>https://nsynovic.dev/summaries/how-to-read-a-paper/</link><pubDate>Wed, 28 Sep 2022 15:11:09 -0500</pubDate><guid>https://nsynovic.dev/summaries/how-to-read-a-paper/</guid><description>&lt;h1 id="a-summary-of-how-to-read-a-paper">A summary of &lt;em>How to Read a Paper&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>S. Keshav;
&lt;a href="https://doi.org/10.1145/1273445.1273458">https://doi.org/10.1145/1273445.1273458&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-how-to-read-a-paper">A summary of &lt;em>How to Read a Paper&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#third-pass">Third Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#discussion-of-the-proofs">Discussion of the Proofs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#how-would-i-present-the-ideas">How Would I Present the Idea(s)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The paper, &lt;em>How to Read a Paper&lt;/em> by S. Keshav is a tutorial for graduate
students on how to read an academic paper. They propose a &amp;ldquo;three-pass&amp;rdquo; approach
that aims to reduce the frustration that graduate students face when reading
papers. Additionally, they discuss how to perform a literature survey of a new
field, their experience with this methodology, and write that this document is
meant to exist as a living work, with adjustments to be made as seen fit by the
author.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is definetly a more causal piece of academic work that aims at easing
students into the reading papers. I would classify this paper as &amp;ldquo;meta&amp;rdquo;,
educational, or as a formal letter to students. The later classification is due
to the lack of surveys or qualitative/ quantitative data from others that have
applied this or similar methods to reading papers.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is most closely related to papers that discuss the writing of
academic works and the review process of academic works.&lt;/p>
&lt;h3 id="correctness">Correctness&lt;/h3>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>The assumptions in the abstract and introduction seem reasonable. However,
assuming that only graduate students are the only ones that struggle with
reading academic works is unrepresentative of &lt;em>my particular experinece&lt;/em>.
Undergraduate students as well as professionals in industry also struggle with
reading these works as well.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>S. Keshav&amp;rsquo;s contributions is a three-stage process for reading papers and a
framework for performing literature reviews of a new field.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is well written and is easy to comprehand. I would strongly recommend
this paper to be read by everyone regardless of academic status.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>There are no illustrations to discuss in this paper.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>[1], [2], [3], and [4]&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="third-pass">Third Pass&lt;/h2>
&lt;blockquote>
&lt;p>This section can only be complete after a virtual re-implementation of the
paper&lt;/p>
&lt;/blockquote>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;m nit-picking here, but the S. Keshav focuses solely on graduate students as
the demographic that has trouble reading academic papers. Now while graduate
students do typically read more papers than undergraduates, it is not unheard of
for academic readings to be given to undergraduate students as homework
assingments or for them to read them on their own. Addotionally, professionals
in industry also struggle with this task as well. A more inclusive audience
would have been appreciated, but would not have improved the content or quality
of this paper.&lt;/p>
&lt;h3 id="discussion-of-the-proofs">Discussion of the Proofs&lt;/h3>
&lt;p>The only proof of the &amp;ldquo;three-pass&amp;rdquo; method that was discussed was the experience
of the author. An awfully biased proof, however, I do appreciate at least some
quantifiable data for this method.&lt;/p>
&lt;h3 id="how-would-i-present-the-ideas">How Would I Present the Idea(s)&lt;/h3>
&lt;p>I think the author presented these ideas exceptionally well and clearly, and
cannot think of any additional presenation method aside from the critiques of
the assumptions mentioned in &lt;a href="#author-assumptions">Author Assumptions&lt;/a>.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>A survey of graduate students on their methodology for reading papers&lt;/li>
&lt;li>A survey of industry professionals on their methodology for reading papers&lt;/li>
&lt;li>An artifact that allows for a user to step through a set series of steps to
properly understand a document.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>How to Read a Paper&lt;/em> by S. Keshav is a &amp;ldquo;meta&amp;rdquo; or educational paper
about how to read an academic work. Their main contributions are a three step
process on how to read a paper, as well as a framework for performing a
literature review in a new field. This three step process involves a:&lt;/p>
&lt;ol>
&lt;li>Bird&amp;rsquo;s Eye View of the paper where only the Title, Abstract, Introduction,
Conclusion, and section and sub-section headings are read first,&lt;/li>
&lt;li>A deeper analysis of figures and content of the paper which involves finding
new, unread references to the reader and evaluating the quality of
illustrations to determine the quality of the paper,&lt;/li>
&lt;li>A virtual reimplemenation of the paper where every claim of the paper is
analyzed and critiqued; typically this done by reviewers or those that are
doing a deeper analysis of the work.&lt;/li>
&lt;/ol>
&lt;p>I can see this process being useful for researchers as reimplementers of other&amp;rsquo;s
research must accomplish all three steps to properly appreciate and understand
what they need to do to perform their task. As for the literature review
framework, it involves utilizing academic search engines (e.g.
&lt;a href="https://https://scholar.google.com/">Google Scholar&lt;/a>) to find work within a
particular field, finding shared citations or authors within that field, then
evaluating top confrences within that field to see who the top researchers and
research topics are within that field. For exploratory research, this is both an
extremely simple and effective framework to follow and adapt to different
domains.&lt;/p>
&lt;p>However, S. Keshav does limit the reach of this paper by making it focus solely
on the woes of graduate students. This is inaccurate of the wider academic
readership, as more and more frequently undergraduate and industry professionals
are reading academic papers both for pleasure and for utilization in
assignments. This paper can easily become more inclusive of wider audiences
without changing the content in an updated version of this document. This would
make sense as the author has requested that this paper be treated as a living
document that can be subject to change as the author adapts his process and
framework for academic review.&lt;/p>
&lt;p>I would personally like to see this work be quantified in surveys and
implemented as artifacts that ensure that readers are properly following the
review method that the author has laid out.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using the technique proposed by S. Keshav in his work
&lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/how-to-read-a-paper/</description></item><item><title>Hugo, Gopher, Gemini, and Annoyances</title><link>https://nsynovic.dev/posts/hugo-gopher-gemini-annoyance/</link><pubDate>Thu, 04 Aug 2022 08:38:05 -0500</pubDate><guid>https://nsynovic.dev/posts/hugo-gopher-gemini-annoyance/</guid><description>&lt;h1 id="hugo-gopher-gemini-and-annoyances">Hugo, Gopher, Gemini, and Annoyances&lt;/h1>
&lt;blockquote>
&lt;p>My story with setting up a Gopher and Gemini compatible website powered by
Hugo.&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;p>A little while ago, I wanted to create my own website. I struggle with creating
frontends for websites, so I choose to use the Hugo [0] static site generator
as it has many great themes. Learning Hugo was simple enough since the most
basic sites are just &lt;code>md&lt;/code> content files and &lt;code>toml&lt;/code> config files. So, after a
brief learning period, I built my website, got it hosted on GitHub Pages [1],
and setup a domain from Google Domains [2] to link to it all.&lt;/p>
&lt;p>That should be it, right?&lt;/p>
&lt;p>Well then I saw these two videos from DistroTube [3] [4] and got interested
in creating and hosting my own Gopher [5] phlog and Gemini [6] capsule. This
is where the annoyances began.&lt;/p>
&lt;h2 id="gopher-the-bane-of-my-existence-for-about-a-week">Gopher: The Bane of my Existence (for about a week)&lt;/h2>
&lt;p>I wanted to create a Gopher phlog because I thought it was just plain cool. Who
wouldn&amp;rsquo;t want a text only mirror of their site not indexed by modern search
engines (probably everyone)? I was aware the Hugo could output my website in a
variety of different formats. But, I had no idea how to do that.&lt;/p>
&lt;p>Luckily, Jason F. McBrayer had already figured this out in a blog post on his
website [7]. So I copied and pasted away. I ran the Hugo site generator and I
got output! Huzzah! But when I tested the site with the &lt;code>gophernicus&lt;/code> [8]
server, the site was malformed. Links to blog pages lead to nowhere or returned
error message.&lt;/p>
&lt;p>So I deleted my edits, re-copied, re-pasted (?), and ran the generator. Still,
no dice. Even manually typing in his work into my project didn&amp;rsquo;t work.&lt;/p>
&lt;p>At this point it was getting late, so I put the project into the back of my head
and would continue to work on it for the next week. My stuborness was because I
knew it was possible to do this conversion, after all, I had gotten output. But
there was a syntax error somewhere that I just couldn&amp;rsquo;t find.&lt;/p>
&lt;p>Eventually, I found documentation for the &lt;code>gophermap&lt;/code> syntax [9] and it all
clicked. I use &lt;code>neovim&lt;/code> [10] as my editor of choice. I use spaces instead of
tabs. &lt;code>gophermaps&lt;/code>, as a product of the 90s, require tabs for link formatting.&lt;/p>
&lt;p>Insert face palm here.&lt;/p>
&lt;p>The fix for this problem was to &lt;code>:set noexpandtab&lt;/code> and then paste in McBrayer&amp;rsquo;s
work.&lt;/p>
&lt;p>After that, the links worked. I then spent some time setting the Gopher template
that he provided to what I wanted, but that was minor work.&lt;/p>
&lt;p>In all, it took me a week to RTFM and move on from this arguably ridiculous
project.&lt;/p>
&lt;h2 id="gemini-a-lot-easier-than-gopher">Gemini: A lot easier than Gopher&lt;/h2>
&lt;p>It was significantly easier to create the Gemini capsule than a Gopher Phlog.&lt;/p>
&lt;p>But before that, I had to create the templates and output settings for Gemini in
Hugo. This, again, was taken care of by Sylvain Durand [11]. With my
&lt;code>:set noexpandtab&lt;/code> option, I copied and pasted his work into my project&amp;hellip; And
boom! It worked.&lt;/p>
&lt;p>That was anticlimatic wasn&amp;rsquo;t it? Again, some formatting of the provided template
was necessary, but that was minor.&lt;/p>
&lt;p>The new problem, was how to distribute this site.&lt;/p>
&lt;h2 id="pubnix-whats-old-is-new-again">PubNix: What&amp;rsquo;s Old is New Again&lt;/h2>
&lt;p>In my research of both the Gopher and Gemini protocols, I found out about
PubNixs: community ran servers that run on low powered machines. These servers
are a modern implementation of the time sharing servers of yore where there were
a few large mainframes in the country that only a few people could log into at a
time. A subset of these PubNixs is the &lt;code>tildeverse&lt;/code> [12]. These are &lt;strong>very&lt;/strong>
community oriented servers that offer free webhosting of the Gemini, Gopher,
HTTP, and Spartan sites. All that&amp;rsquo;s required is to create an account and start
uploading.&lt;/p>
&lt;p>For those that are interested in hosting a Gemini and/or Gopher site, I
encourage the usage of these servers as it takes the problems of hosting content
(maintainence, privacy, port forwarding, etc.) off of your shoulders&lt;/p>
&lt;h2 id="the-end">The End&lt;/h2>
&lt;p>If you ware interested in doing something similar, here is a link to my websites
source code on GitHub [13]. There you can see my templates, scripts, and
config options for generating Gemini, Gopher and HTTP sites. Best of luck, and
happy hacking!&lt;/p>
&lt;hr>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/posts/hugo-gopher-gemini-annoyance/</description></item><item><title>Why Sign Commits?</title><link>https://nsynovic.dev/posts/why-sign-commits/</link><pubDate>Mon, 01 Aug 2022 15:38:48 -0500</pubDate><guid>https://nsynovic.dev/posts/why-sign-commits/</guid><description>&lt;h1 id="why-sign-commits">Why Sign Commits?&lt;/h1>
&lt;blockquote>
&lt;p>Why should you sign your commits?&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;h2 id="preface">Preface&lt;/h2>
&lt;p>I recently read this article [0] by Alessandro Segala about why I should sign
my commits. And I completely agree with, and would like to expand upon, their
work.&lt;/p>
&lt;h2 id="identify-theft">Identify Theft&lt;/h2>
&lt;p>Identify theft is not a joke [1].&lt;/p>
&lt;p>The FTC in their 2021 edition of the CSN Annual Data Book [2] reported that
there were 1,434,676 reports of identity fraud in 2021 This theft allows the
perpetrator to commit acts of fraud in your name while reaping the benefits. As
developers, we not only have to protect our real world identities from theft,
but our digital ones as well. And while it is important to have strong and
secure passwords, I&amp;rsquo;m not referring to your accounts as digital identities. I&amp;rsquo;m
instead talking about your contributions to open source projects.&lt;/p>
&lt;p>This article focusses around &lt;code>git&lt;/code> [3] and online version control systems
(VCSs) that implement &lt;code>git&lt;/code> as their backend.&lt;/p>
&lt;p>It is not only possible, but increadibly easy to sign a commit under a different
identity. In addition, online VCSs will read the &lt;code>git&lt;/code> commit history and per
commit, add the appropriate account information to the commit (assuming an
account exists with the email address that is attached ot the &lt;code>git&lt;/code> repository).
This feature, is meant to provide a user friendly way of viewing &lt;code>git&lt;/code> commits.
However, it also allows for an attacker to take advantage of these tools and
publish commits to a project under someone else&amp;rsquo;s identity.&lt;/p>
&lt;h2 id="the-dangers-of-developer-identity-theft">The Dangers of Developer Identity Theft&lt;/h2>
&lt;p>The biggest threat to a developer who doesn&amp;rsquo;t sign their commits is the lack of
trust a community can have for a particular developer.&lt;/p>
&lt;p>A malicious attacker who signs off on infected, poorly written, or malformed
commits and publishes to a project can ruin a developer&amp;rsquo;s relationship to a
community.&lt;/p>
&lt;p>A malicious attacker could publish commits that actively ruin existing features.
They could also introduce bugs into a repository under someone&amp;rsquo;s name.&lt;/p>
&lt;h2 id="benefits-of-signing">Benefits of Signing&lt;/h2>
&lt;p>To combat this, &lt;code>git&lt;/code> allows for individuals to sign their commits with a GPG
[4] key.&lt;/p>
&lt;p>This allows for a number of benefits:&lt;/p>
&lt;ol>
&lt;li>Commits in the &lt;code>git&lt;/code> history that are signed have metadata attached to them
saying that they&amp;rsquo;re signed.&lt;/li>
&lt;li>If the GPG key is published to an online VCS that supports this feature, a
&lt;em>verified&lt;/em> tag will be applied to commits that are signed and match a user&amp;rsquo;s
GPG key.&lt;/li>
&lt;li>Developer identity can be confirmed by running checks against the public
facing key of a commit and a developer&amp;rsquo;s private key.&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>Since reading [0], I have implemented commit signing for my project going
forward. I also now require all group projects to have signed commits prior to
acceptance.&lt;/p>
&lt;p>Setting up signed commits was trivial, and there were plenty of guides [0]
[5] [6] on how to do so.&lt;/p>
&lt;p>I strongly encourage all developers to sign their commits in order to improve
the verification of work done by legitimate developers, instead of allowing the
work of theives to perforate throughout our community.&lt;/p>
&lt;hr>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/posts/why-sign-commits/</description></item><item><title>Hello World</title><link>https://nsynovic.dev/posts/hello-world/</link><pubDate>Thu, 28 Jul 2022 19:47:48 -0500</pubDate><guid>https://nsynovic.dev/posts/hello-world/</guid><description>&lt;h1 id="hello-world">Hello World&lt;/h1>
&lt;blockquote>
&lt;p>An introduction to me, this site, and the content I want to make.&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;h2 id="hi-there">Hi there!&lt;/h2>
&lt;p>My name is Nicholas M. Synovic.&lt;/p>
&lt;p>At the time of writing, I&amp;rsquo;m a Research Assistant at Loyola University Chicago
[0] with the Software and Systems Laboratory [1]. I graduated Loyola in May
of 2022 with a B.S in Computer Science. Currently, I&amp;rsquo;m pursuing a M.S in
Computer Science with a concentration in Artificial Intelligence. My
concentration is more specifically targetting low power computer vision systems.&lt;/p>
&lt;p>As an undergrad, I pursued and lead Software Engineering Metrics research and
was successfully able to publish a paper into the IEEE ASE 2022 [2]
confrence&amp;rsquo;s tools track as the lead author [3]. In addition, I was fortunate
enough to be an author on a Software Engineering paper focussing on deep
learning model reuse with researchers from Purude University [4]. I currently
work on both Software Engineering and Computer Vision research.&lt;/p>
&lt;p>In addition to my academic work, I have experience leading IT teams and
developing full stack software. I was one of the first in my high school to join
the student IT program which performed real world maintaince on my district&amp;rsquo;s
network, personal, and tech infrastructure [5]. As a 2nd year student at
Loyola, I won second place at the Fall Student Showcase for my work in
identifying and providing solution for Congress.gov&amp;rsquo;s search functionality as
well as creating a dataset of all the content on Congress.gov [6]. And as a
4th year senior, I developed an application for Medline Industries [7] to
facilitate their corporate wide Talent Planning Process. This application was
used to assess ~30,000 employees in 2022 to find the next generation of leaders.&lt;/p>
&lt;p>On the social side of things, I am currently a board member of the Loyola AI
Club [8] as well as a student ambassador for Intel&amp;rsquo;s OneAPI platform [9].&lt;/p>
&lt;p>For further information on how to contact me, see my Contact page [10].&lt;/p>
&lt;h2 id="whats-up-with-this-site">What&amp;rsquo;s up with this site?&lt;/h2>
&lt;p>This site is generated from Markdown using Hugo [11] and the Ananke theme
[12].&lt;/p>
&lt;p>This decision was made due to Hugo&amp;rsquo;s extensive templating feature, which allows
me to accessible to those using HTTP [13], Gopher [14], and Gemini [15]
protocols. I intend to use this site to experiment with other alternative
network protocols as they are developed.&lt;/p>
&lt;p>This site is also mirrored across several different services. Currently, it is
hosted on both GitHub Pages [16] as &lt;code>https://nsynovic.dev&lt;/code> [17]. Blog posts
are mirrored on my the Gopher and Gemini versions of this site, as well as my
Dev.to [18] page. In addition, I intend to use my account at &lt;code>tilde.team&lt;/code>
[19] to add an additional backup of the HTTP site there as well.&lt;/p>
&lt;h2 id="what-content-is-coming">What content is coming?&lt;/h2>
&lt;p>I don&amp;rsquo;t have a set plan at the moment, however, I intend to provide less
academicly leaning posts on this site. Think more opinion pieces, tutorials, or
other ramblings. Primarily text based content will be posted on this site as
there are other platforms that are better at hosting multimedia content.&lt;/p>
&lt;hr>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/posts/hello-world/</description></item><item><title>Contact</title><link>https://nsynovic.dev/contact/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nsynovic.dev/contact/</guid><description>&lt;h1 id="contact-me">Contact Me&lt;/h1>
&lt;blockquote>
&lt;p>How to Contact Me&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;h2 id="social-media">Social Media&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.facebook.com/nsynovic">Facebook&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://twitter.com/nick_synovic">Twitter&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.instagram.com/nicholas_synovic/">Instagram&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/channel/UCVG2VYy7GJ86BLiUaj5LEZQ">Youtube&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.linkedin.com/in/nsynovic/">LinkedIn&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/NicholasSynovic">GitHub&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="blogging-platforms">Blogging Platforms&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://dev.to/nicholassynovic">Dev.to&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="mirrors-of-this-site">Mirrors of this Site&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://tilde.team/~nosnow">https://tilde.team/~nosnow&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="alternative-protocols">Alternative Protocols&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://nsynovic.dev/posts/feed.xml">RSS&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://tilde.team/~nosnow/posts/feed.xml">RSS #2&lt;/a>&lt;/li>
&lt;li>&lt;a href="gopher://tilde.team/1/~nosnow">Gopher&lt;/a>&lt;/li>
&lt;li>&lt;a href="gemini://tilde.team/~nosnow">Gemini&lt;/a>&lt;/li>
&lt;li>&lt;a href="spartan://tilde.team/~nosnow">Spartan&lt;/a>&lt;/li>
&lt;/ul>
Citations availible at https://nsynovic.dev/contact/</description></item></channel></rss>