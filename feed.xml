<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Nicholas M. Synovic</title><link>https://nsynovic.dev/</link><description>Recent content on Nicholas M. Synovic</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Nicholas M. Synovic</copyright><lastBuildDate>Sun, 26 Jun 2022 21:04:02 -0500</lastBuildDate><atom:link href="https://nsynovic.dev/feed.xml" rel="self" type="application/rss+xml"/><item><title>A summary of How Developers and Managers Define and Trade Productivity for Quality by Margaret-Anne Storey et al.</title><link>https://nsynovic.dev/summaries/how-developers-and-managers-define-and-trade-productivity-for-quality/</link><pubDate>Thu, 27 Oct 2022 16:03:42 -0500</pubDate><guid>https://nsynovic.dev/summaries/how-developers-and-managers-define-and-trade-productivity-for-quality/</guid><description>&lt;h1 id="a-summary-of-how-developers-and-managers-define-and-trade-productivity-for-quality">A summary of &lt;em>How Developers and Managers Define and Trade Productivity for Quality&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Margaret-Anne Storey et al.; &lt;a href="https://doi.org/10.1145/3528579.3529177">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-how-developers-and-managers-define-and-trade-productivity-for-quality">A summary of &lt;em>How Developers and Managers Define and Trade Productivity for Quality&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>This paper aims to understand the issue of developers and managers having
differing views of productivity, and when to trade the quality of the product
for more productivity. Additionally, this calls into question what is quality,
as well as how does one measure both of these attributes.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this paper as it provides a case study conducted with
Microsoft developers and managers about how they measure and value productivity.
Thereby allowing establishing what a sample of developers define productivity
as, and what a sample of managers define it as well. Additionally, the authors
propose utilize their existing framework SPACE to codify developer and manager
responses. They also propose a new framework, TRUCE, designed to help developers
and managers make decisions about software quality vs productivity tradeoffs.
These frameworks are releated but provide different lenses into software
development.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is a survey paper of practicioners in industry.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is closely related to industry metric usage survey papers, productity
and quality papers, and - more broadly - papers that disucss the usage of and of
software metrics in teams.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Thier contributions is a survey of what developers and managers at Microsoft
consider to be productivity and quality, as well as the TRUCE framework for
identifying when quality vs productivity should be tradedoff.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done into understanding what software quality is, how to measure
productivity, and what is productivity. Also, the authors have previously
described a framework called SPACE (Satisfaction, Performance, Activity,
Collaboration, and Efficiency) which was used to codify the responses from the
survey participants.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the figures are labeled properly, easy to understand, and have clear
captions.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is well written. However, I&amp;rsquo;m not a fan with how the abstract was
structured. I found the topic-description approach didn&amp;rsquo;t engage me as a reader.
But that is more of a personal opinion than an objective fact.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://doi.org/10.1145/3454122.3454124">The SPACE of Developer Productivity: Thereâ€™s More to It than You Think&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://doi.org/10.1109/TSE.2018.2842201">Motivation and satisfaction of software engineers&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://doi.org/10.1007/978-1-4842-4221-6_3">Why We Should Not Measure Productivity&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.microsoft.com/en-us/research/publication/appendix-to-productivity-quality-alignment">Appendix to How Developers and Managers Define and Trade Off Productivity and Quality&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://doi.org/10.1109/TSE.2019.2944354">Towards a Theory of Software Developer Job Satisfaction and Perceived Productivity&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors conducted a survey of Microsoft developers and managers on how they
define productivity and quality, as well as the trade offs between productivity
and quality. 167 responses were collected, with 131 responses being from
developers and 34 from managers. Responses were codified using the SPACE
methodology proposed by the authors in a previous work.&lt;/p>
&lt;p>Comparisons were made between how:&lt;/p>
&lt;ul>
&lt;li>Developers define productivity&lt;/li>
&lt;li>Managers define team productivity&lt;/li>
&lt;li>Developers define quality&lt;/li>
&lt;li>Managers define team quality&lt;/li>
&lt;li>How developers &lt;em>think&lt;/em> managers define team productivity&lt;/li>
&lt;li>How managers &lt;em>think&lt;/em> developers define productivity&lt;/li>
&lt;li>Do developers and managers trade quality for productivity?&lt;/li>
&lt;/ul>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>They propose the TRUCE framework for software quality (Timeliness, Robustness,
User Needs, Collaboration Needs, and Evolvable) based on this study alone.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>I don&amp;rsquo;t think that TRUCE can stand on its own just on this work. Additional
surveys and work need to be done to validate the usefulenss and how applicable
this framework is outside of the subset of developers and managers at Microsoft
that responded to the survey. The authors do address this in their paper.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d love to perform a literary review of related works and surveys to identify
if the TRUCE framework is applicable. Additionally, I&amp;rsquo;d like to perform a follow
up study of answering and analyzing the question as to what developers &lt;em>think&lt;/em>
managers consider to be quality work and vice versa.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>What prevented the authors from asking about what developers and managers
&lt;em>think&lt;/em> the other considers quality work?&lt;/p>
&lt;p>Can any of the SPACE or TRUCE definitions be quantified automatically by analyzing feature requests?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>I appreciate the clear figures, charts, and tables. I don&amp;rsquo;t like the style of
the abstract as it doesn&amp;rsquo;t engage me as a reader. But the finding boxes that
summarize the survey results per subsection were a nice touch and are
appreciated.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>How Developers and Managers Define and Trade Productivity for
Quality&lt;/em> by Margaret-Anne Storey et al. [1] presents the results of a survey
conducted on 131 Microsoft developers and 34 managers about what they consider
to be productive work, quality work, what the other cohort defines each topic,
and if they have ever made explicit trade offs between productivity and quality.
The authors took the responses and codified them using the SPACE framework that
they proposed in an earlier paper [2].&lt;/p>
&lt;p>They found that developers tend to define productive work as activities (number
of tasks completed or iterations; 50%), efficiency and flow (entering a flow
state; 38%), and productivity (delivering on projects; 35%). Managers tend to
define productive work across the team in productivity (67%), efficiency and
flow (45%), and collaboration (working with others to brainstorm ideas/
providing feedback; 33%).&lt;/p>
&lt;p>However, developers &lt;em>think&lt;/em> managers define team productivity in activity (53%),
productivity (37%), and collaboration (19%). Whereas managers &lt;em>think&lt;/em> developers
define productivity in activity (52%), efficiency and flow (42%), and
productivity (24%). Here, both managers and developers &lt;em>think&lt;/em> the other defines
productivity than what is actually true.&lt;/p>
&lt;p>The take away with productivity, is that all participants defined productivity
under the SPACE framework.&lt;/p>
&lt;p>Quality was defined under the proposed TRUCE (Timeliness, Robustness, User
Needs, Collaboration, Evolution) framework. Developers and managers both define
quality similarly as robustness (71%, 88%), evolution (44%, 33%), and user need
(38%, 39%). Managers rank user need higher than evolution, where as developers
disagree with them.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/how-developers-and-managers-define-and-trade-productivity-for-quality/</description></item><item><title>A summary of Robust Real Time-Face Detection by P. Viola and M.J. Jones</title><link>https://nsynovic.dev/summaries/robust-real-time-face-detection/</link><pubDate>Mon, 24 Oct 2022 19:29:57 -0500</pubDate><guid>https://nsynovic.dev/summaries/robust-real-time-face-detection/</guid><description>&lt;h1 id="a-summary-of-robust-real-time-face-detection">A summary of &lt;em>Robust Real-Time Face Detection&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>P. Viola and M.J. Jones;
&lt;a href="https://doi.org/10.1023/B:VISI.0000013087.49260.fb">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-robust-real-time-face-detection">A summary of &lt;em>Robust Real-Time Face Detection&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Robust Real-Time Face Detection&lt;/em> by P. Viola and M.J. Jones [1]
presents a new methodology for effieciently performing face detection. They due
this through the usage of an integral image which is able to reduce the
computational complexity to constant time (O(1)) of analyzing an image as it
doesn&amp;rsquo;t rely on scale invarience and thus an image pyramid. Additionaly, the
classifier that they build is &amp;ldquo;simple and efficient&amp;rdquo; and allows for the engineer
to specify a large number of features to be analyzed without comprimising on
performance as it relies upon the AdaBoost algorithm to select imporant
features. Furthermore, the authors propose a method for building a cascade of
classifiers which further reduces computation time as each classifier specifies
. Finally, they propose experiments that can be ran on face detection datasets
to conduct supervised learning.&lt;/p>
&lt;p>While this paper does propose many new and innovative ideas, the paper
originates from 2003.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is a research paper focussing on improving the Computer Vision task of face
detection without the relience of CNNs.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is most closely related to non-CNN face detection papers.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>They create an integral image which is able to reduce the computational
complexity to constant time (O(1)) of analyzing an image as it doesn&amp;rsquo;t rely on
scale invarience and thus an image pyramid. Additionaly, the classifier that
they build is &amp;ldquo;simple and efficient&amp;rdquo; and allows for the engineer to specify a
large number of features to be analyzed without comprimising on performance as
it relies upon the AdaBoost algorithm to select imporant features. Furthermore,
the authors propose a method for building a cascade of classifiers which further
reduces computation time as each classifier specifies . Finally, they propose
experiments that can be ran on face detection datasets to conduct supervised
learning.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Prior work has been done in creating face detection systems. Prior work has been
done in creating the AdaBoost algorithm that is used to create a cascade of
classifiers. Prior work has been done in identifying methodologies to create
image features.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this paper as it presents a non-CNN methodology for
reliably identifying faces in images. Additionally, the authors also present a
methodology for doing this task effieciently on low end hardware.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The figures, diagrams, and graphs are well explained and designed.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>Yes, however a bit lengthy. Optimizations could have been made with respect to
reducing the amount of content describing the background to the AdaBoost
algorithm.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>A Decision-Theoretic Generalization of On-Line Learning and an Application to
Boosting [2]&lt;/li>
&lt;/ul>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d like to reimplement their work on a low powered device and compare it to a
newer CNN model on ML metrics.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Robust Real-Time Face Detection&lt;/em> by P. Viola and M.J. Jones [1]
presents a new methodology for effieciently performing face detection. They due
this through the usage of an integral image which is able to reduce the
computational complexity to constant time (O(1)) of analyzing an image as it
doesn&amp;rsquo;t rely on scale invarience and thus an image pyramid. Additionaly, the
classifier that they build is &amp;ldquo;simple and efficient&amp;rdquo; and allows for the engineer
to specify a large number of features to be analyzed without comprimising on
performance as it relies upon the AdaBoost algorithm to select imporant
features. Furthermore, the authors propose a method for building a cascade of
classifiers which further reduces computation time as each classifier specifies
. Finally, they propose experiments that can be ran on face detection datasets
to conduct supervised learning.&lt;/p>
&lt;p>The main &amp;ldquo;wow&amp;rdquo; factor of this work is that it was built on a low powered system.
This same application could be more performant on modern smartphones in
comparison to the system that it was originally tested on.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/robust-real-time-face-detection/</description></item><item><title>A summary of Learning Deep Features for Discriminative Localization by Bolei Zhou et al.</title><link>https://nsynovic.dev/summaries/learning-deep-features-for-discriminative-localization/</link><pubDate>Mon, 24 Oct 2022 14:44:26 -0500</pubDate><guid>https://nsynovic.dev/summaries/learning-deep-features-for-discriminative-localization/</guid><description>&lt;h1 id="a-summary-of-learning-deep-features-for-discriminative-localization">A summary of &lt;em>Learning Deep Features for Discriminative Localization&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Bolei Zhou et al.; &lt;a href="http://arxiv.org/abs/1512.04150">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-learning-deep-features-for-discriminative-localization">A summary of &lt;em>Learning Deep Features for Discriminative Localization&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Learning Deep Features for Discriminative Localization&lt;/em> by Bolei Zhou
et al. [1] describes using the global average pooling layer of CNNs to not
only regularize data, but also to localize objects in an image &lt;strong>even if the
network wasn&amp;rsquo;t trained for object detection&lt;/strong>. The authors propose a method for
object localization that involves a simple modification to the layer to generate
what they call &amp;ldquo;class activation maps&amp;rdquo; (CAMs), which are heatmaps of where the
CNN is &amp;ldquo;looking&amp;rdquo; at an image for labeling. The hotter the heatmap, the more
focus the CNN is putting on that specific image region.&lt;/p>
&lt;p>The authors go into detail as to how one would accomplish this with a
weakly-supervised object localization method, and its applications towards deep
features for generic localization, fine-grained recognition, and pattern
discovery. They conclude with visualizing class specific units.&lt;/p>
&lt;p>Their technique accomplishes object localization in a single forward pass on
existing CNN models that utilize a global average pooling layer.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is a CNN understanding and technique paper. It discusses a method for
understanding what a CNN is looking at as well as expanding the usage of image
classifiers for object localization.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is related to works involving object localization, image
classification, CNNs, and Deep Learning papers.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The author&amp;rsquo;s main contribution is a method for modifying the global average
pooling layer in CNNs to perform object localization in a single forward pass.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>There has been work done in utilizing weakly-supervised learning to perform
object localization. However, these works either don&amp;rsquo;t evaluate the object
localization task, or utilize multiple passes to perform the task.&lt;/p>
&lt;p>There has been numerous work that has gone into visualizing what occurs within a
CNN. Additionally, there has been work that has looked at the global &lt;em>max&lt;/em>
pooling layer, however, this work is the first to utilize the global &lt;em>average&lt;/em>
layer.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this paper as it provides a methodology of utilizing
existing CNNs trained on image classification to perform object localization
tasks &amp;ldquo;for free&amp;rdquo;. In other words, this paper presents a methodology for object
localization by reusing existing SOTA CNNs.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the figures and tables are labelled clearly, have detailed captions, and
make sense with respect to the paper.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is well written.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>Self-taught object localization with deep networks [2]&lt;/li>
&lt;li>Weakly supervised object localization with multi-fold multiple instance
learning [3]&lt;/li>
&lt;li>Learning and transferring mid-level image representations using convolutional
neural networks [4]&lt;/li>
&lt;li>Is object localization for free? weakly-supervised learning with convolutional
neural networks [5]&lt;/li>
&lt;li>Visualizing and understanding convolutional networks [6]&lt;/li>
&lt;li>Object detectors emerge in deep scene cnns [7]&lt;/li>
&lt;li>Network in network [8]&lt;/li>
&lt;li>Going deeper with convolutions [9]&lt;/li>
&lt;/ul>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I would love to take this work and apply it to my current research in low
powered computer vision. By utilizing larger networks to localize where in a
static scene the object of interest is most likely to be in (for example, a
static video of a bird sitting on a wire), I can pass in this mapping into a CNN
to specifically be interested in that region of the video/ image. Additionally,
by figuring out where a larger CNN is localizing data, I can then mask out any
cold area of the image prior to analysis by a smaller CNN.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Learning Deep Features for Discriminative Localization&lt;/em> by Bolei Zhou
et al. [1] discusses a weakly supervised method of performing object
localization on existing CNN models. Their method involves replacing the fully
connected layer at the end of a CNN performing image classifcation, with a
global average pooling layer into a softmax layer. This is so that the models
original functionality is not cut from the new model. However, the global
average pooling layer is modified so that a heatmap can be extracted focusing on
what the CNN is focussing on prior to labelling the image.&lt;/p>
&lt;p>Previous work involved the usage of weakly supervised CNNs, but relied on global
max pooling. Additional work utilized deconvolutional layers to perform a
similar task.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/learning-deep-features-for-discriminative-localization/</description></item><item><title>A summary of How to implement SVMs by John Platt</title><link>https://nsynovic.dev/summaries/how-to-implement-svms/</link><pubDate>Mon, 24 Oct 2022 13:46:36 -0500</pubDate><guid>https://nsynovic.dev/summaries/how-to-implement-svms/</guid><description>&lt;h1 id="a-summary-of-how-to-implement-svms">A summary of &lt;em>How to implement SVMs&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>John Platt; &lt;a href="https://doi.ieeecomputersociety.org/10.1109/5254.708428">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-how-to-implement-svms">A summary of &lt;em>How to implement SVMs&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The essay &lt;em>How to implement SVMs&lt;/em> by John Platt (as part of the larger &lt;em>Support
vector machine&lt;/em> collection of essays in the July/ August edition of the 1998
IEEE Intelligent Systems magazine) [1] discusses how to implement a Support
Vector Machine (SVM). This essay goes into great detail on implementation
strategies for handling larger datasets, as well as methods for training SVMs.
Topics include understanding the Quadratic Problem (what SVMs aim to solve),
sequential minimal optimization (reaching a global minimal value), and where to
find SVM implementations.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This essay seems to be a tutorial/ workshop paper about SVMs.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>I would expect papers that are about implementing SVMs from scratch would be
related to this essay.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contributions are an understanding of how SVMs work as well as how to
implement them efficently.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has already been done on experimenting optimal SVM algorithms and
minimization functions.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this paper as it provides an understanding of what a SVM is
and how they function.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The figure and charts have proper labels and captions that explain what they are
representing.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>For the most part, yes. However, the essay expects the reader to be knowledgable
about SVMs prior to reading the essay. This is shown mostly through the usage of
mathematical notation specific to the problem domain, and linking to other work
to explain it. While this is a short essay for a magazine, a brief sentence or
two about the notation would have been appreciated.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>A Tutorial on Support Vector Machines for Pattern Recognition [2]&lt;/li>
&lt;/ul>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The author assumes that the reader, should they implement their own SVM
algorithm, will be using a commericial numerical analysis package.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>Without understanding the nature of the numerical analysis packages of 1998, I
would assume that this assumption is correct. I base this on that the author
mentions that free numerical analysis packages (not if they were open sourced or
not) run slower than commercial packages and may have errors due to precision
mistakes.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;m not interested in creating my own SVM algorithm. However, having a better
understanding of how SVMs work as well as the different minimization functions
that they implement, would be nice to know.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The essay &lt;em>How to implement SVMs&lt;/em> by John Platt (as part of the larger &lt;em>Support
vector machine&lt;/em> collection of essays in the July/ August edition of the 1998
IEEE Intelligent Systems magazine) [1] discusses how to implement a Support
Vector Machine (SVM). The author goes into detail about what an SVM is trying to
accomplish (minimize a quadratic problem on a high dimensional matrix), what
techniques exist to solve this problem, as well as availible programs to allow
for researchers to utilize SVMs in their work.&lt;/p>
&lt;p>Overall, the essay does a good job of explaining the problem space as well as
implementation details, however, the essay is very much a product of its time.
There is less of a need to develop new SVM algorithms as there are many that are
provided off of the shelf in free and open source numerical analysis packages
[3] [4]. Additionally, the suggestion that readers should purchase a
numerical analysis package to create thier own SVM is dated in my opinion, as
again, there are many free options availible [5].&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/how-to-implement-svms/</description></item><item><title>A summary of Applying SVMS to Face Detection by Edgar Osuna</title><link>https://nsynovic.dev/summaries/applying-svms-to-face-detection/</link><pubDate>Mon, 24 Oct 2022 09:20:40 -0500</pubDate><guid>https://nsynovic.dev/summaries/applying-svms-to-face-detection/</guid><description>&lt;h1 id="a-summary-of-applying-svms-to-face-detection">A summary of &lt;em>Applying SVMs to Face Detection&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Edgar Isuna; &lt;a href="https://doi.ieeecomputersociety.org/10.1109/5254.708428">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-applying-svms-to-face-detection">A summary of &lt;em>Applying SVMs to Face Detection&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The essay &lt;em>Applying SVMS to Face Detection&lt;/em> by Edgar Osuna (as part of the
larger &lt;em>Support vector machine&lt;/em> collection of essays in the July/ August edition
of the 1998 IEEE Intelligent Systems magazine) [1] describes the usage of
Support Vector Machines (SVMs) to identify faces in static images and real time
systems. The work goes into detail about previous systems that attempted this
task, as well as a real time system that can classify images at 4 to 5 frames
per second.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is both a small systems essay, as well as a CV task analysis of the
state of the art when using this particular technique.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This essay is most similar to papers that discuss systems that implement face
detection.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The author&amp;rsquo;s main contributions are a system that utilizes SVMs for real time
facial detection. Additionally, their contributions include a discuss of
previous systems that attempted this task.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Quote from the &lt;em>Previous systems&lt;/em> section of the paper:&lt;/p>
&lt;p>&amp;ldquo;Researchers have approached the face-detection problem with different
techniques in the last few years, including neural networks [2] [3],
detection of face features and use of geometrical constraints [4], density
estimation of the training data [5], labeled graphs [6], and clustering and
distribution-based modeling [7] [8].&amp;rdquo;&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this essay as it proposes an SVM based solution for both
static image and real time face detection.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The figures are clear and explained well through their captions. However, Table
2 uses a metric called &amp;ldquo;False Alarms&amp;rdquo; to measure the number of times the system
reported a &amp;ldquo;face&amp;rdquo; that wasn&amp;rsquo;t a face. A more appropriate metric, such as recall,
would have been appropriate in this case.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is well written, however, it can be improved upon. The biggest
complaint that I have is the usage of bullet points to describe tasks/ steps
that were taken to complete a task. Additionally, many bullet points contained
more than one sentence. I find it to be more appropriate for papers to utilize
bullet points for short, unordered lists. Most appropriately used when listing
off different techniques or definitions, which this essay does utilize. Aside
from that, the individual steps are written well and clearly, and seem to be
fairly reproducable.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>Detection and localization of faces on digital images [2]&lt;/li>
&lt;li>Human Face Detection in Visual Scenes [3]&lt;/li>
&lt;li>Human face detection in a complex background [4]&lt;/li>
&lt;li>Probabilistic visual learning for object detection [5]&lt;/li>
&lt;li>Determination of face position and pose with a learned representation based on
labelled graphs [6]&lt;/li>
&lt;li>Learning and Example Selection for Object and Pattern Detection [7]&lt;/li>
&lt;li>Example-based learning for view-based human face detection [8]&lt;/li>
&lt;/ul>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors trained their system to identify vertically oriented, gray-scale
images of faces for their static image face detector. They make no mention as to
whether this detector is capable of identifying faces in off axis positions, or
if their system is capable enough to orient faces properly.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>Without understanding the availiblity of datasets at the time, this seems like a
valid assumption to make. However, simple data augmentation (such as rotating
the image) could&amp;rsquo;ve been done to increase the number of training examples of
faces not in the vertical orientation.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>A reimplementation of their work, both on static images and real time image
capture, would be interesting to perform on devices such as cameras, Raspberry
Pis, or other low powered systems. Additionally, comparing the power draw
between an SVM based solution and one that is powered by DL would be interesting
as well.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The essay &lt;em>Applying SVMS to Face Detection&lt;/em> by Edgar Osuna (as part of the
larger &lt;em>Support vector machine&lt;/em> collection of essays in the July/ August edition
of the 1998 IEEE Intelligent Systems magazine) [1] describes the usage of
Support Vector Machines (SVMs) to identify faces in static images and real time
systems. The author goes into detail about existing systems that were powered by
non-SVM techniques, as well as presenting their own system (for both static
image and real time image capture) for face detection.&lt;/p>
&lt;p>Their satic image system only works on gray scale images of vertically aligned
faces. Additionally, they used a small dataset to train the SVM. In doing so,
they limit the usage of the static image system to that specific domain, as well
as potentially creating a system that is unable to detect a face in all
potential cases (such as different ethnicities, lighting conditions, face
orientations, etc.).&lt;/p>
&lt;p>Their real time image capture system works on full color images of vertically
aligned faces by using a combination of a skin detector and a &amp;ldquo;primitive&amp;rdquo; motion
detector. This system was capable of recognizing faces at 4 to 5 frames per
second.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/applying-svms-to-face-detection/</description></item><item><title>A summary of Using SVMs For Text Categorization by Susan Dumais et al</title><link>https://nsynovic.dev/summaries/using-svms-for-text-categorization/</link><pubDate>Sun, 23 Oct 2022 16:45:32 -0500</pubDate><guid>https://nsynovic.dev/summaries/using-svms-for-text-categorization/</guid><description>&lt;h1 id="a-summary-of-using-svms-for-text-categorization">A summary of &lt;em>Using SVMs For Text Categorization&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Susan Dumais et al.;
&lt;a href="https://doi.ieeecomputersociety.org/10.1109/5254.708428">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-using-svms-for-text-categorization">A summary of &lt;em>Using SVMs For Text Categorization&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The essay &lt;em>Using SVMs For Text Categorization&lt;/em> by Susan Dumais et al. (as part
of the larger &lt;em>Support vector machine&lt;/em> collection of essays in the July/ August
edition of the 1998 IEEE Intelligent Systems magazine) [1] provides examples
of when using a Support Vector Machine (SVM) is beneficial with respect to text
classification. They discuss text classification, text representation and
feature selection, and an example use case on the Reuters collection. They
support the position that using SVMs for text classification (or really any
algorithm so long as it isn&amp;rsquo;t run by a human) is beneficial for this task.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This essay is more arguementative and position oriented.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This essay would most likely be classified alongside similar works that
evaluated the usefulness of SVMs with respect to human tasks.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their contributions is an analysis of SVMs for text classification.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done to understand what SVMs are, as well as usecases for SVMs.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Because it provides a case study of using SVMs on the Reuters collection with
respect to text classification.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the graphs and charts are clear to understand and have properly labeled
axis.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>This essay is clearly written.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>Introduction to Modern Information Retrieval [2]&lt;/li>
&lt;li>Fast Training of SVMs Using Sequential Minimal Optimization [3]&lt;/li>
&lt;/ul>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I would like to reimplement their study using the five different learning
algorithms they utilized to validate their results. The algorithms in question
are: Findsim, Naive Bayes, BayesNets, Trees, and LinearSVM.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The essay &lt;em>Using SVMs For Text Categorization&lt;/em> by Susan Dumais et al. (as part
of the larger &lt;em>Support vector machine&lt;/em> collection of essays in the July/ August
edition of the 1998 IEEE Intelligent Systems magazine) [1] presents the usage
of SVMs for text categorization on the Reuters collection in comparison to other
classification algorithms. They found that SVMs perform best on this
classification task.&lt;/p>
&lt;p>The greater reason for this essay is to encourage engineers to use learning
algorithms for human intensive tasks - such as text classification.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/using-svms-for-text-categorization/</description></item><item><title>A summary of SVMs - A Practical Consequence of Learning Theory by Bernhard Scholkopf</title><link>https://nsynovic.dev/summaries/svms-a-practical-consequence-of-learning-theory/</link><pubDate>Sun, 23 Oct 2022 10:02:41 -0500</pubDate><guid>https://nsynovic.dev/summaries/svms-a-practical-consequence-of-learning-theory/</guid><description>&lt;h1 id="a-summary-of-svms-a-practical-consequence-of-learning-theory">A summary of &lt;em>SVMs: A Practical Consequence of Learning Theory&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Bernhard Scholkopf;
&lt;a href="https://doi.ieeecomputersociety.org/10.1109/5254.708428">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-svms-a-practical-consequence-of-learning-theory">A summary of &lt;em>SVMs: A Practical Consequence of Learning Theory&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The essay &lt;em>SVMs - a practical consequence of learning theory&lt;/em> by Bernhard
Scholkopf (as part of the larger &lt;em>Support vector machine&lt;/em> collection of essays
in the July/ August edition of the 1998 IEEE Intelligent Systems magazine) [1]
discusses the underlying theory that powers Support Vector Machine (SVM)
algorithms and argues that these algortihms are useful and performant. His essay
contains sections on &lt;em>Learning pattern recognition from examples&lt;/em>,
&lt;em>Hyperplanes&lt;/em>, &lt;em>Feature spaces and kernels&lt;/em>, &lt;em>SVMs&lt;/em>, and &lt;em>Current developments
and open issues&lt;/em> which indicates an essay that will wholistically look at SVMs,
rather than a particualr facet of them.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is best classified as an informative essay on the benefits of SVMs
from a theoretical and practical view.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is most related to other papers within the magazine&amp;rsquo;s collection, as
well as work that goes into the theory behind SVMs.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>A brief description of the theory that powers SVMs, as well as identifying where
SVMs are practical.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done to develop and implement the SVM alogirthm.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Because it provides a concise description of the theory that powers SVMs, and
practical usages of SVMs.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The paper doesn&amp;rsquo;t provide and graphs or charts. However, the figures and
diagrams that are presented are clearly explained in the descriptions, are well
made, and are easy to comprehand.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>Sort of? The theory components of the essay are written distinctly differently
than the introduction and concluding sections of the paper. This could be due to
the discussion of mathematical prose; but due to this, the essay has two
different voices.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>The Nature of Statistical Learning Theory [2]&lt;/li>
&lt;/ul>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d like to explore the usage of SVMs for face or object detection and compare
it against the usage of DL techniques on both traditional and low-powered
centric metrics.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The essay &lt;em>SVMs: A Practical Consequence of Learning Theory&lt;/em> by Bernhard
Scholkopf (as part of the larger &lt;em>Support vector machine&lt;/em> collection of essays
in the July/ August edition of the 1998 IEEE Intelligent Systems magazine) [1]
discuss both the mathematical theory and current practice of using SVMs. SVMs
are useful in a research aspect as their functionality can be mathematically
explained. SVMs are a linear classifier that operate in multi-dimensional space
through the usage of a hyper plane. Hyper planes are choosen by finding support
vectors, which are instances of a class that are closest to one another. The
hyper plane then splits these two instances into two seperable sides. To assist
in this calculation, a kernel algorthm is applied to map one dimensionality
space to another for easier computation.&lt;/p>
&lt;p>Overall, this paper provides a good understanding of the theory behind SVMs. It
also alludes to additional usages of SVMs and their current problems, but it is
not focused on discussing or resolving them.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/svms-a-practical-consequence-of-learning-theory/</description></item><item><title>A summary of Runnemede: An Architecture for Ubiquitous High-Performance Computing by Nicholas P. Carter et al.</title><link>https://nsynovic.dev/summaries/runnemede-an-architecture-for-ubiquitous-high-performance-computing/</link><pubDate>Fri, 30 Sep 2022 09:07:41 -0500</pubDate><guid>https://nsynovic.dev/summaries/runnemede-an-architecture-for-ubiquitous-high-performance-computing/</guid><description>&lt;h1 id="a-summary-of-runnemede-an-architecture-for-ubiquitous-high-performance-computing">A summary of &lt;em>Runnemede: An Architecture for Ubiquitous High-Performance Computing&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Nicholas P. Carter et al;
&lt;a href="https://doi.org/10.1109/HPCA.2013.6522319">https://doi.org/10.1109/HPCA.2013.6522319&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-runnemede-an-architecture-for-ubiquitous-high-performance-computing">A summary of &lt;em>Runnemede: An Architecture for Ubiquitous High-Performance Computing&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Runnemede: An Architecture for Ubiquitous High-Performance Computing&lt;/em>
by Nicholas P. Carter et al. [1] describes the Runnemede high performance
computing architecture targeting extrene-scale systems. This architecture was
developed for the DARPA&amp;rsquo;s Ubiquitous High-Performance Computing program. The
authors describe multiple facets of the architecture including the networking,
hardware and software design, the energy effeciencies of the architecture. They
also evaluate the performance of the architecture as well. Their mainy
contributions are a theoretical architecture that is well optimized for energy
effiecieny on extra-scale computers.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is a theoretical paper describing an architecture for HPC systems.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>Similar works would involve HPC architecture descriptions as well as low powered
computing architectures as well.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their contributions are a theoretical design and analysis of a HPC architecture
focused on energy effieciency.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Runnemede is a one of four architectures under the DARPA UHPC program.
Additionally, work has been done before to build both low powered cluster
computers, and HPC.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>The justification for this work is that there exists a theory that larger and
larger HPC computers will require more and more power, without fully utilizing
the entire device array. Additionally, a test chip was designed, but never
produced, called &amp;ldquo;Sunshine&amp;rdquo;. By designing this chip, the authors were able to
theoretically test the ideas presented in the paper as well as develop new ones
for the architecture.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the figures and tables are clear and easy to understand.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is well written and clear to understand.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>[2]&lt;/li>
&lt;/ul>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The author&amp;rsquo;s assumed that, &amp;ldquo;&amp;hellip; The power consumed by logic is expected to scale
well as feature sizes shrink, but not as well as transistor density, leading to
the design of &lt;em>overprovisioned, energy-limited&lt;/em> systems that contain more
hardware than they can operate simultanously&amp;rdquo;. In other words, systems will have
more and more &lt;em>power hungry&lt;/em> hardware that cannot be utilized in its entirety.
Additionally, they assume that the current trend with DRAM will cause power
consumption to decrement over time, but not fast enough.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>As the year is 2022, the current next generation hardware from NVIDIA, Intel,
and AMD has been announced, all of which require immense power draw to operate.
Additionally, DDR5 DRAM exists and consumes less power than the previous DDR4
DRAM. Therefore, I agree with the assumptions of the authors.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Runnemede: An Architecture for Ubiquitous High-Performance Computing&lt;/em>
by Nicholas P. Carter et al. [1] describes the Runnemede high performance
computing architecture targeting extrene-scale systems. This architecture was
developed for the DARPA&amp;rsquo;s Ubiquitous High-Performance Computing program to
address overprovisioned, energy limited HPC architecture designs. The authors
proposed a theoretical architecture design, and justify it via benchmarking that
they performed with simulations. Their work assumes (correctly in my opinion)
that systems will continue to require more power to operate in order to achieve
better performance.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/runnemede-an-architecture-for-ubiquitous-high-performance-computing/</description></item><item><title>A summary of ImageNet Classification with Deep Convolutional Neural Networks by Krizhevsky et al.</title><link>https://nsynovic.dev/summaries/imagenet-classification-with-deep-convolutional-neural-networks/</link><pubDate>Thu, 29 Sep 2022 14:33:01 -0500</pubDate><guid>https://nsynovic.dev/summaries/imagenet-classification-with-deep-convolutional-neural-networks/</guid><description>&lt;h1 id="a-summary-of-imagenet-classification-with-deep-convolutional-neural-networks">A summary of &lt;em>ImageNet Classification with Deep Convolutional Neural Networks&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Krizhevsky et al.;
&lt;a href="https://doi.org/10.1145/3065386">https://doi.org/10.1145/3065386&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-imagenet-classification-with-deep-convolutional-neural-networks">A summary of &lt;em>ImageNet Classification with Deep Convolutional Neural Networks&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;li>&lt;a href="#discussion-of-the-proofs">Discussion of the Proofs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>ImageNet Classification with Deep Convolutional Neural Networks&lt;/em> by
Krizhevsky et al. discusses the AlexNet model and its architecture as well as
its SOTA achievements in the 2012 ImageNet Challenge. The difference between
AlexNet and other contestants was that the model relies on GPU training to train
the convulational neural network model. By utilizing the GPU, training time can
be accelerated significatnly more than what was previously possible. Their major
contributions is that a large, deep convulational neural network is capable of
achieving record-breaking resuls via supervised learning. They did not utilize
unsupervised pre-training, but the authors suspect that it would improve the
accuracy of the model.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is a computer vision model evaluation and architecture paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is similar to others that have published about SOTA results from the
ImageNet Challenge.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contributions were that training on GPUs allows for accelerated
training, that large and deep convulutional neural networks are effective at
clasifying images, and that removing layers does decrease the performance of
models. Therefore, a larger, deeper model is applicable. It should be noted that
AlexNet was the largest model ever at the time of publication.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Previous work on designing convulational neural networks and architectures.
However, they were bounded by not being particularly deep.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Because it is one of the key papers that demonstrates that large, deep,
convulational neural networks are effective for image classification. As well as
providing evidence that training on GPUs is not only effective but recommended
for optimal performance. Additionally it provides empirical evidence that
removing a layer from a convolutional neural network is detrimental to the
performance of the model. In other words, the more layers you add, the more
potential there is for improvement.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>Nearly all of the figures are designed well, with the exception of Figure 2.
Figure 2 is the model architecture of AlexNet. This figure suffers from
information density and a three dimensional design which makes it hard to
determin what is going on and in what dimension are images being manipulated.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>They assume that it is because of the larger compute devices and datasets that
make these deep convolutional neural networks possible.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>While true, &lt;a href="going-deeper-with-convolutions.md">Szegedy et al.&lt;/a> designed thier
own architecture using unique algorithms not prevelant in existing convolutional
neural networks.&lt;/p>
&lt;h3 id="discussion-of-the-proofs">Discussion of the Proofs&lt;/h3>
&lt;p>Their training involved both dropout and data augmentation.&lt;/p>
&lt;p>Dropout involves not using the outputs of neurons whose activation is less than
0.5.&lt;/p>
&lt;p>Data augmentation involves manipulating the input images such that 5 244 x 244
images are derived from one 256 x 256 image (e.g. the four corners and one
centered). Additionally, PCA was done on the RGB channels of all of the images
in the ImageNet 2010 and 2012 datasets. These eigenvectors were then added to
each of the images respective color channels.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>A reimplemenation of the work would be interesting, with particular respect to
benchmarking training time, as the authors were limited by their GPU compute
units&amp;rsquo; performance.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>&lt;em>Taken from &lt;a href="#first-pass">First Pass&lt;/a>&lt;/em>&lt;/p>
&lt;p>The paper &lt;em>ImageNet Classification with Deep Convolutional Neural Networks&lt;/em> by
Krizhevsky et al. [1] discusses the AlexNet model and its architecture as well
as its SOTA achievements in the 2012 ImageNet Challenge. The difference between
AlexNet and other contestants was that the model relies on GPU training to train
the convulational neural network model as well as being a deep convolutional
neural network.&lt;/p>
&lt;p>By utilizing the GPU, training time can be accelerated significatnly more than
what was previously possible. The benefits of being a deep convolutional neural
network is that the classification of images builds off of the features found in
the previous images. The result of this is that their top 1% and top 5% error
were the lowest ever in the competition.&lt;/p>
&lt;p>They trained their model by utilizing both dropout, where neurons that activated
with a value less than 0.5 are not inputted into the next layer, and by
augmenting the Imagenet 2010 and 2012 datasets to increase the amount of data
that they can throw at the model.&lt;/p>
&lt;p>Their work is important as it kicked off the usage of both deep convolutional
neural networks and the usage of GPUs to reduce training time.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/imagenet-classification-with-deep-convolutional-neural-networks/</description></item><item><title>A summary of Very Deep Convolutional Networks for Large-Scale Image Recognition by Karen Simonyan and Andrew Zisserman</title><link>https://nsynovic.dev/summaries/very-deep-convolutional-networks-for-large-scale-image-recognition/</link><pubDate>Wed, 28 Sep 2022 22:40:46 -0500</pubDate><guid>https://nsynovic.dev/summaries/very-deep-convolutional-networks-for-large-scale-image-recognition/</guid><description>&lt;h1 id="a-summary-of-very-deep-convolutional-networks-for-large-scale-image-recognition-by-karen-simonyan-and-andrew-zisserman">A summary of &lt;em>Very Deep Convolutional Networks for Large-Scale Image Recognition&lt;/em> by Karen Simonyan and Andrew Zisserman&lt;/h1>
&lt;blockquote>
&lt;p>Karen Simonyan and Andrew Zisserman;
&lt;a href="https://doi.org/10.48550/arXiv.1409.1556">https://doi.org/10.48550/arXiv.1409.1556&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-very-deep-convolutional-networks-for-large-scale-image-recognition-by-karen-simonyan-and-andrew-zisserman">A summary of &lt;em>Very Deep Convolutional Networks for Large-Scale Image Recognition&lt;/em> by Karen Simonyan and Andrew Zisserman&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Very Deep Convolutional Networks for Large Scale Image Recognition&lt;/em>
by Karen Simonyan and Andrew Zissernman discusses the SOTA performance of their
model in the 2014 ImageNet Challenge on localisation and classification tasks.
They discuss that be extending the depth of convolutional neural networks to 16
up to 19 layers, with a 3x3 filter size, SOTA performance is possible without
redeveloping the architecture of existing convolutional neural networks. This is
in contrast to &lt;a href="going-deeper-with-convolutions.md">Szegedy&amp;rsquo;s work&lt;/a> who proposes
the Inception architecture for classification and object detection; with which
the reference implementation also came first in the 2014 ImageNet Challenge in
its respective tasks. Simoyan et al. discuss the architecture and training that
went into their model (VGG) and how to architect future models to perform as
well or better.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is both a computer vision model evaluation and architecture paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is most closely related to others who publish work regarding SOTA
performance on CV architecture and models.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contributions is an exploration of depth in traditional convulational
neural networks to achieve SOTA performance.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Prior work has gone into optimizing the width and intial convulations of
convultional neural networks.&lt;/p>
&lt;p>&lt;a href="going-deeper-with-convolutions.md">Szegedy et al.&lt;/a> proposed a new architecture
(Inception) that achieved SOTA performance in the 2014 ImageNet Challenge. Else,
Krizhevsky et al. [2] and others have proposed improvments to the
convulational neural network architecture.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about the authors work as increasing the depth of a neural
network by their proposed architecture allows for easy expansion of existing
convulational neural networks without redesigning the libraries used to create
them.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The tables that are presented are easy to read, but can be improved upon. Often,
multiple rows will correspond with a single model configuration. This is fine,
however, it is difficult to make out what configuration each row corresponds to.
Additionally, the tables make comparing error percentages easy across model
configurations.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is well written and can be understood.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>Classical convultional neural network architecture - [3]&lt;/li>
&lt;li>GoogLeNet - [2]&lt;/li>
&lt;li>Clarifai&lt;/li>
&lt;li>ImageNet classification with deep convolutional neural net- works [4]&lt;/li>
&lt;li>Isotropically-rescaled training image&lt;/li>
&lt;li>ImageNet 2013 submissions - [5], [6] Localization and Detection using
Convolutional Networks&lt;/li>
&lt;/ul>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors assume that the performance improvements that convulational neural
networks are achieving are based off of larger datasets and better compute
optimization.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>I agree with their assumption. However, [2] created a SOTA model utilizing a
new architecture, rather than improving upon an existing one.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I would love to try and optimize the input layer of convulational neural
networks by having a computation that not only looks at the color space, but
also the opacity of an image. This would allow for images to have their
background removed for the purposes of classification by making the background
less opaque than the foreground.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Very Deep Convolutional Networks for Large Scale Image Recognition&lt;/em>
by Karen Simonyan and Andrew Zissernman discusses the SOTA performance of their
model in the 2014 ImageNet Challenge on localisation and classification tasks.
They discuss that be extending the depth of convolutional neural networks to 16
up to 19 layers, with a 3x3 filter size, SOTA performance is possible without
redeveloping the architecture of existing convolutional neural networks. Their
work builds of previous efforts of improving convulational neural network
performance by optimizing the filter size and intial layer, but contrasts
contemporaries [2] by not developing a new architecture. Their work has
importance as it shows that the existing convulational neural network
architecture is capable of SOTA performance by increasing the depth of the
model. They justify this by trying six different model configurations, and
finding that models with 16 to 19 layers performed best on the 2014 ImageNet
Challenge classification and localisation challenges.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/very-deep-convolutional-networks-for-large-scale-image-recognition/</description></item><item><title>A summary of Going deeper with convolutions by Christian Szegedy et al..</title><link>https://nsynovic.dev/summaries/going-deeper-with-convolutions/</link><pubDate>Wed, 28 Sep 2022 20:07:40 -0500</pubDate><guid>https://nsynovic.dev/summaries/going-deeper-with-convolutions/</guid><description>&lt;h1 id="a-summary-of-going-deeper-with-convolutions">A summary of &lt;em>Going deeper with convolutions&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Christian Szegedy et al.;
&lt;a href="https://doi.org/10.48550/arXiv.1409.4842">https://doi.org/10.48550/arXiv.1409.4842&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-going-deeper-with-convolutions">A summary of &lt;em>Going deeper with convolutions&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Going deeper with convolutions&lt;/em> by Christian Szegedy et al. [1]
describes a 2014 state of the art computer vision model (on the ImageNet
Large-Scale Visual Recognition Challenge) called GoogLeNet architected based on
Hebbian principles (i.e. neruons that fire together, are wired together)and a
constant computational budget. Their approach relies on creative algorithms and
neuroscience principles and aims to be a more power effiecient model for mobile
devices by limiting the computations during infrencing. Additionally, their
model is deep but not wide and is considered &amp;ldquo;sparse&amp;rdquo; by the authors. In other
words, there are as few nodes as possible within the neural network.&lt;/p>
&lt;p>Szegedy et al.&amp;rsquo;s contributions are a state of the art computer vision model that
provides experimental evidence that, &amp;ldquo;&amp;hellip; Approximating the expected optimal
sparse structure by readily avilible dense building blocks is a viable method
for improving neural networks for computer vision&amp;rdquo;. This means that this model
proves that dense neural networks for computer vision are not necessary in the
author&amp;rsquo;s viewpoint.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is a computer vision paper describing both a machine learning architecture
and refernce model.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is related to other computer vision papers that achieve state of the
art performance values based on the ImageNet Large-Scale Visual Recognition
Challenge.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Szegedy et al.&amp;rsquo;s contributions are:&lt;/p>
&lt;ul>
&lt;li>A computer vision model architecture (Inception) that is both sparse and aims
to be computationally efficient on mobile (non-server) devices,&lt;/li>
&lt;li>A reference model of the aforementioned computer vision model architecture&lt;/li>
&lt;li>A comparison of previous state of the art work to justify their claims that
sparser networks are the future of computer vision models.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the tables have proper column labels. However, Table 1 does not provide
default values for blank cells. This is most likely due to the layer type not
performaing a specific operation (as described in the column label). Regardless,
the remaining tables look good.&lt;/p>
&lt;p>Additionally, Figure 3 is very clear to read, if a little dense. However, as it
describes all of the layers of GoogLeNet and how they are connected, I find the
size to be appropriate.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is fairly well written. The only complaints that I have are minor
grammatical mistakes that the author&amp;rsquo;s left in (by accident I assume).
Additionally, that the authors didn&amp;rsquo;t optimize their tables and figures to
better fit on the pages. As tables and figures are stacked on top of one
another, it would be possible to reclaim paper space by rearranging multiple
tables and figures to be next to one another, with the exception of Figure 3 due
to the sheer size of it.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>Contrast Normalization&lt;/li>
&lt;li>Max Pooling&lt;/li>
&lt;li>Average Pooling&lt;/li>
&lt;li>Softmax Activation&lt;/li>
&lt;li>Dropout - [2]&lt;/li>
&lt;li>Localization Task - [3]&lt;/li>
&lt;li>Gabor Filters - [4]&lt;/li>
&lt;li>Network in Network - [5]&lt;/li>
&lt;li>Rectified Linear Activation - [6]&lt;/li>
&lt;li>Regions with Convolutional Neural Networks - [7]&lt;/li>
&lt;li>Multi-Box Prediction - [8]&lt;/li>
&lt;li>Arora proof - [9]&lt;/li>
&lt;li>LeNet 5 - [10]&lt;/li>
&lt;li>Fisher vectors&lt;/li>
&lt;li>Polyak Averaging - [11]&lt;/li>
&lt;li>Jaccard index&lt;/li>
&lt;li>Selective Search - [12]&lt;/li>
&lt;li>Photometric Distortions - [13]&lt;/li>
&lt;/ul>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>One assumption that the authors make is that overfitting is more prone to occur
in large models. Additionally, overfitting can occur when there is not enough
labeled examples in a dataset when a large model is training. Furthermore,
increasing the size of a model increases the number of computations that must be
done between layers (e.g. chaining two convulational layers results in
computation cost quadratically increasing) Their solutions relies on moving from
fully connected to sparsely connected architectures including within
convolutional layers. Also, their model architecutre is based on the idea that
computers are inefficent when, &amp;ldquo;&amp;hellip; Computing numerical calculations on
non-uniform sparse data structures&amp;rdquo;.&lt;/p>
&lt;p>They assume that 1x1, 3x3, and 5x5 filters are the proper filters to use, but
did not test other size of filters. They also assume that using, &amp;ldquo;Inception
modules&amp;rdquo; is only useful at higher levels, whereas the initial levels are
standard convultational levels. However, this was not tested either and was due
to, &amp;ldquo;infrastructural inefficiences&amp;rdquo; in the implementation.&lt;/p>
&lt;p>Finally, that the model that achieved state of the art performance was the best
model. The authors had been training and testing other models for months prior,
however, it is unclear what the testing methodology was and why a particular
model was choosen to compete in the ImageNet competition.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>The first paragraph of assumptions seems reasonable and correct. However, the
remaining two paragraphs seem unreasonable. This is due to the lack of testing
that the author&amp;rsquo;s put in when optimizing their model with respect to the filter
sizes and choosing models. Furthermore, if testing did occur to address these
issues, it is not addressed in this paper, thus leaving the reader to wonder why
testing wasn&amp;rsquo;t performed.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>Based off of Section 3 (Motiviation and High Level Considerations), one
promising area of study would be to perform a network architecture search
utilizing the principles and reasoning of their approach to other machine
learning and computer vision domains.&lt;/p>
&lt;p>An enhancement to their work is possible by analyzing what filter sizes most
optimal improve performance. Currently the author&amp;rsquo;s are restricting GoogLeNet to
1x1, 3x3, and 5x5 filter sizes, but this was due to convience and no data was
given to support this.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Going deeper with convolutions&lt;/em> by Szegedy et al. [1] introduces a
computer vision model architecture called Inception and a reference model called
GoogLeNet.&lt;/p>
&lt;p>Inception is a model architecuture that is both sparse and (attempts to be)
computationally efficent during inferencing with only 1.5 billion multiply-add
operations allowed. Inception models are composed of multiple Inception modules
that are stacked on top of each other. Each Inception module takes in data from
the previous layer and passes it into small convultional filters (i.e. 1x1
typically). There are three of these small filters that are wired to the input
of the Inception module, with one of them connected directly to the output. The
outputs of two of these filters are then passed into larger filters (i.e. 5x5)
to which it is then passed into a DepthConcat function. Additionally, a 3x3
filter is wired to the input of the module and the output of which goes into a
1x1 filter to be passed into the DepthConcat function as well. From there, it is
passed into another Inception module and the process repeats.&lt;/p>
&lt;p>&lt;strong>Note:&lt;/strong> Depth when referring to two dimensional images refers to the color
channel of the image. As images typically have three color channels (i.e. red,
green, blue), an image would have a depth of 3.&lt;/p>
&lt;p>&lt;em>Example:&lt;/em> A 200 pixel by 200 pixel full color spectrum image would be
represented as 200x200x3.&lt;/p>
&lt;p>It is possible for Inception modules to have an additional connection to the
input of the module to perform average pooling for softmax activation.&lt;/p>
&lt;p>GoogLeNet achieved SOTA performance in the ImageNet Large-Scale Visual
Recognition Challenge image classification task by having a top-5 error of 6.67%
on both the validation and testing data. This is an improvment of 56.5% in
comparison to 2012&amp;rsquo;s SOTA performer (SuperVision) and 2013&amp;rsquo;s SOTA performer
(Clarifai). Additionally, they achieved SOTA perfomance for the ImageNet
Large-Scale Visual Recognition Challenge detection task with a mean average
precision of 43.9% utilizing an ensemble inference approach. This model was
architected using the Inception architecture with 22 layers. However, not every
layer was an Inception module; the first few layers were standard convulational
layers.&lt;/p>
&lt;p>The author&amp;rsquo;s contributions were as follows;&lt;/p>
&lt;ol>
&lt;li>The Inception computer vision architecture,&lt;/li>
&lt;li>The GoogLeNet SOTA computer vision model for classification and object
detection.&lt;/li>
&lt;/ol>
&lt;p>My opinion on this paper is that while it is well written, the author&amp;rsquo;s make
numerous assumptions about the optimal performance of their model&amp;rsquo;s
architecture. They don&amp;rsquo;t test optimal sizes for filters as well as resolving
bugs such as the usage of standard convulational layers early in the model. Both
of which can be solved by performing a neural architecture search.&lt;/p>
&lt;p>Future work for this paper would involve optimizing the model architecture via a
neural architecture search. As well as evaluating the performance of the model
by both increasing and decreasing the depth of the model.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/going-deeper-with-convolutions/</description></item><item><title>A summary of How to Read a Paper by S. Keshav</title><link>https://nsynovic.dev/summaries/how-to-read-a-paper/</link><pubDate>Wed, 28 Sep 2022 15:11:09 -0500</pubDate><guid>https://nsynovic.dev/summaries/how-to-read-a-paper/</guid><description>&lt;h1 id="a-summary-of-how-to-read-a-paper">A summary of &lt;em>How to Read a Paper&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>S. Keshav;
&lt;a href="https://doi.org/10.1145/1273445.1273458">https://doi.org/10.1145/1273445.1273458&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-how-to-read-a-paper">A summary of &lt;em>How to Read a Paper&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#discussion-of-the-proofs">Discussion of the Proofs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#how-would-i-present-the-ideas">How Would I Present the Idea(s)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The paper, &lt;em>How to Read a Paper&lt;/em> by S. Keshav is a tutorial for graduate
students on how to read an academic paper. They propose a &amp;ldquo;three-pass&amp;rdquo; approach
that aims to reduce the frustration that graduate students face when reading
papers. Additionally, they discuss how to perform a literature survey of a new
field, their experience with this methodology, and write that this document is
meant to exist as a living work, with adjustments to be made as seen fit by the
author.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is definetly a more causal piece of academic work that aims at easing
students into the reading papers. I would classify this paper as &amp;ldquo;meta&amp;rdquo;,
educational, or as a formal letter to students. The later classification is due
to the lack of surveys or qualitative/ quantitative data from others that have
applied this or similar methods to reading papers.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is most closely related to papers that discuss the writing of
academic works and the review process of academic works.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>The assumptions in the abstract and introduction seem reasonable. However,
assuming that only graduate students are the only ones that struggle with
reading academic works is unrepresentative of &lt;em>my particular experinece&lt;/em>.
Undergraduate students as well as professionals in industry also struggle with
reading these works as well.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>S. Keshav&amp;rsquo;s contributions is a three-stage process for reading papers and a
framework for performing literature reviews of a new field.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is well written and is easy to comprehand. I would strongly recommend
this paper to be read by everyone regardless of academic status.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>There are no illustrations to discuss in this paper.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>[1], [2], [3], and [4]&lt;/li>
&lt;/ul>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;m nit-picking here, but the S. Keshav focuses solely on graduate students as
the demographic that has trouble reading academic papers. Now while graduate
students do typically read more papers than undergraduates, it is not unheard of
for academic readings to be given to undergraduate students as homework
assingments or for them to read them on their own. Addotionally, professionals
in industry also struggle with this task as well. A more inclusive audience
would have been appreciated, but would not have improved the content or quality
of this paper.&lt;/p>
&lt;h3 id="discussion-of-the-proofs">Discussion of the Proofs&lt;/h3>
&lt;p>The only proof of the &amp;ldquo;three-pass&amp;rdquo; method that was discussed was the experience
of the author. An awfully biased proof, however, I do appreciate at least some
quantifiable data for this method.&lt;/p>
&lt;h3 id="how-would-i-present-the-ideas">How Would I Present the Idea(s)&lt;/h3>
&lt;p>I think the author presented these ideas exceptionally well and clearly, and
cannot think of any additional presenation method aside from the critiques of
the assumptions mentioned in &lt;a href="#author-assumptions">Author Assumptions&lt;/a>.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>A survey of graduate students on their methodology for reading papers&lt;/li>
&lt;li>A survey of industry professionals on their methodology for reading papers&lt;/li>
&lt;li>An artifact that allows for a user to step through a set series of steps to
properly understand a document.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>How to Read a Paper&lt;/em> by S. Keshav is a &amp;ldquo;meta&amp;rdquo; or educational paper
about how to read an academic work. Their main contributions are a three step
process on how to read a paper, as well as a framework for performing a
literature review in a new field. This three step process involves a:&lt;/p>
&lt;ol>
&lt;li>Bird&amp;rsquo;s Eye View of the paper where only the Title, Abstract, Introduction,
Conclusion, and section and sub-section headings are read first,&lt;/li>
&lt;li>A deeper analysis of figures and content of the paper which involves finding
new, unread references to the reader and evaluating the quality of
illustrations to determine the quality of the paper,&lt;/li>
&lt;li>A virtual reimplemenation of the paper where every claim of the paper is
analyzed and critiqued; typically this done by reviewers or those that are
doing a deeper analysis of the work.&lt;/li>
&lt;/ol>
&lt;p>I can see this process being useful for researchers as reimplementers of other&amp;rsquo;s
research must accomplish all three steps to properly appreciate and understand
what they need to do to perform their task. As for the literature review
framework, it involves utilizing academic search engines (e.g.
&lt;a href="https://https://scholar.google.com/">Google Scholar&lt;/a>) to find work within a
particular field, finding shared citations or authors within that field, then
evaluating top confrences within that field to see who the top researchers and
research topics are within that field. For exploratory research, this is both an
extremely simple and effective framework to follow and adapt to different
domains.&lt;/p>
&lt;p>However, S. Keshav does limit the reach of this paper by making it focus solely
on the woes of graduate students. This is inaccurate of the wider academic
readership, as more and more frequently undergraduate and industry professionals
are reading academic papers both for pleasure and for utilization in
assignments. This paper can easily become more inclusive of wider audiences
without changing the content in an updated version of this document. This would
make sense as the author has requested that this paper be treated as a living
document that can be subject to change as the author adapts his process and
framework for academic review.&lt;/p>
&lt;p>I would personally like to see this work be quantified in surveys and
implemented as artifacts that ensure that readers are properly following the
review method that the author has laid out.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/how-to-read-a-paper/</description></item><item><title>Hugo, Gopher, Gemini, and Annoyances</title><link>https://nsynovic.dev/posts/hugo-gopher-gemini-annoyance/</link><pubDate>Thu, 04 Aug 2022 08:38:05 -0500</pubDate><guid>https://nsynovic.dev/posts/hugo-gopher-gemini-annoyance/</guid><description>&lt;h1 id="hugo-gopher-gemini-and-annoyances">Hugo, Gopher, Gemini, and Annoyances&lt;/h1>
&lt;blockquote>
&lt;p>My story with setting up a Gopher and Gemini compatible website powered by
Hugo.&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;p>A little while ago, I wanted to create my own website. I struggle with creating
frontends for websites, so I choose to use the Hugo [0] static site generator
as it has many great themes. Learning Hugo was simple enough since the most
basic sites are just &lt;code>md&lt;/code> content files and &lt;code>toml&lt;/code> config files. So, after a
brief learning period, I built my website, got it hosted on GitHub Pages [1],
and setup a domain from Google Domains [2] to link to it all.&lt;/p>
&lt;p>That should be it, right?&lt;/p>
&lt;p>Well then I saw these two videos from DistroTube [3] [4] and got interested
in creating and hosting my own Gopher [5] phlog and Gemini [6] capsule. This
is where the annoyances began.&lt;/p>
&lt;h2 id="gopher-the-bane-of-my-existence-for-about-a-week">Gopher: The Bane of my Existence (for about a week)&lt;/h2>
&lt;p>I wanted to create a Gopher phlog because I thought it was just plain cool. Who
wouldn&amp;rsquo;t want a text only mirror of their site not indexed by modern search
engines (probably everyone)? I was aware the Hugo could output my website in a
variety of different formats. But, I had no idea how to do that.&lt;/p>
&lt;p>Luckily, Jason F. McBrayer had already figured this out in a blog post on his
website [7]. So I copied and pasted away. I ran the Hugo site generator and I
got output! Huzzah! But when I tested the site with the &lt;code>gophernicus&lt;/code> [8]
server, the site was malformed. Links to blog pages lead to nowhere or returned
error message.&lt;/p>
&lt;p>So I deleted my edits, re-copied, re-pasted (?), and ran the generator. Still,
no dice. Even manually typing in his work into my project didn&amp;rsquo;t work.&lt;/p>
&lt;p>At this point it was getting late, so I put the project into the back of my head
and would continue to work on it for the next week. My stuborness was because I
knew it was possible to do this conversion, after all, I had gotten output. But
there was a syntax error somewhere that I just couldn&amp;rsquo;t find.&lt;/p>
&lt;p>Eventually, I found documentation for the &lt;code>gophermap&lt;/code> syntax [9] and it all
clicked. I use &lt;code>neovim&lt;/code> [10] as my editor of choice. I use spaces instead of
tabs. &lt;code>gophermaps&lt;/code>, as a product of the 90s, require tabs for link formatting.&lt;/p>
&lt;p>Insert face palm here.&lt;/p>
&lt;p>The fix for this problem was to &lt;code>:set noexpandtab&lt;/code> and then paste in McBrayer&amp;rsquo;s
work.&lt;/p>
&lt;p>After that, the links worked. I then spent some time setting the Gopher template
that he provided to what I wanted, but that was minor work.&lt;/p>
&lt;p>In all, it took me a week to RTFM and move on from this arguably ridiculous
project.&lt;/p>
&lt;h2 id="gemini-a-lot-easier-than-gopher">Gemini: A lot easier than Gopher&lt;/h2>
&lt;p>It was significantly easier to create the Gemini capsule than a Gopher Phlog.&lt;/p>
&lt;p>But before that, I had to create the templates and output settings for Gemini in
Hugo. This, again, was taken care of by Sylvain Durand [11]. With my
&lt;code>:set noexpandtab&lt;/code> option, I copied and pasted his work into my project&amp;hellip; And
boom! It worked.&lt;/p>
&lt;p>That was anticlimatic wasn&amp;rsquo;t it? Again, some formatting of the provided template
was necessary, but that was minor.&lt;/p>
&lt;p>The new problem, was how to distribute this site.&lt;/p>
&lt;h2 id="pubnix-whats-old-is-new-again">PubNix: What&amp;rsquo;s Old is New Again&lt;/h2>
&lt;p>In my research of both the Gopher and Gemini protocols, I found out about
PubNixs: community ran servers that run on low powered machines. These servers
are a modern implementation of the time sharing servers of yore where there were
a few large mainframes in the country that only a few people could log into at a
time. A subset of these PubNixs is the &lt;code>tildeverse&lt;/code> [12]. These are &lt;strong>very&lt;/strong>
community oriented servers that offer free webhosting of the Gemini, Gopher,
HTTP, and Spartan sites. All that&amp;rsquo;s required is to create an account and start
uploading.&lt;/p>
&lt;p>For those that are interested in hosting a Gemini and/or Gopher site, I
encourage the usage of these servers as it takes the problems of hosting content
(maintainence, privacy, port forwarding, etc.) off of your shoulders&lt;/p>
&lt;h2 id="the-end">The End&lt;/h2>
&lt;p>If you ware interested in doing something similar, here is a link to my websites
source code on GitHub [13]. There you can see my templates, scripts, and
config options for generating Gemini, Gopher and HTTP sites. Best of luck, and
happy hacking!&lt;/p>
&lt;hr>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/posts/hugo-gopher-gemini-annoyance/</description></item><item><title>Why Sign Commits?</title><link>https://nsynovic.dev/posts/why-sign-commits/</link><pubDate>Mon, 01 Aug 2022 15:38:48 -0500</pubDate><guid>https://nsynovic.dev/posts/why-sign-commits/</guid><description>&lt;h1 id="why-sign-commits">Why Sign Commits?&lt;/h1>
&lt;blockquote>
&lt;p>Why should you sign your commits?&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;h2 id="preface">Preface&lt;/h2>
&lt;p>I recently read this article [0] by Alessandro Segala about why I should sign
my commits. And I completely agree with, and would like to expand upon, their
work.&lt;/p>
&lt;h2 id="identify-theft">Identify Theft&lt;/h2>
&lt;p>Identify theft is not a joke [1].&lt;/p>
&lt;p>The FTC in their 2021 edition of the CSN Annual Data Book [2] reported that
there were 1,434,676 reports of identity fraud in 2021 This theft allows the
perpetrator to commit acts of fraud in your name while reaping the benefits. As
developers, we not only have to protect our real world identities from theft,
but our digital ones as well. And while it is important to have strong and
secure passwords, I&amp;rsquo;m not referring to your accounts as digital identities. I&amp;rsquo;m
instead talking about your contributions to open source projects.&lt;/p>
&lt;p>This article focusses around &lt;code>git&lt;/code> [3] and online version control systems
(VCSs) that implement &lt;code>git&lt;/code> as their backend.&lt;/p>
&lt;p>It is not only possible, but increadibly easy to sign a commit under a different
identity. In addition, online VCSs will read the &lt;code>git&lt;/code> commit history and per
commit, add the appropriate account information to the commit (assuming an
account exists with the email address that is attached ot the &lt;code>git&lt;/code> repository).
This feature, is meant to provide a user friendly way of viewing &lt;code>git&lt;/code> commits.
However, it also allows for an attacker to take advantage of these tools and
publish commits to a project under someone else&amp;rsquo;s identity.&lt;/p>
&lt;h2 id="the-dangers-of-developer-identity-theft">The Dangers of Developer Identity Theft&lt;/h2>
&lt;p>The biggest threat to a developer who doesn&amp;rsquo;t sign their commits is the lack of
trust a community can have for a particular developer.&lt;/p>
&lt;p>A malicious attacker who signs off on infected, poorly written, or malformed
commits and publishes to a project can ruin a developer&amp;rsquo;s relationship to a
community.&lt;/p>
&lt;p>A malicious attacker could publish commits that actively ruin existing features.
They could also introduce bugs into a repository under someone&amp;rsquo;s name.&lt;/p>
&lt;h2 id="benefits-of-signing">Benefits of Signing&lt;/h2>
&lt;p>To combat this, &lt;code>git&lt;/code> allows for individuals to sign their commits with a GPG
[4] key.&lt;/p>
&lt;p>This allows for a number of benefits:&lt;/p>
&lt;ol>
&lt;li>Commits in the &lt;code>git&lt;/code> history that are signed have metadata attached to them
saying that they&amp;rsquo;re signed.&lt;/li>
&lt;li>If the GPG key is published to an online VCS that supports this feature, a
&lt;em>verified&lt;/em> tag will be applied to commits that are signed and match a user&amp;rsquo;s
GPG key.&lt;/li>
&lt;li>Developer identity can be confirmed by running checks against the public
facing key of a commit and a developer&amp;rsquo;s private key.&lt;/li>
&lt;/ol>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>Since reading [0], I have implemented commit signing for my project going
forward. I also now require all group projects to have signed commits prior to
acceptance.&lt;/p>
&lt;p>Setting up signed commits was trivial, and there were plenty of guides [0]
[5] [6] on how to do so.&lt;/p>
&lt;p>I strongly encourage all developers to sign their commits in order to improve
the verification of work done by legitimate developers, instead of allowing the
work of theives to perforate throughout our community.&lt;/p>
&lt;hr>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/posts/why-sign-commits/</description></item><item><title>Hello World</title><link>https://nsynovic.dev/posts/hello-world/</link><pubDate>Thu, 28 Jul 2022 19:47:48 -0500</pubDate><guid>https://nsynovic.dev/posts/hello-world/</guid><description>&lt;h1 id="hello-world">Hello World&lt;/h1>
&lt;blockquote>
&lt;p>An introduction to me, this site, and the content I want to make.&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;h2 id="hi-there">Hi there!&lt;/h2>
&lt;p>My name is Nicholas M. Synovic.&lt;/p>
&lt;p>At the time of writing, I&amp;rsquo;m a Research Assistant at Loyola University Chicago
[0] with the Software and Systems Laboratory [1]. I graduated Loyola in May
of 2022 with a B.S in Computer Science. Currently, I&amp;rsquo;m pursuing a M.S in
Computer Science with a concentration in Artificial Intelligence. My
concentration is more specifically targetting low power computer vision systems.&lt;/p>
&lt;p>As an undergrad, I pursued and lead Software Engineering Metrics research and
was successfully able to publish a paper into the IEEE ASE 2022 [2]
confrence&amp;rsquo;s tools track as the lead author [3]. In addition, I was fortunate
enough to be an author on a Software Engineering paper focussing on deep
learning model reuse with researchers from Purude University [4]. I currently
work on both Software Engineering and Computer Vision research.&lt;/p>
&lt;p>In addition to my academic work, I have experience leading IT teams and
developing full stack software. I was one of the first in my high school to join
the student IT program which performed real world maintaince on my district&amp;rsquo;s
network, personal, and tech infrastructure [5]. As a 2nd year student at
Loyola, I won second place at the Fall Student Showcase for my work in
identifying and providing solution for Congress.gov&amp;rsquo;s search functionality as
well as creating a dataset of all the content on Congress.gov [6]. And as a
4th year senior, I developed an application for Medline Industries [7] to
facilitate their corporate wide Talent Planning Process. This application was
used to assess ~30,000 employees in 2022 to find the next generation of leaders.&lt;/p>
&lt;p>On the social side of things, I am currently a board member of the Loyola AI
Club [8] as well as a student ambassador for Intel&amp;rsquo;s OneAPI platform [9].&lt;/p>
&lt;p>For further information on how to contact me, see my Contact page [10].&lt;/p>
&lt;h2 id="whats-up-with-this-site">What&amp;rsquo;s up with this site?&lt;/h2>
&lt;p>This site is generated from Markdown using Hugo [11] and the Ananke theme
[12].&lt;/p>
&lt;p>This decision was made due to Hugo&amp;rsquo;s extensive templating feature, which allows
me to accessible to those using HTTP [13], Gopher [14], and Gemini [15]
protocols. I intend to use this site to experiment with other alternative
network protocols as they are developed.&lt;/p>
&lt;p>This site is also mirrored across several different services. Currently, it is
hosted on both GitHub Pages [16] as &lt;code>https://nsynovic.dev&lt;/code> [17]. Blog posts
are mirrored on my the Gopher and Gemini versions of this site, as well as my
Dev.to [18] page. In addition, I intend to use my account at &lt;code>tilde.team&lt;/code>
[19] to add an additional backup of the HTTP site there as well.&lt;/p>
&lt;h2 id="what-content-is-coming">What content is coming?&lt;/h2>
&lt;p>I don&amp;rsquo;t have a set plan at the moment, however, I intend to provide less
academicly leaning posts on this site. Think more opinion pieces, tutorials, or
other ramblings. Primarily text based content will be posted on this site as
there are other platforms that are better at hosting multimedia content.&lt;/p>
&lt;hr>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/posts/hello-world/</description></item><item><title>Contact</title><link>https://nsynovic.dev/contact/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nsynovic.dev/contact/</guid><description>&lt;h1 id="contact-me">Contact Me&lt;/h1>
&lt;blockquote>
&lt;p>How to Contact Me&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;h2 id="social-media">Social Media&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.facebook.com/nsynovic">Facebook&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://twitter.com/nick_synovic">Twitter&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.instagram.com/nicholas_synovic/">Instagram&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/channel/UCVG2VYy7GJ86BLiUaj5LEZQ">Youtube&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.linkedin.com/in/nsynovic/">LinkedIn&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/NicholasSynovic">GitHub&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="blogging-platforms">Blogging Platforms&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://dev.to/nicholassynovic">Dev.to&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="mirrors-of-this-site">Mirrors of this Site&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://tilde.team/~nosnow">https://tilde.team/~nosnow&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="alternative-protocols">Alternative Protocols&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://nsynovic.dev/posts/feed.xml">RSS&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://tilde.team/~nosnow/posts/feed.xml">RSS #2&lt;/a>&lt;/li>
&lt;li>&lt;a href="gopher://tilde.team/1/~nosnow">Gopher&lt;/a>&lt;/li>
&lt;li>&lt;a href="gemini://tilde.team/~nosnow">Gemini&lt;/a>&lt;/li>
&lt;li>&lt;a href="spartan://tilde.team/~nosnow">Spartan&lt;/a>&lt;/li>
&lt;/ul>
Citations availible at https://nsynovic.dev/contact/</description></item></channel></rss>