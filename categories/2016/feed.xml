<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2016 on Nicholas M. Synovic</title><link>https://nsynovic.dev/categories/2016/</link><description>Recent content in 2016 on Nicholas M. Synovic</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Nicholas M. Synovic</copyright><lastBuildDate>Fri, 02 Dec 2022 21:50:54 -0600</lastBuildDate><atom:link href="https://nsynovic.dev/categories/2016/feed.xml" rel="self" type="application/rss+xml"/><item><title>A summary of You Only Look Once: Unified, Real-Time Object Detection by Joseph Redmon et al.</title><link>https://nsynovic.dev/summaries/you-only-look-once-unified-real0time-object-detection/</link><pubDate>Fri, 02 Dec 2022 21:50:54 -0600</pubDate><guid>https://nsynovic.dev/summaries/you-only-look-once-unified-real0time-object-detection/</guid><description>&lt;h1 id="a-summary-of-you-only-look-once-unified-real-time-object-detection">A summary of &lt;em>You Only Look Once: Unified, Real-Time Object Detection&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Joseph Redmon et al. CVPR, 2016 &lt;a href="https://doi.org/10.1109/CVPR.2016.91">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-you-only-look-once-unified-real-time-object-detection">A summary of &lt;em>You Only Look Once: Unified, Real-Time Object Detection&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>The authors wanted to create a very fast object detection network that handles
object detection using both region proposals and class probability maps in one
unified model.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Because this model was the fastest object detection model of its time with being
able to infrence at 45 FPS or 155 FPS.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is an object detection computer vision paper&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is related to work in real time object detection.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contribution is a model architecture (implemented with VGG) that is
very fast at performing object detection in real time (45 FPS) or super fast
(155 FPS) at the cost of accuracy.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done to develop real time object detection systems as well as
region proposal based object detection models.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The figures and tables in this paper are clear and easy to understand.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is well written.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors tested variations of their model on the PASCAL VOC dataset against
other SOTA models and measured the MAP percentage of the results. Furthermore,
they measured the real time object detection performance of their model
variations as well.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>YOLO trades accuracy for speed which the author&amp;rsquo;s argue in their Introduction is
applicable to the domain of self driving cars.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>While infrence speed is a necessity within that domain, I would argue that
accuracy is more important, as I would want a system that could detect a stop
sign in front of the car accurately but slower, than quickly but innacurately.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>This work has undergone many revisions since its initial publication. I&amp;rsquo;d like
to review the enhancements that were made to the model since this initial
publication and see what was changed and understand why.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>Is this model bottlenecked by the number of classes that it has to look at
and/or understand?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>This was a really good paper. I would encourage further work in this field and
specifically to test this model out on low powered devices.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>You Only Look Once: Unified, Real-Time Object Detection&lt;/em> by Joseph
Redmon et al. [1] describes an object detection strategy that aims to
outperform previous methods &lt;strong>in terms of infrence speed measured in FPS&lt;/strong> by
creating a unified model that can perform region proposals and class probability
mapping to a source image in parallel. This model is called YOLO.&lt;/p>
&lt;p>The authors of this paepr were able to accomplis this by using the following
technique:&lt;/p>
&lt;ol>
&lt;li>Divide the source image into many sub-sections.&lt;/li>
&lt;li>In parallel, compute the regions of interest as well as the class probability
mapping for each sub-section.&lt;/li>
&lt;li>Predict the class label and bounding boxes at the final layer by analyzing
the overlap between the class probability mapping and regions of interest.&lt;/li>
&lt;/ol>
&lt;p>Their methodology was implemented using the VGG architecture + extra layers to
accomodate for the parallel nature of the design.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/you-only-look-once-unified-real0time-object-detection/</description></item></channel></rss>