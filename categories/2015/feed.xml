<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2015 on Nicholas M. Synovic</title><link>https://nsynovic.dev/categories/2015/</link><description>Recent content in 2015 on Nicholas M. Synovic</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Nicholas M. Synovic</copyright><lastBuildDate>Fri, 11 Nov 2022 09:55:18 -0600</lastBuildDate><atom:link href="https://nsynovic.dev/categories/2015/feed.xml" rel="self" type="application/rss+xml"/><item><title>A summary of Hidden Technical Debt in Machine Learning Systems by D. Sculley et al.</title><link>https://nsynovic.dev/summaries/hidden-technical-debt-in-machine-learning-systems/</link><pubDate>Fri, 11 Nov 2022 09:55:18 -0600</pubDate><guid>https://nsynovic.dev/summaries/hidden-technical-debt-in-machine-learning-systems/</guid><description>&lt;h1 id="a-summary-of-hidden-technical-debt-in-machine-learning-systems">A summary of &lt;em>Hidden Technical Debt in Machine Learning Systems&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>D. Sculley Proceedings of NeurIPS, 2015
&lt;a href="https://proceedings.neurips.cc/paper/2015/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-hidden-technical-debt-in-machine-learning-systems">A summary of &lt;em>Hidden Technical Debt in Machine Learning Systems&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>Machine learning systems incur technical debt like traditional software systems.
However, they also incur additional debt that traditional software systems do
not.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Because this paper aims at establishing definitions for potential technical debt
that can be incurred while developing machine learning systems.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is a language definition paper. In other words, it is suggesting language
to be used when describing the technical debt of machine learning systems.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is related to those that discuss and define technical debt broadly
and in domain specific applications.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contributions are definitions and language to be used when describing
the different technical debt that can be incurred while developing machine
learning systems.&lt;/p>
&lt;p>These debts include:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Boundary Erosion&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Entanglement&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Hidden Feedback Loops&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Undeclared Consumers&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Data Dependencies&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Configuration Issues&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Changes in the External World&lt;/strong>&lt;/li>
&lt;li>&lt;strong>System-Level Anti-Patterns&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Traditional Software System Technical Debt&lt;/strong>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done to identify traditional software technical debt and software
system anti-patterns.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>There is one figure in this paper. While the visualization works, the usage of a
gray scale image makes it difficult to read the text.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is well written. The structure of this paper is quite simplistic,
which I appreciate as it makes it easier to digest.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ol start="2">
&lt;li>J. D. Morgenthaler, M. Gridnev, R. Sauciuc, and S. Bhansali. &lt;em>Searching for
build debt: Experiences managing technical debt at google&lt;/em>. In Proceedings of
the Third International Workshop on Managing Technical Debt, 2012.&lt;/li>
&lt;li>H. B. McMahan, G. Holt, D. Sculley, M. Young, D. Ebner, J. Grady, L. Nie, T.
Phillips, E. Davydov, D. Golovin, S. Chikkerur, D. Liu, M. Wattenberg, A. M.
Hrafnkelsson, T. Boulos, and J. Kubica. &lt;em>Ad click prediction: a view from the
trenches&lt;/em>. In The 19th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD 2013, Chicago, IL, USA, August 11-14, 2013,
2013.&lt;/li>
&lt;li>. Langford and T. Zhang. &lt;em>The epoch-greedy algorithm for multi-armed bandits
with side information&lt;/em>. In Advances in neural information processing systems,
pages 817–824, 2008.&lt;/li>
&lt;li>T. M. Chilimbi, Y. Suzue, J. Apacible, and K. Kalyanaraman. &lt;em>Project adam:
Building an efficient and scalable deep learning training system&lt;/em>. In 11th
USENIX Symposium on Operating Systems Design and Implementation, OSDI ’14,
Broomfield, CO, USA, October 6-8, 2014., pages 571–582, 2014.&lt;/li>
&lt;li>B. Dalessandro, D. Chen, T. Raeder, C. Perlich, M. Han Williams, and F.
Provost. &lt;em>Scalable hands free transfer learning for online advertising&lt;/em>. In
Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 1573–1582. ACM, 2014.&lt;/li>
&lt;li>M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V. Josifovski, J.
Long, E. J. Shekita, and B. Su. &lt;em>Scaling distributed machine learning with
the parameter server&lt;/em>. In 11th USENIX Symposium on Operating Systems Design
and Implementation, OSDI ’14, Broomfield, CO, USA, October 6-8, 2014., pages
583–598, 2014.&lt;/li>
&lt;li>D. Sculley, M. E. Otey, M. Pohl, B. Spitznagel, J. Hainsworth, and Y. Zhou.
&lt;em>Detecting adversarial advertisements in the wild. In Proceedings of the 17th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining&lt;/em>,
San Diego, CA, USA, August 21-24, 2011, 2011&lt;/li>
&lt;/ol>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The author&amp;rsquo;s look to traditional software and machine learning system examples
for anti-patterns and&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>As this paper proposes potential areas of technical debt for machine learning
systems, nearly all of it is assuming that something will go wrong when working
with a machine learning system.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>Given the type of paper that this is, yes, the assumptions seem valid.
Furthermore, the justifications for each type of technical debt are well
explained and made clear in the literature.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>As I&amp;rsquo;m now starting to study ML dependencies, this work is a great springboard
to drill deeper into what both ML and software engineers consider technical debt
and dependents.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>Are there identifiable cases of ML models that have one or more of the technical
debts described here?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>Overall this was a solid paper. I&amp;rsquo;d appreciate more real world examples of
models that have dealt with technical debt. Additionally, a survey of engineers
about whether they think each type of technical debt is worthy of considerations
would have been appreciated.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Hidden Technical Debt in Machine Learning Systems&lt;/em> by D. Sculley et
al. [1] presents potential technical debt considerations that ML engineers
need to consider when developing ML systems. These considerations stem both from
experience as well as traditional software engineering technical debt.
Furthermore, they pose several questions that engineers should ask when taking
on technical debt.&lt;/p>
&lt;p>These questions are:&lt;/p>
&lt;ul>
&lt;li>How easily can an entirely new algorithmic approach be tested at full scale?&lt;/li>
&lt;li>What is the transitive closure of all data dependencies?&lt;/li>
&lt;li>How precisely can the impact of a new change to the system be measured?&lt;/li>
&lt;li>Does improving one model or signal degrade others?&lt;/li>
&lt;li>How quickly can new members of the team be brought up to speed?&lt;/li>
&lt;/ul>
&lt;p>The authors defined several areas where technical debt can accrue:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Model Boundary Erosion&lt;/strong>
&lt;ul>
&lt;li>&lt;em>Entanglement&lt;/em>: CACE principle (Changing Anything Changes Everything) which
applies to both data and model training&lt;/li>
&lt;li>&lt;em>Correction Cascades&lt;/em>: Transfer learning/ fine tuning a PTM creates a new
model (B) which is now dependent upon the original model&amp;rsquo;s (A) weights and
architecture&lt;/li>
&lt;li>&lt;em>Undeclared Consumers&lt;/em>: Similar to visibility debt [2]; creates tight and
hidden coupling to the model which if changed, could affect the wider system&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Data Dependencies&lt;/strong>: Static analysis of data dependencies could resolve this
[3]
&lt;ul>
&lt;li>&lt;em>Unstable Data Dependencies&lt;/em>: Some data signals change qualitatively or
quantitatively over time. Therefore, improving input signals to the system
could harm the output signals of the model&lt;/li>
&lt;li>&lt;em>Underutilized Data Dependencies&lt;/em>: Similar to underutilized dependencies in
traditional software engineering; data signals that the model is trained on
but have little to no effect on the output signal. Removing these signals
post-training, however, could greatly affect the quality of the model.
Examples include:
&lt;ul>
&lt;li>&lt;code>Legacy Features&lt;/code>&lt;/li>
&lt;li>&lt;code>Bundled Features&lt;/code>: Features that collectively improve performance that
are bundled together&lt;/li>
&lt;li>&lt;code>ǫ-Features&lt;/code>: Adding features to marginally improve accuracy&lt;/li>
&lt;li>&lt;code>Correlated Features&lt;/code>: Models struggle to distinguish between two
correlated features, one of which is causal (important) and the other
non-causal (not important)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Feedback Loops&lt;/strong>
&lt;ul>
&lt;li>&lt;em>Direct Feedback Loops&lt;/em>: A model may directly influence the selection if its
own future training data based on the decisions it makes&lt;/li>
&lt;li>&lt;em>Hidden Feedback Loops&lt;/em>: Two systems may influence each other indirectly by
affecting the sources of their data&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>ML-System Anti-Patterns&lt;/strong>
&lt;ul>
&lt;li>&lt;em>Glue Code&lt;/em>: Trying to stitch together two incompatible components of a
system results in significant code overhead&lt;/li>
&lt;li>&lt;em>Pipeline Jungles&lt;/em>: Data pre-processing&lt;/li>
&lt;li>&lt;em>Dead Experimental Codepaths&lt;/em>&lt;/li>
&lt;li>&lt;em>Abstraction Debt&lt;/em>: There is a lack of standardized abstractions for ML
systems components&lt;/li>
&lt;li>&lt;em>Common Smells&lt;/em>:
&lt;ul>
&lt;li>&lt;code>Plain-Old-Data Type Smell&lt;/code>&lt;/li>
&lt;li>&lt;code>Multiple-(Programming) Language Smell&lt;/code>&lt;/li>
&lt;li>&lt;code>Prototype Smell&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Configuration Debt&lt;/strong>
&lt;ul>
&lt;li>Taken verbatim from the paper:&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-md" data-lang="md">&lt;span style="display:flex;">&lt;span>• It should be easy to specify a configuration as a small change from a previous configuration.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>• It should be hard to make manual errors, omissions, or oversights.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>• It should be easy to see, visually, the difference in configuration between two models.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>• It should be easy to automatically assert and verify basic facts about the configuration:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>number of features used, transitive closure of data dependencies, etc.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>• It should be possible to detect unused or redundant settings.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>• Configurations should undergo a full code review and be checked into a repository
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>&lt;strong>A Changing External World&lt;/strong>
&lt;ul>
&lt;li>&lt;em>Fixed Threshold in Dynamic Systems&lt;/em>: Manually selecting a decision
threshold that a system has to abide by.&lt;/li>
&lt;li>&lt;em>Monitoring and Testing&lt;/em>: &amp;ldquo;Comprehensive live monitoring of system behavior
in real time combined with automated response is critical for long-term
system reliability&amp;rdquo;
&lt;ul>
&lt;li>&lt;code>Prediction Bias&lt;/code>&lt;/li>
&lt;li>&lt;code>Action Limits&lt;/code>&lt;/li>
&lt;li>&lt;code>Up-Stream Producers&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>Data Testing Debt&lt;/em>&lt;/li>
&lt;li>&lt;em>Reproducibility Debt&lt;/em>: It is difficult to reproduce results from ML
research due to randomized algorithms, non-determinism inherent in parallel
learning, initial conditions, and interactions with the external world&lt;/li>
&lt;li>&lt;em>Process Management Debt&lt;/em>: How does one handle a system with many models?&lt;/li>
&lt;li>&lt;em>Cultural Debt&lt;/em>: Difficulty re-using academic research in industry&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/hidden-technical-debt-in-machine-learning-systems/</description></item><item><title>A summary of Fast R-CNN by Ross Girshick</title><link>https://nsynovic.dev/summaries/fast-r-cnn/</link><pubDate>Thu, 10 Nov 2022 15:18:13 -0600</pubDate><guid>https://nsynovic.dev/summaries/fast-r-cnn/</guid><description>&lt;h1 id="a-summary-of-fast-r-cnn">A summary of &lt;em>Fast R-CNN&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Ross Girshick, IEEE International Conference on Computer Vision, 2015;
&lt;a href="https://openaccess.thecvf.com/content_iccv_2015/html/Girshick_Fast_R-CNN_ICCV_2015_paper.html">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-fast-r-cnn">A summary of &lt;em>Fast R-CNN&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>The problem addressed in this paper is that there exists a method that is better
at performing object detection and semantic segmentation within region proposals
that is not implemented in either the original R-CNN model or the SPPNet model.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Because it provides an updated model architecture for performing object
detection and semantic segmentation within region proposals, thereby speeding up
inference time and reducing computational cost.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is a CNN paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>Papers that discuss CNN models with respect to region proposals.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>An updated R-CNN model that is substantially faster than the original R-CNN
model and the competing SPPNet model. This Fast R-CNN model achieves SOTA mean
average precision (mAP) on the PASCAL VOC 2007, 2010, and 2012 datasets. Fast
training and testing compared to R-CNN and SPPNet. And that fine tuning ConvNet
layers in VGG 16 improves mAP.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Region proposal convolutional neural networks have been created prior to this
work. Furthermore, this work utilizes techniques that other successful CV DL
models have utilized to achieve SOTA results.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The majority of the figures are clear. Figure 2 is a bit difficult to read due
to how squished the text is to each other. Additionally, the model architecture
in Figure 1 uses an identical image as presented in the seminal R-CNN paper. It
would have been nicer to see a different test image utilized for this paper.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is well written, if a bit technical. However, the technicality is
important as it distinguishes the improvements made to the original R-CNN and
SPPNet models.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>They performed a similar study to their previous paper (R-CNN) where they
compared the mAP of competing models against their model. Additionally, they
performed an analysis of their model where they tested different improvements
and DL techniques used in other models to improve performance.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>They utilized the VGG 16 model as their CNN model. However, other existing
models could&amp;rsquo;ve been used/ re-implemented with their fast region proposal model
to potentially improve performance.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>This assumption makes sense to a degree as VGG 16 is a popular model for
research purposes. However, evaluating other CNN models would have been more
interesting in my opinion.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d like to implement there work on non-VGG 16 models, such as ResNet or on a
MobileNet.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>Why weren&amp;rsquo;t other models implemented with the fast region proposal component?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>Overall good paper. I don&amp;rsquo;t recommend on creating a paper of a third variation
of this model unless there are substantial improvements made. These improvements
can be in further reducing computational or energy cost, an even simpler
architecture, or an substantial overall increase of mAP on the PASCAL VOC
datasets.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Fast R-CNN&lt;/em> by Ross Girshick [1] proposes a new method to perform
region proposal CNN tasks that is significantly faster than the previously
proposed method. To do so, both the region proposals and the image itself are
passed into the CNN layer for analysis. Additionally, many layers of the
previous architectures are collapsed into one to reduce the complexity.
Furthermore, the SVM classifier was replaced with a Softmax classifier which is
both faster and more accurate than the previous SVM classifier.&lt;/p>
&lt;hr>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/fast-r-cnn/</description></item><item><title>A summary of Deep Learning by Yann LeCun et al,</title><link>https://nsynovic.dev/summaries/deep-learning/</link><pubDate>Tue, 08 Nov 2022 12:55:10 -0600</pubDate><guid>https://nsynovic.dev/summaries/deep-learning/</guid><description>&lt;h1 id="a-summary-of-deep-learning">A summary of &lt;em>Deep Learning&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Yann LeCunn et al, Nature, 2015 &lt;a href="https://doi.org/10.1038/nature14539">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-deep-learning">A summary of &lt;em>Deep Learning&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>This paper discusses the usage of deep learning (DL) models and how they have
led to improvements in speech recognition, visual object recognition, object
detection, drug discovery, and genomics. It talks about how these models are
created, what type of models are typically applied to what domains, and the
usage of the backpropagation algorithm to train the model. Additionally, a
discussion about the usage of Recurrent Neural Networks (RNNs) and their
benefits is had.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this paper as it is a review of different DL techniques for
different problem domains.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is a literary review paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>It is closest related to papers that summarize a body of literature for the
purposes of understanding what the current SOTA techniques for a problem are.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contribution is a discussion of DL, its usages, RNNs, and a general
summary of the SOTA DL techniques for different problem domains.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done to develop DL and RNN techniques.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the figures are clear and easy to understand.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is well written.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ol start="2">
&lt;li>Krizhevsky, A., Sutskever, I. &amp;amp; Hinton, G. &lt;em>ImageNet classification with
deep convolutional neural networks.&lt;/em> In Proc. Advances in Neural Information
Processing Systems 25 1090–1098 (2012).&lt;/li>
&lt;li>Hinton, G. et al. &lt;em>Deep neural networks for acoustic modeling in speech&lt;/em>
recognition. IEEE Signal Processing Magazine 29, 82–97 (2012).&lt;/li>
&lt;li>Sutskever, I. Vinyals, O. &amp;amp; Le. Q. V. &lt;em>Sequence to sequence learning with
neural&lt;/em> networks. In Proc. Advances in Neural Information Processing Systems
27 3104–3112 (2014)&lt;/li>
&lt;li>Glorot, X., Bordes, A. &amp;amp; Bengio. Y. &lt;em>Deep sparse rectifier neural networks.&lt;/em>
In Proc. 14th International Conference on Artificial Intelligence and
Statistics 315–323 (2011).&lt;/li>
&lt;li>Hinton, G. E., Osindero, S. &amp;amp; Teh, Y.-W. &lt;em>A fast learning algorithm for deep
belief nets&lt;/em>. Neural Comp. 18, 1527–1554 (2006).&lt;/li>
&lt;li>Bengio, Y., Lamblin, P., Popovici, D. &amp;amp; Larochelle, H. &lt;em>Greedy layer-wise
training of deep networks.&lt;/em> In Proc. Advances in Neural Information
Processing Systems 19 153–160 (2006).&lt;/li>
&lt;li>LeCun, Y. et al. &lt;em>Handwritten digit recognition with a back-propagation
network.&lt;/em> In Proc. Advances in Neural Information Processing Systems 396–404
(1990).&lt;/li>
&lt;li>LeCun, Y., Bottou, L., Bengio, Y. &amp;amp; Haffner, P. G&lt;em>radient-based learning
applied to document recognition&lt;/em>. Proc. IEEE 86, 2278–2324 (1998).&lt;/li>
&lt;li>Hochreiter, S. &amp;amp; Schmidhuber, J. &lt;em>Long short-term memory&lt;/em>. Neural Comput. 9,
1735–1780 (1997).&lt;/li>
&lt;/ol>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The author&amp;rsquo;s of this paper reviewed literary sources for examples and usage of
DL and RNN techniques applied to different problem domains.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The author&amp;rsquo;s assume that unsupervised learning will become far more important in
the future than supervised learning.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>Potentially. Unsupervised learning presents problems and challenges not explored
in this paper, and is therefore treated as the next logical evolution of
techniques, rather than a series of unknowns and problems that need to be solved
first.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d like to implement the different DL techniques to the suggested problem
domains presented in this paper.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>Why wasn&amp;rsquo;t a discussion about Generative Adversarial Networks (GANs) not had in
this work? What are the performance differences of the presented loss functions?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>Overall, a pretty good paper. A follow up paper on unsupervised learning would
be nice to read.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The review paper &lt;em>Deep Learning&lt;/em> by Yann LeCun et al [1]. discusses the
advances and advantages of deep learning (DL) techniques made up to 2015. The
authors discuss what is DL, how and where it is applied, commercial and academic
usages of DL, the advantages of merging two different architectures together to
solve challenging tasks, and the usage of Recurrent Neural Networks (RNNs) for
handling natural language processing and speech recognition tasks. As their
paper is purely a listing of work that others have done prior to them, their
contributions were mostly the synthesis of such information into a digestible
document. With that said, each section of their work can be summarized, which is
what I have done here.&lt;/p>
&lt;p>DL allows for machine learning to surpass its previous limitations of having to
manually represent data in a suitable internal representation (through feature
extraction) by learning the representation itself. Current DL models are
typically trained using labeled datasets in what is known as supervised
learning. A sub-set of the data is used for training, which when ran through the
model, adjusts the hidden weights. These weights are adjusted using a technique
called stochastic gradient descent (SGD). SGD is accomplished by working
backwards through the model and taking the derivative of each weight which is
then used to adjust the hidden weights. Algorithms to do this include &lt;code>tanh(x)&lt;/code>
and &lt;code>ReLU&lt;/code>. &lt;code>ReLU&lt;/code> is the most popular algorithm for this task which is more
commonly known as backpropagation.&lt;/p>
&lt;p>Convolutional neural networks (ConvNets) are useful for analyzing data
structured as a series of multi-dimensional arrays. A typical application of
ConvNets are for analyzing images. RNNs are useful for analyzing data that is
dependent upon prior understanding. Chat bots, speech recognition, and answering
questions about data (i.e where is a character in a book?) are all problems that
are reliant upon the model having some sort of &amp;ldquo;memory&amp;rdquo;. Memory solutions
include &lt;code>long short-term memory&lt;/code> which has been useful for accomplishing these
tasks.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/deep-learning/</description></item><item><title>A summary of Learning Deep Features for Discriminative Localization by Bolei Zhou et al.</title><link>https://nsynovic.dev/summaries/learning-deep-features-for-discriminative-localization/</link><pubDate>Mon, 24 Oct 2022 14:44:26 -0500</pubDate><guid>https://nsynovic.dev/summaries/learning-deep-features-for-discriminative-localization/</guid><description>&lt;h1 id="a-summary-of-learning-deep-features-for-discriminative-localization">A summary of &lt;em>Learning Deep Features for Discriminative Localization&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Bolei Zhou et al.; &lt;a href="http://arxiv.org/abs/1512.04150">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-learning-deep-features-for-discriminative-localization">A summary of &lt;em>Learning Deep Features for Discriminative Localization&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Learning Deep Features for Discriminative Localization&lt;/em> by Bolei Zhou
et al. [1] describes using the global average pooling layer of CNNs to not
only regularize data, but also to localize objects in an image &lt;strong>even if the
network wasn&amp;rsquo;t trained for object detection&lt;/strong>. The authors propose a method for
object localization that involves a simple modification to the layer to generate
what they call &amp;ldquo;class activation maps&amp;rdquo; (CAMs), which are heat maps of where the
CNN is &amp;ldquo;looking&amp;rdquo; at an image for labeling. The hotter the heat map, the more
focus the CNN is putting on that specific image region.&lt;/p>
&lt;p>The authors go into detail as to how one would accomplish this with a
weakly-supervised object localization method, and its applications towards deep
features for generic localization, fine-grained recognition, and pattern
discovery. They conclude with visualizing class specific units.&lt;/p>
&lt;p>Their technique accomplishes object localization in a single forward pass on
existing CNN models that utilize a global average pooling layer.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is a CNN understanding and technique paper. It discusses a method for
understanding what a CNN is looking at as well as expanding the usage of image
classifiers for object localization.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is related to works involving object localization, image
classification, CNNs, and Deep Learning papers.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The author&amp;rsquo;s main contribution is a method for modifying the global average
pooling layer in CNNs to perform object localization in a single forward pass.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>There has been work done in utilizing weakly-supervised learning to perform
object localization. However, these works either don&amp;rsquo;t evaluate the object
localization task, or utilize multiple passes to perform the task.&lt;/p>
&lt;p>There has been numerous work that has gone into visualizing what occurs within a
CNN. Additionally, there has been work that has looked at the global &lt;em>max&lt;/em>
pooling layer, however, this work is the first to utilize the global &lt;em>average&lt;/em>
layer.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this paper as it provides a methodology of utilizing
existing CNNs trained on image classification to perform object localization
tasks &amp;ldquo;for free&amp;rdquo;. In other words, this paper presents a methodology for object
localization by reusing existing SOTA CNNs.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the figures and tables are labeled clearly, have detailed captions, and
make sense with respect to the paper.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is well written.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>Self-taught object localization with deep networks [2]&lt;/li>
&lt;li>Weakly supervised object localization with multi-fold multiple instance
learning [3]&lt;/li>
&lt;li>Learning and transferring mid-level image representations using convolutional
neural networks [4]&lt;/li>
&lt;li>Is object localization for free? weakly-supervised learning with convolutional
neural networks [5]&lt;/li>
&lt;li>Visualizing and understanding convolutional networks [6]&lt;/li>
&lt;li>Object detectors emerge in deep scene CNNs [7]&lt;/li>
&lt;li>Network in network [8]&lt;/li>
&lt;li>Going deeper with convolutions [9]&lt;/li>
&lt;/ul>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I would love to take this work and apply it to my current research in low
powered computer vision. By utilizing larger networks to localize where in a
static scene the object of interest is most likely to be in (for example, a
static video of a bird sitting on a wire), I can pass in this mapping into a CNN
to specifically be interested in that region of the video/ image. Additionally,
by figuring out where a larger CNN is localizing data, I can then mask out any
cold area of the image prior to analysis by a smaller CNN.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Learning Deep Features for Discriminative Localization&lt;/em> by Bolei Zhou
et al. [1] discusses a weakly supervised method of performing object
localization on existing CNN models. Their method involves replacing the fully
connected layer at the end of a CNN performing image classification, with a
global average pooling layer into a Softmax layer. This is so that the models
original functionality is not cut from the new model. However, the global
average pooling layer is modified so that a heat map can be extracted focusing
on what the CNN is focusing on prior to labeling the image.&lt;/p>
&lt;p>Previous work involved the usage of weakly supervised CNNs, but relied on global
max pooling. Additional work utilized deconvolutional layers to perform a
similar task.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/learning-deep-features-for-discriminative-localization/</description></item><item><title>A summary of Very Deep Convolutional Networks for Large-Scale Image Recognition by Karen Simonyan and Andrew Zisserman</title><link>https://nsynovic.dev/summaries/very-deep-convolutional-networks-for-large-scale-image-recognition/</link><pubDate>Wed, 28 Sep 2022 22:40:46 -0500</pubDate><guid>https://nsynovic.dev/summaries/very-deep-convolutional-networks-for-large-scale-image-recognition/</guid><description>&lt;h1 id="a-summary-of-very-deep-convolutional-networks-for-large-scale-image-recognition-by-karen-simonyan-and-andrew-zisserman">A summary of &lt;em>Very Deep Convolutional Networks for Large-Scale Image Recognition&lt;/em> by Karen Simonyan and Andrew Zisserman&lt;/h1>
&lt;blockquote>
&lt;p>Karen Simonyan and Andrew Zisserman;
&lt;a href="https://doi.org/10.48550/arXiv.1409.1556">https://doi.org/10.48550/arXiv.1409.1556&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-very-deep-convolutional-networks-for-large-scale-image-recognition-by-karen-simonyan-and-andrew-zisserman">A summary of &lt;em>Very Deep Convolutional Networks for Large-Scale Image Recognition&lt;/em> by Karen Simonyan and Andrew Zisserman&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Very Deep Convolutional Networks for Large Scale Image Recognition&lt;/em>
by Karen Simonyan and Andrew Zissernman discusses the SOTA performance of their
model in the 2014 ImageNet Challenge on localization and classification tasks.
They discuss that be extending the depth of convolutional neural networks to 16
up to 19 layers, with a 3x3 filter size, SOTA performance is possible without
redeveloping the architecture of existing convolutional neural networks. This is
in contrast to &lt;a href="going-deeper-with-convolutions.md">Szegedy&amp;rsquo;s work&lt;/a> who proposes
the Inception architecture for classification and object detection; with which
the reference implementation also came first in the 2014 ImageNet Challenge in
its respective tasks. Simoyan et al. discuss the architecture and training that
went into their model (VGG) and how to architect future models to perform as
well or better.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is both a computer vision model evaluation and architecture paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is most closely related to others who publish work regarding SOTA
performance on CV architecture and models.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contributions is an exploration of depth in traditional convolutional
neural networks to achieve SOTA performance.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Prior work has gone into optimizing the width and initial convolutions of
convolutional neural networks.&lt;/p>
&lt;p>&lt;a href="going-deeper-with-convolutions.md">Szegedy et al.&lt;/a> proposed a new architecture
(Inception) that achieved SOTA performance in the 2014 ImageNet Challenge. Else,
Krizhevsky et al. [2] and others have proposed improvements to the
convolutional neural network architecture.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about the authors work as increasing the depth of a neural
network by their proposed architecture allows for easy expansion of existing
convolutional neural networks without redesigning the libraries used to create
them.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The tables that are presented are easy to read, but can be improved upon. Often,
multiple rows will correspond with a single model configuration. This is fine,
however, it is difficult to make out what configuration each row corresponds to.
Additionally, the tables make comparing error percentages easy across model
configurations.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is well written and can be understood.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>Classical convolutional neural network architecture - [3]&lt;/li>
&lt;li>GoogLeNet - [2]&lt;/li>
&lt;li>Clarifai&lt;/li>
&lt;li>ImageNet classification with deep convolutional neural net- works [4]&lt;/li>
&lt;li>Isotropically-rescaled training image&lt;/li>
&lt;li>ImageNet 2013 submissions - [5], [6] Localization and Detection using
Convolutional Networks&lt;/li>
&lt;/ul>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors assume that the performance improvements that convolutional neural
networks are achieving are based off of larger data sets and better compute
optimization.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>I agree with their assumption. However, [2] created a SOTA model utilizing a
new architecture, rather than improving upon an existing one.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I would love to try and optimize the input layer of convolutional neural
networks by having a computation that not only looks at the color space, but
also the opacity of an image. This would allow for images to have their
background removed for the purposes of classification by making the background
less opaque than the foreground.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Very Deep Convolutional Networks for Large Scale Image Recognition&lt;/em>
by Karen Simonyan and Andrew Zissernman discusses the SOTA performance of their
model in the 2014 ImageNet Challenge on localization and classification tasks.
They discuss that be extending the depth of convolutional neural networks to 16
up to 19 layers, with a 3x3 filter size, SOTA performance is possible without
redeveloping the architecture of existing convolutional neural networks. Their
work builds of previous efforts of improving convolutional neural network
performance by optimizing the filter size and initial layer, but contrasts
contemporaries [2] by not developing a new architecture. Their work has
importance as it shows that the existing convolutional neural network
architecture is capable of SOTA performance by increasing the depth of the
model. They justify this by trying six different model configurations, and
finding that models with 16 to 19 layers performed best on the 2014 ImageNet
Challenge classification and localization challenges.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/very-deep-convolutional-networks-for-large-scale-image-recognition/</description></item></channel></rss>