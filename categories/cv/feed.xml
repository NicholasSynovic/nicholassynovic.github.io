<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>cv on Nicholas M. Synovic</title><link>https://nsynovic.dev/categories/cv/</link><description>Recent content in cv on Nicholas M. Synovic</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Nicholas M. Synovic</copyright><lastBuildDate>Mon, 24 Oct 2022 13:46:36 -0500</lastBuildDate><atom:link href="https://nsynovic.dev/categories/cv/feed.xml" rel="self" type="application/rss+xml"/><item><title>A summary of How to implement SVMs by John Platt</title><link>https://nsynovic.dev/summaries/how-to-implement-svms/</link><pubDate>Mon, 24 Oct 2022 13:46:36 -0500</pubDate><guid>https://nsynovic.dev/summaries/how-to-implement-svms/</guid><description>&lt;h1 id="a-summary-of-how-to-implement-svms">A summary of &lt;em>How to implement SVMs&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>John Platt; &lt;a href="https://doi.ieeecomputersociety.org/10.1109/5254.708428">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-how-to-implement-svms">A summary of &lt;em>How to implement SVMs&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>&lt;/li>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#third-pass">Third Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The essay &lt;em>How to implement SVMs&lt;/em> by John Platt (as part of the larger &lt;em>Support
vector machine&lt;/em> collection of essays in the July/ August edition of the 1998
IEEE Intelligent Systems magazine) [1] discusses how to implement a Support
Vector Machine (SVM). This essay goes into great detail on implementation
strategies for handling larger datasets, as well as methods for training SVMs.
Topics include understanding the Quadratic Problem (what SVMs aim to solve),
sequential minimal optimization (reaching a global minimal value), and where to
find SVM implementations.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This essay seems to be a tutorial/ workshop paper about SVMs.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>I would expect papers that are about implementing SVMs from scratch would be
related to this essay.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contributions are an understanding of how SVMs work as well as how to
implement them efficently.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h2 id="background-work">Background Work&lt;/h2>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has already been done on experimenting optimal SVM algorithms and
minimization functions.&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this paper as it provides an understanding of what a SVM is
and how they function.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The figure and charts have proper labels and captions that explain what they are
representing.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>For the most part, yes. However, the essay expects the reader to be knowledgable
about SVMs prior to reading the essay. This is shown mostly through the usage of
mathematical notation specific to the problem domain, and linking to other work
to explain it. While this is a short essay for a magazine, a brief sentence or
two about the notation would have been appreciated.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>A Tutorial on Support Vector Machines for Pattern Recognition [2]&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="third-pass">Third Pass&lt;/h2>
&lt;blockquote>
&lt;p>This section can only be complete after a virtual re-implementation of the
paper&lt;/p>
&lt;/blockquote>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The author assumes that the reader, should they implement their own SVM
algorithm, will be using a commericial numerical analysis package.&lt;/p>
&lt;h3 id="correctness">Correctness&lt;/h3>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>Without understanding the nature of the numerical analysis packages of 1998, I
would assume that this assumption is correct. I base this on that the author
mentions that free numerical analysis packages (not if they were open sourced or
not) run slower than commercial packages and may have errors due to precision
mistakes.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;m not interested in creating my own SVM algorithm. However, having a better
understanding of how SVMs work as well as the different minimization functions
that they implement, would be nice to know.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The essay &lt;em>How to implement SVMs&lt;/em> by John Platt (as part of the larger &lt;em>Support
vector machine&lt;/em> collection of essays in the July/ August edition of the 1998
IEEE Intelligent Systems magazine) [1] discusses how to implement a Support
Vector Machine (SVM). The author goes into detail about what an SVM is trying to
accomplish (minimize a quadratic problem on a high dimensional matrix), what
techniques exist to solve this problem, as well as availible programs to allow
for researchers to utilize SVMs in their work.&lt;/p>
&lt;p>Overall, the essay does a good job of explaining the problem space as well as
implementation details, however, the essay is very much a product of its time.
There is less of a need to develop new SVM algorithms as there are many that are
provided off of the shelf in free and open source numerical analysis packages
[3] [4]. Additionally, the suggestion that readers should purchase a
numerical analysis package to create thier own SVM is dated in my opinion, as
again, there are many free options availible [5].&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using the technique proposed by S. Keshav in his work
&lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/how-to-implement-svms/</description></item><item><title>A summary of Applying SVMS to Face Detection by Edgar Osuna</title><link>https://nsynovic.dev/summaries/applying-svms-to-face-detection/</link><pubDate>Mon, 24 Oct 2022 09:20:40 -0500</pubDate><guid>https://nsynovic.dev/summaries/applying-svms-to-face-detection/</guid><description>&lt;h1 id="a-summary-of-applying-svms-to-face-detection">A summary of &lt;em>Applying SVMs to Face Detection&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Edgar Isuna; &lt;a href="https://doi.ieeecomputersociety.org/10.1109/5254.708428">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-applying-svms-to-face-detection">A summary of &lt;em>Applying SVMs to Face Detection&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>&lt;/li>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#third-pass">Third Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The essay &lt;em>Applying SVMS to Face Detection&lt;/em> by Edgar Osuna (as part of the
larger &lt;em>Support vector machine&lt;/em> collection of essays in the July/ August edition
of the 1998 IEEE Intelligent Systems magazine) [1] describes the usage of
Support Vector Machines (SVMs) to identify faces in static images and real time
systems. The work goes into detail about previous systems that attempted this
task, as well as a real time system that can classify images at 4 to 5 frames
per second.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is both a small systems essay, as well as a CV task analysis of the
state of the art when using this particular technique.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This essay is most similar to papers that discuss systems that implement face
detection.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The author&amp;rsquo;s main contributions are a system that utilizes SVMs for real time
facial detection. Additionally, their contributions include a discuss of
previous systems that attempted this task.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h2 id="background-work">Background Work&lt;/h2>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Quote from the &lt;em>Previous systems&lt;/em> section of the paper:&lt;/p>
&lt;p>&amp;ldquo;Researchers have approached the face-detection problem with different
techniques in the last few years, including neural networks [2] [3],
detection of face features and use of geometrical constraints [4], density
estimation of the training data [5], labeled graphs [6], and clustering and
distribution-based modeling [7] [8].&amp;rdquo;&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this essay as it proposes an SVM based solution for both
static image and real time face detection.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The figures are clear and explained well through their captions. However, Table
2 uses a metric called &amp;ldquo;False Alarms&amp;rdquo; to measure the number of times the system
reported a &amp;ldquo;face&amp;rdquo; that wasn&amp;rsquo;t a face. A more appropriate metric, such as recall,
would have been appropriate in this case.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is well written, however, it can be improved upon. The biggest
complaint that I have is the usage of bullet points to describe tasks/ steps
that were taken to complete a task. Additionally, many bullet points contained
more than one sentence. I find it to be more appropriate for papers to utilize
bullet points for short, unordered lists. Most appropriately used when listing
off different techniques or definitions, which this essay does utilize. Aside
from that, the individual steps are written well and clearly, and seem to be
fairly reproducable.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>Detection and localization of faces on digital images [2]&lt;/li>
&lt;li>Human Face Detection in Visual Scenes [3]&lt;/li>
&lt;li>Human face detection in a complex background [4]&lt;/li>
&lt;li>Probabilistic visual learning for object detection [5]&lt;/li>
&lt;li>Determination of face position and pose with a learned representation based on
labelled graphs [6]&lt;/li>
&lt;li>Learning and Example Selection for Object and Pattern Detection [7]&lt;/li>
&lt;li>Example-based learning for view-based human face detection [8]&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="third-pass">Third Pass&lt;/h2>
&lt;blockquote>
&lt;p>This section can only be complete after a virtual re-implementation of the
paper&lt;/p>
&lt;/blockquote>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors trained their system to identify vertically oriented, gray-scale
images of faces for their static image face detector. They make no mention as to
whether this detector is capable of identifying faces in off axis positions, or
if their system is capable enough to orient faces properly.&lt;/p>
&lt;h3 id="correctness">Correctness&lt;/h3>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>Without understanding the availiblity of datasets at the time, this seems like a
valid assumption to make. However, simple data augmentation (such as rotating
the image) could&amp;rsquo;ve been done to increase the number of training examples of
faces not in the vertical orientation.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>A reimplementation of their work, both on static images and real time image
capture, would be interesting to perform on devices such as cameras, Raspberry
Pis, or other low powered systems. Additionally, comparing the power draw
between an SVM based solution and one that is powered by DL would be interesting
as well.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The essay &lt;em>Applying SVMS to Face Detection&lt;/em> by Edgar Osuna (as part of the
larger &lt;em>Support vector machine&lt;/em> collection of essays in the July/ August edition
of the 1998 IEEE Intelligent Systems magazine) [1] describes the usage of
Support Vector Machines (SVMs) to identify faces in static images and real time
systems. The author goes into detail about existing systems that were powered by
non-SVM techniques, as well as presenting their own system (for both static
image and real time image capture) for face detection.&lt;/p>
&lt;p>Their satic image system only works on gray scale images of vertically aligned
faces. Additionally, they used a small dataset to train the SVM. In doing so,
they limit the usage of the static image system to that specific domain, as well
as potentially creating a system that is unable to detect a face in all
potential cases (such as different ethnicities, lighting conditions, face
orientations, etc.).&lt;/p>
&lt;p>Their real time image capture system works on full color images of vertically
aligned faces by using a combination of a skin detector and a &amp;ldquo;primitive&amp;rdquo; motion
detector. This system was capable of recognizing faces at 4 to 5 frames per
second.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using the technique proposed by S. Keshav in his work
&lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/applying-svms-to-face-detection/</description></item><item><title>A summary of Very Deep Convolutional Networks for Large-Scale Image Recognition by Karen Simonyan and Andrew Zisserman</title><link>https://nsynovic.dev/summaries/very-deep-convolutional-networks-for-large-scale-image-recognition/</link><pubDate>Wed, 28 Sep 2022 22:40:46 -0500</pubDate><guid>https://nsynovic.dev/summaries/very-deep-convolutional-networks-for-large-scale-image-recognition/</guid><description>&lt;h1 id="a-summary-of-very-deep-convolutional-networks-for-large-scale-image-recognition-by-karen-simonyan-and-andrew-zisserman">A summary of &lt;em>Very Deep Convolutional Networks for Large-Scale Image Recognition&lt;/em> by Karen Simonyan and Andrew Zisserman&lt;/h1>
&lt;blockquote>
&lt;p>Karen Simonyan and Andrew Zisserman;
&lt;a href="https://doi.org/10.48550/arXiv.1409.1556">https://doi.org/10.48550/arXiv.1409.1556&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-very-deept-cnvolutional-networks-for-large-scale-image-recognition-by-karen-simonyan-and-andrew-zisserman">A summary of &lt;em>Very Deep Convolutional Networks for Large-Scale Image Recognition&lt;/em> by Karen Simonyan and Andrew Zisserman&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>&lt;/li>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#third-pass">Third Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Very Deep Convolutional Networks for Large Scale Image Recognition&lt;/em>
by Karen Simonyan and Andrew Zissernman discusses the SOTA performance of their
model in the 2014 ImageNet Challenge on localisation and classification tasks.
They discuss that be extending the depth of convolutional neural networks to 16
up to 19 layers, with a 3x3 filter size, SOTA performance is possible without
redeveloping the architecture of existing convolutional neural networks. This is
in contrast to &lt;a href="going-deeper-with-convolutions.md">Szegedy&amp;rsquo;s work&lt;/a> who proposes
the Inception architecture for classification and object detection; with which
the reference implementation also came first in the 2014 ImageNet Challenge in
its respective tasks. Simoyan et al. discuss the architecture and training that
went into their model (VGG) and how to architect future models to perform as
well or better.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is both a computer vision model evaluation and architecture paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is most closely related to others who publish work regarding SOTA
performance on CV architecture and models.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contributions is an exploration of depth in traditional convulational
neural networks to achieve SOTA performance.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h2 id="background-work">Background Work&lt;/h2>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Prior work has gone into optimizing the width and intial convulations of
convultional neural networks.&lt;/p>
&lt;p>&lt;a href="going-deeper-with-convolutions.md">Szegedy et al.&lt;/a> proposed a new architecture
(Inception) that achieved SOTA performance in the 2014 ImageNet Challenge. Else,
Krizhevsky et al. [2] and others have proposed improvments to the
convulational neural network architecture.&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about the authors work as increasing the depth of a neural
network by their proposed architecture allows for easy expansion of existing
convulational neural networks without redesigning the libraries used to create
them.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The tables that are presented are easy to read, but can be improved upon. Often,
multiple rows will correspond with a single model configuration. This is fine,
however, it is difficult to make out what configuration each row corresponds to.
Additionally, the tables make comparing error percentages easy across model
configurations.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is well written and can be understood.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>Classical convultional neural network architecture - [3]&lt;/li>
&lt;li>GoogLeNet - [2]&lt;/li>
&lt;li>Clarifai&lt;/li>
&lt;li>ImageNet classification with deep convolutional neural net- works [4]&lt;/li>
&lt;li>Isotropically-rescaled training image&lt;/li>
&lt;li>ImageNet 2013 submissions - [5], [6] Localization and Detection using
Convolutional Networks&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="third-pass">Third Pass&lt;/h2>
&lt;blockquote>
&lt;p>This section can only be complete after a virtual re-implementation of the
paper&lt;/p>
&lt;/blockquote>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors assume that the performance improvements that convulational neural
networks are achieving are based off of larger datasets and better compute
optimization.&lt;/p>
&lt;h3 id="correctness">Correctness&lt;/h3>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>I agree with their assumption. However, [2] created a SOTA model utilizing a
new architecture, rather than improving upon an existing one.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I would love to try and optimize the input layer of convulational neural
networks by having a computation that not only looks at the color space, but
also the opacity of an image. This would allow for images to have their
background removed for the purposes of classification by making the background
less opaque than the foreground.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Very Deep Convolutional Networks for Large Scale Image Recognition&lt;/em>
by Karen Simonyan and Andrew Zissernman discusses the SOTA performance of their
model in the 2014 ImageNet Challenge on localisation and classification tasks.
They discuss that be extending the depth of convolutional neural networks to 16
up to 19 layers, with a 3x3 filter size, SOTA performance is possible without
redeveloping the architecture of existing convolutional neural networks. Their
work builds of previous efforts of improving convulational neural network
performance by optimizing the filter size and intial layer, but contrasts
contemporaries [2] by not developing a new architecture. Their work has
importance as it shows that the existing convulational neural network
architecture is capable of SOTA performance by increasing the depth of the
model. They justify this by trying six different model configurations, and
finding that models with 16 to 19 layers performed best on the 2014 ImageNet
Challenge classification and localisation challenges.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using the technique proposed by S. Keshav in his work
&lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/very-deep-convolutional-networks-for-large-scale-image-recognition/</description></item><item><title>A summary of Going deeper with convolutions by Christian Szegedy et al..</title><link>https://nsynovic.dev/summaries/going-deeper-with-convolutions/</link><pubDate>Wed, 28 Sep 2022 20:07:40 -0500</pubDate><guid>https://nsynovic.dev/summaries/going-deeper-with-convolutions/</guid><description>&lt;h1 id="a-summary-of-going-deeper-with-convolutions">A summary of &lt;em>Going deeper with convolutions&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Christian Szegedy et al.;
&lt;a href="https://doi.org/10.48550/arXiv.1409.4842">https://doi.org/10.48550/arXiv.1409.4842&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-going-deeper-with-convolutions">A summary of &lt;em>Going deeper with convolutions&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#third-pass">Third Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Going deeper with convolutions&lt;/em> by Christian Szegedy et al. [1]
describes a 2014 state of the art computer vision model (on the ImageNet
Large-Scale Visual Recognition Challenge) called GoogLeNet architected based on
Hebbian principles (i.e. neruons that fire together, are wired together)and a
constant computational budget. Their approach relies on creative algorithms and
neuroscience principles and aims to be a more power effiecient model for mobile
devices by limiting the computations during infrencing. Additionally, their
model is deep but not wide and is considered &amp;ldquo;sparse&amp;rdquo; by the authors. In other
words, there are as few nodes as possible within the neural network.&lt;/p>
&lt;p>Szegedy et al.&amp;rsquo;s contributions are a state of the art computer vision model that
provides experimental evidence that, &amp;ldquo;&amp;hellip; Approximating the expected optimal
sparse structure by readily avilible dense building blocks is a viable method
for improving neural networks for computer vision&amp;rdquo;. This means that this model
proves that dense neural networks for computer vision are not necessary in the
author&amp;rsquo;s viewpoint.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is a computer vision paper describing both a machine learning architecture
and refernce model.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is related to other computer vision papers that achieve state of the
art performance values based on the ImageNet Large-Scale Visual Recognition
Challenge.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Szegedy et al.&amp;rsquo;s contributions are:&lt;/p>
&lt;ul>
&lt;li>A computer vision model architecture (Inception) that is both sparse and aims
to be computationally efficient on mobile (non-server) devices,&lt;/li>
&lt;li>A reference model of the aforementioned computer vision model architecture&lt;/li>
&lt;li>A comparison of previous state of the art work to justify their claims that
sparser networks are the future of computer vision models.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the tables have proper column labels. However, Table 1 does not provide
default values for blank cells. This is most likely due to the layer type not
performaing a specific operation (as described in the column label). Regardless,
the remaining tables look good.&lt;/p>
&lt;p>Additionally, Figure 3 is very clear to read, if a little dense. However, as it
describes all of the layers of GoogLeNet and how they are connected, I find the
size to be appropriate.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is fairly well written. The only complaints that I have are minor
grammatical mistakes that the author&amp;rsquo;s left in (by accident I assume).
Additionally, that the authors didn&amp;rsquo;t optimize their tables and figures to
better fit on the pages. As tables and figures are stacked on top of one
another, it would be possible to reclaim paper space by rearranging multiple
tables and figures to be next to one another, with the exception of Figure 3 due
to the sheer size of it.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>Contrast Normalization&lt;/li>
&lt;li>Max Pooling&lt;/li>
&lt;li>Average Pooling&lt;/li>
&lt;li>Softmax Activation&lt;/li>
&lt;li>Dropout - [2]&lt;/li>
&lt;li>Localization Task - [3]&lt;/li>
&lt;li>Gabor Filters - [4]&lt;/li>
&lt;li>Network in Network - [5]&lt;/li>
&lt;li>Rectified Linear Activation - [6]&lt;/li>
&lt;li>Regions with Convolutional Neural Networks - [7]&lt;/li>
&lt;li>Multi-Box Prediction - [8]&lt;/li>
&lt;li>Arora proof - [9]&lt;/li>
&lt;li>LeNet 5 - [10]&lt;/li>
&lt;li>Fisher vectors&lt;/li>
&lt;li>Polyak Averaging - [11]&lt;/li>
&lt;li>Jaccard index&lt;/li>
&lt;li>Selective Search - [12]&lt;/li>
&lt;li>Photometric Distortions - [13]&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="third-pass">Third Pass&lt;/h2>
&lt;blockquote>
&lt;p>This section can only be complete after a virtual re-implementation of the
paper&lt;/p>
&lt;/blockquote>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>One assumption that the authors make is that overfitting is more prone to occur
in large models. Additionally, overfitting can occur when there is not enough
labeled examples in a dataset when a large model is training. Furthermore,
increasing the size of a model increases the number of computations that must be
done between layers (e.g. chaining two convulational layers results in
computation cost quadratically increasing) Their solutions relies on moving from
fully connected to sparsely connected architectures including within
convolutional layers. Also, their model architecutre is based on the idea that
computers are inefficent when, &amp;ldquo;&amp;hellip; Computing numerical calculations on
non-uniform sparse data structures&amp;rdquo;.&lt;/p>
&lt;p>They assume that 1x1, 3x3, and 5x5 filters are the proper filters to use, but
did not test other size of filters. They also assume that using, &amp;ldquo;Inception
modules&amp;rdquo; is only useful at higher levels, whereas the initial levels are
standard convultational levels. However, this was not tested either and was due
to, &amp;ldquo;infrastructural inefficiences&amp;rdquo; in the implementation.&lt;/p>
&lt;p>Finally, that the model that achieved state of the art performance was the best
model. The authors had been training and testing other models for months prior,
however, it is unclear what the testing methodology was and why a particular
model was choosen to compete in the ImageNet competition.&lt;/p>
&lt;h3 id="correctness">Correctness&lt;/h3>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>The first paragraph of assumptions seems reasonable and correct. However, the
remaining two paragraphs seem unreasonable. This is due to the lack of testing
that the author&amp;rsquo;s put in when optimizing their model with respect to the filter
sizes and choosing models. Furthermore, if testing did occur to address these
issues, it is not addressed in this paper, thus leaving the reader to wonder why
testing wasn&amp;rsquo;t performed.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>Based off of Section 3 (Motiviation and High Level Considerations), one
promising area of study would be to perform a network architecture search
utilizing the principles and reasoning of their approach to other machine
learning and computer vision domains.&lt;/p>
&lt;p>An enhancement to their work is possible by analyzing what filter sizes most
optimal improve performance. Currently the author&amp;rsquo;s are restricting GoogLeNet to
1x1, 3x3, and 5x5 filter sizes, but this was due to convience and no data was
given to support this.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Going deeper with convolutions&lt;/em> by Szegedy et al. [1] introduces a
computer vision model architecture called Inception and a reference model called
GoogLeNet.&lt;/p>
&lt;p>Inception is a model architecuture that is both sparse and (attempts to be)
computationally efficent during inferencing with only 1.5 billion multiply-add
operations allowed. Inception models are composed of multiple Inception modules
that are stacked on top of each other. Each Inception module takes in data from
the previous layer and passes it into small convultional filters (i.e. 1x1
typically). There are three of these small filters that are wired to the input
of the Inception module, with one of them connected directly to the output. The
outputs of two of these filters are then passed into larger filters (i.e. 5x5)
to which it is then passed into a DepthConcat function. Additionally, a 3x3
filter is wired to the input of the module and the output of which goes into a
1x1 filter to be passed into the DepthConcat function as well. From there, it is
passed into another Inception module and the process repeats.&lt;/p>
&lt;p>&lt;strong>Note:&lt;/strong> Depth when referring to two dimensional images refers to the color
channel of the image. As images typically have three color channels (i.e. red,
green, blue), an image would have a depth of 3.&lt;/p>
&lt;p>&lt;em>Example:&lt;/em> A 200 pixel by 200 pixel full color spectrum image would be
represented as 200x200x3.&lt;/p>
&lt;p>It is possible for Inception modules to have an additional connection to the
input of the module to perform average pooling for softmax activation.&lt;/p>
&lt;p>GoogLeNet achieved SOTA performance in the ImageNet Large-Scale Visual
Recognition Challenge image classification task by having a top-5 error of 6.67%
on both the validation and testing data. This is an improvment of 56.5% in
comparison to 2012&amp;rsquo;s SOTA performer (SuperVision) and 2013&amp;rsquo;s SOTA performer
(Clarifai). Additionally, they achieved SOTA perfomance for the ImageNet
Large-Scale Visual Recognition Challenge detection task with a mean average
precision of 43.9% utilizing an ensemble inference approach. This model was
architected using the Inception architecture with 22 layers. However, not every
layer was an Inception module; the first few layers were standard convulational
layers.&lt;/p>
&lt;p>The author&amp;rsquo;s contributions were as follows;&lt;/p>
&lt;ol>
&lt;li>The Inception computer vision architecture,&lt;/li>
&lt;li>The GoogLeNet SOTA computer vision model for classification and object
detection.&lt;/li>
&lt;/ol>
&lt;p>My opinion on this paper is that while it is well written, the author&amp;rsquo;s make
numerous assumptions about the optimal performance of their model&amp;rsquo;s
architecture. They don&amp;rsquo;t test optimal sizes for filters as well as resolving
bugs such as the usage of standard convulational layers early in the model. Both
of which can be solved by performing a neural architecture search.&lt;/p>
&lt;p>Future work for this paper would involve optimizing the model architecture via a
neural architecture search. As well as evaluating the performance of the model
by both increasing and decreasing the depth of the model.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using the technique proposed by S. Keshav in his work
&lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/going-deeper-with-convolutions/</description></item></channel></rss>