<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>summary on Nicholas M. Synovic</title><link>https://nsynovic.dev/categories/summary/</link><description>Recent content in summary on Nicholas M. Synovic</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Nicholas M. Synovic</copyright><lastBuildDate>Sun, 19 Feb 2023 13:39:57 -0600</lastBuildDate><atom:link href="https://nsynovic.dev/categories/summary/feed.xml" rel="self" type="application/rss+xml"/><item><title>A summary of Toward Fail Safety for Security Decisions by Trent Jaeger</title><link>https://nsynovic.dev/summaries/toward-fail-safety-for-security-decisions/</link><pubDate>Sun, 19 Feb 2023 13:39:57 -0600</pubDate><guid>https://nsynovic.dev/summaries/toward-fail-safety-for-security-decisions/</guid><description>&lt;h1 id="a-summary-of-toward-fail-safety-for-security-decisions">A summary of &lt;em>Toward Fail Safety for Security Decisions&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Trent Jaeger IEEE Computer Volume 55; Issue 4, 2022 DOI [0]&lt;/p>
&lt;/blockquote>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-toward-fail-safety-for-security-decisions">A summary of &lt;em>Toward Fail Safety for Security Decisions&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#examples-of-how-user-decisions-impact-security">Examples of How User Decisions Impact Security&lt;/a>&lt;/li>
&lt;li>&lt;a href="#a-design-goal-fail-safe-decisions">A Design Goal: Fail-Safe Decisions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#using-fail-safety-effectively">Using Fail Safety Effectively&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Developers of technologies aim to automate security decisions on behalf of the
consumer. However, security decisions made by end users are often ad-hoc and
unguided. As researchers, we should figure out how to create systems that are
reduce vulnerabilities when poor manual choices are made.&lt;/p>
&lt;h3 id="examples-of-how-user-decisions-impact-security">Examples of How User Decisions Impact Security&lt;/h3>
&lt;p>Mobile applications often request access to sensors on the device to operate.
However, it is often unclear as to why these applications want to utilize these
sensors. Additionally, by granting security access to malicious apps, it is
possible for the apps to modify core system functionality or compromise core
processes for their own gain.&lt;/p>
&lt;p>Within the IoT space, often developers let the end users handle the security
permissions on their end. However, it is both the developers and the end-users
job to ensure that the devices are operating within in a secure environment.&lt;/p>
&lt;h3 id="a-design-goal-fail-safe-decisions">A Design Goal: Fail-Safe Decisions&lt;/h3>
&lt;p>We should have fail-safe defaults that expresses the permissions that a device
or application has rather than what it doesn&amp;rsquo;t. In other words, return positive
feedback as to the what an application or device can do, rather than what it
can&amp;rsquo;t.&lt;/p>
&lt;p>To support this, applications need to help users make better decisions. This
could be done through supporting safe choices through the path of least
resistance. In other words, make it easy to secure an application and difficult
to disable security. However, it is difficult for programmers to decide what is
and isn&amp;rsquo;t a safe decision.&lt;/p>
&lt;h3 id="using-fail-safety-effectively">Using Fail Safety Effectively&lt;/h3>
&lt;p>If fail safety is implemented on a per device level, it won&amp;rsquo;t be able to scale
to entire technology sectors. Therefore, best practices need to be developed in
order to allow fail-safety to grow.&lt;/p>
&lt;p>However, as legacy applications did not focus on information-flow integrity, it
might be impossible to back port fail safety to legacy applications.&lt;/p>
&lt;p>Furthermore, research needs to be done on how to identify when manual security
changes expose applications to security vulnerabilities. This way programmers
and researchers can devise new methods of protecting the end user when they
disable security features.&lt;/p>
&lt;hr>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/toward-fail-safety-for-security-decisions/</description></item><item><title>A summary of Pervasive Security and Privacy by - A Brief Reflection on Challenges and Opportunities by Florian Alt</title><link>https://nsynovic.dev/summaries/pervasive-security-and-privacy-a-brief-reflection-on-challenges-and-opportunities/</link><pubDate>Sun, 19 Feb 2023 10:56:20 -0600</pubDate><guid>https://nsynovic.dev/summaries/pervasive-security-and-privacy-a-brief-reflection-on-challenges-and-opportunities/</guid><description>&lt;h1 id="a-summary-of-pervasive-security-and-privacy-by---a-brief-reflection-on-challenges-and-opportunities">A summary of &lt;em>Pervasive Security and Privacy by - A Brief Reflection on Challenges and Opportunities&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Florian Alt IEEE Computer Volume 55; Issue 4, 2022 DOI [0]&lt;/p>
&lt;/blockquote>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-pervasive-security-and-privacy-by---a-brief-reflection-on-challenges-and-opportunities">A summary of &lt;em>Pervasive Security and Privacy by - A Brief Reflection on Challenges and Opportunities&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#the-state-of-security-and-privacy-in-pervasive-computing">The State of Security and Privacy in Pervasive Computing&lt;/a>&lt;/li>
&lt;li>&lt;a href="#implications">Implications&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#security-and-privacy-decision-overload">Security and Privacy Decision Overload&lt;/a>&lt;/li>
&lt;li>&lt;a href="#unawareness-of-data-sensitivity">Unawareness of Data Sensitivity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#sensing-close-to-the-body">Sensing Close to the Body&lt;/a>&lt;/li>
&lt;li>&lt;a href="#unclear-flow-of-data">Unclear Flow of Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#multidevice-environments">Multidevice Environments&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#challenges-and-opportunities">Challenges and Opportunities&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#designing-appropriate-mechanisms">Designing Appropriate Mechanisms&lt;/a>&lt;/li>
&lt;li>&lt;a href="#involvement-of-different-stakeholders">Involvement of Different Stakeholders&lt;/a>&lt;/li>
&lt;li>&lt;a href="#out-of-the-box-security-and-privacy">Out-of-the-Box Security and Privacy&lt;/a>&lt;/li>
&lt;li>&lt;a href="#adaptive-security-and-privacy-mechanisms">Adaptive security and Privacy Mechanisms&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Pervasive computing developments open up the opportunity for different human
experiences and research. However, security and privacy methods must take into
account what these new developments bring to the table; including both the
positives and the negatives of such computing. With this technology, user states
and contexts can be more easily inferred as well as their emotional and
cognitive states.&lt;/p>
&lt;p>This article is meant to give an overview of the challenges and opportunities
that arise within the security and privacy domain of pervasive computing.&lt;/p>
&lt;h3 id="the-state-of-security-and-privacy-in-pervasive-computing">The State of Security and Privacy in Pervasive Computing&lt;/h3>
&lt;p>The &lt;em>mainframe era&lt;/em> involved securing the intellectual rights of technologies
stored on big iron. The &lt;em>personal computing era&lt;/em> involved securing protecting
the privacy and data of everyday users. The &lt;em>pervasive computing era&lt;/em> will
involve protecting all of the data generating and capturing devices that a user
not only owns but interacts with.&lt;/p>
&lt;p>Pervasive computing includes both traditional computing devices, but also edge
devices, smart devices and appliances, and internet software. Pervasive
computing allows for sensitive data to be accessed both locally and remotely,
and therefore presents new challenges w.r.t security and privacy.&lt;/p>
&lt;p>Boundaries between domains are changing. For example, their used to be a barrier
between work and home, but with the COVID-19 pandemic, work and home became one.
This opened the doors to new attack vectors as it became more common for people
to work from home.&lt;/p>
&lt;h3 id="implications">Implications&lt;/h3>
&lt;h4 id="security-and-privacy-decision-overload">Security and Privacy Decision Overload&lt;/h4>
&lt;p>As we interface with more and more computers, we (as users) become overloaded
with different authentication schemes and practices. Additionally, all of the
devices that we interact with have many different privacy permissions and
options that the user might not be aware of and therefore enable or disable.&lt;/p>
&lt;h4 id="unawareness-of-data-sensitivity">Unawareness of Data Sensitivity&lt;/h4>
&lt;p>It is possible to generate many data points about an individual from a single
sensor. Therefore, it is imperative that users not only know about these
different data points, but also the implications for each data point. However,
it is currently very difficult to inform users of the importance of each data
point.&lt;/p>
&lt;h4 id="sensing-close-to-the-body">Sensing Close to the Body&lt;/h4>
&lt;p>Many of the &lt;a href="#unawareness-of-data-sensitivity">data sensitivity problems&lt;/a> arises
from users wearing sensors close to the body. These sensors can pick up on
health related information about an individual. Current sensor providers do very
little to protect this information. Therefore figuring out methods of obscuring
or reducing the collection of such information is important.&lt;/p>
&lt;h4 id="unclear-flow-of-data">Unclear Flow of Data&lt;/h4>
&lt;p>It is very difficult to understand where all of the data from internet of things
(IoT) devices is being stored. Thus the flow of data from a sensor to the
end-user is unclear. What data goes to the cloud? What stays locally? How is
data accessed? Who can access that data? How is it processed? Which data is
being collected? Novel solutions (i.e., privacy labels/badges) must be developed
to answer these questions to protect consumers.&lt;/p>
&lt;h4 id="multidevice-environments">Multidevice Environments&lt;/h4>
&lt;p>There is a push by hardware and software developers to integrate experiences
tightly together via multidevice communication. For example, logging into
Netflix on a smart TV through your cell phone. However, these multidevice
experiences raise security and privacy concerns as there are more points of
failure and attack vectors as the number of devices involved scales.&lt;/p>
&lt;h3 id="challenges-and-opportunities">Challenges and Opportunities&lt;/h3>
&lt;h4 id="designing-appropriate-mechanisms">Designing Appropriate Mechanisms&lt;/h4>
&lt;p>Better security interfaces need to be designed to promote users to protect their
data. Furthermore, good enough security practices (i.e., password
authentication) need to be revisited to see if and where areas of improvement
need to occur to better protect end users.&lt;/p>
&lt;h4 id="involvement-of-different-stakeholders">Involvement of Different Stakeholders&lt;/h4>
&lt;p>The end user is not the enemy with respect to security. Therefore, pervasive
computing technologies and experiences need to take into account the security
practices and limitations that end users experience and accommodate that. For
example, replacing password authentication with bio-metrics.&lt;/p>
&lt;h4 id="out-of-the-box-security-and-privacy">Out-of-the-Box Security and Privacy&lt;/h4>
&lt;p>Pervasive computing technologies need to be secure out-of-the-box and involve
very little user interaction to enable sensible security and privacy settings.&lt;/p>
&lt;h4 id="adaptive-security-and-privacy-mechanisms">Adaptive security and Privacy Mechanisms&lt;/h4>
&lt;p>Pervasive computing can allow for authentication schemes based on the state of
the user. However, this information must simultaneously be protected and secured
in order to prevent data leaks.&lt;/p>
&lt;hr>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/pervasive-security-and-privacy-a-brief-reflection-on-challenges-and-opportunities/</description></item><item><title>A summary of Economics of Artificial Intelligence in Cybersecurity by Nir Kshetri</title><link>https://nsynovic.dev/summaries/economics-of-artificial-intelligence-in-cybersecurity/</link><pubDate>Sun, 19 Feb 2023 10:16:22 -0600</pubDate><guid>https://nsynovic.dev/summaries/economics-of-artificial-intelligence-in-cybersecurity/</guid><description>&lt;h1 id="a-summary-of-economics-of-artificial-intelligence-in-cybersecurity">A summary of &lt;em>Economics of Artificial Intelligence in Cybersecurity&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Nir Kshetri IEEE Computer Volume 55; Issue 4, 2022 DOI [0]&lt;/p>
&lt;/blockquote>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-economics-of-artificial-intelligence-in-cybersecurity">A summary of &lt;em>Economics of Artificial Intelligence in Cybersecurity&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#current-state-of-ai-in-cybersecurity-and-key-areas-being-transformed">Current State of AI in Cybersecurity and Key Areas Being Transformed&lt;/a>&lt;/li>
&lt;li>&lt;a href="#key-factors-driving-ais-use-in-cybersecurity">Key Factors Driving AI&amp;rsquo;s Use in Cybersecurity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#some-shortcomings-limitations-and-challenges">Some Shortcomings, Limitations, and Challenges&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>AI in cybersecurity is a growing market estimated to reach $101.8 billion by
2030.&lt;/p>
&lt;p>As the internet was designed &lt;strong>without&lt;/strong> security in mind, their exists an
asymmetric where small organizations or individuals can comprise large groups,
companies, or nation states. Therefore, defense mechanisms must be in place to
stop these attacks. Since defendants have access to a large corpus of malware
examples, it is possible to train AI models to defend against attacks.&lt;/p>
&lt;h3 id="current-state-of-ai-in-cybersecurity-and-key-areas-being-transformed">Current State of AI in Cybersecurity and Key Areas Being Transformed&lt;/h3>
&lt;p>AI is often faster at detecting malware than humans. Additionally, it is also
faster at containing infected devices on a network than a human. Furthermore, AI
can lower the cost of detecting malware.&lt;/p>
&lt;p>Traditional antivirus programs rely on a database of malware examples to check
against. However, AI can learn the representations of malware and then find new
malware examples prior to them being added to a database. Thus, it is possible
for AI to stop zero-day vulnerabilities by recognizing the representation of
malware during or prior to the attack.&lt;/p>
&lt;p>However, security experts recommend taking an augmented intelligence approach
with AI security. This involves a human-AI partnership in identifying and acting
upon threats. This is because current AI techniques are not accurate or advanced
enough to entirely replace a human security professional.&lt;/p>
&lt;p>AI has already been used to stop advance attacks from foreign, state run
organizations (i.e., APT41) and more common attacks.&lt;/p>
&lt;p>AI is already in use to detect anomalies in identity and access security. It is
possible for an AI to monitor and make decisions based off of an accounts
activity. As an example, Facebook uses an AI-powered deep entity classification
(DEC) to determine if an account is fraudulent or not. If so, the account is
removed from Facebook. This tool was used to crack down on fake accounts and
accounts that utilized deep fakes.&lt;/p>
&lt;p>AI cybersecurity software is being used by academic institutions as attackers
are increasingly targeting universities and academic institutions. These tools
have been successful in stopping or preventing cyber threats on students, staff,
and networks.&lt;/p>
&lt;h3 id="key-factors-driving-ais-use-in-cybersecurity">Key Factors Driving AI&amp;rsquo;s Use in Cybersecurity&lt;/h3>
&lt;p>The cost of AI cybersecurity tools is dropping both for consumers and for
enterprises. There exists more publicly available and enterprise-only datasets
for training AI to detect malware. And there is a shortage of cybersecurity
professionals entering the workforce, so AI could be used to address this
shortage of staff.&lt;/p>
&lt;h3 id="some-shortcomings-limitations-and-challenges">Some Shortcomings, Limitations, and Challenges&lt;/h3>
&lt;p>There exists several shortcomings with using AI cybersecurity tools. For
starters, it is difficult to explain what the tool is doing. Security
professionals therefore prefer an, &amp;ldquo;Explainable First, Predictive Second&amp;rdquo; [1]
approach to AI tools. Additionally, AI tools can not make security related
decisions without human interventions.&lt;/p>
&lt;p>And as AI tools grow in popularity, bias will start to develop within these
tools. This could potentially allow an attacker to exploit this bias and write
malware that is not detected by the tool.&lt;/p>
&lt;p>Furthermore, it is uncertain how AI tools would handle volatile situations, such
as the COVID-19 pandemic. As such, cybersecurity professionals might turn &lt;em>off&lt;/em>
or raise the detection threshold on these tools during such situations,
potentially allowing attacks to slip through.&lt;/p>
&lt;p>Finally, not all of the necessary data to properly train these tools is
available due to federal regulation. Personally identifying information (PII)
cannot be made publicly available for the purposes of training. Additionally, it
is assumed that large &amp;ldquo;data lakes&amp;rdquo; of Americans exist under the control of
foreign entities. It is therefore possible for an entity to utilize one of these
data lakes to write attacks that would not be detected as the attack is coming
from an American rather than a foreign entity.&lt;/p>
&lt;hr>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/economics-of-artificial-intelligence-in-cybersecurity/</description></item><item><title>A summary of Sentiment Analysis and Topic Recognition in Video Transcripts by Lukas Stappen et al.</title><link>https://nsynovic.dev/summaries/sentiment-analysis-and-topic-recognition-in-video-transcripts/</link><pubDate>Sat, 18 Feb 2023 19:03:11 -0600</pubDate><guid>https://nsynovic.dev/summaries/sentiment-analysis-and-topic-recognition-in-video-transcripts/</guid><description>&lt;h1 id="a-summary-of-sentiment-analysis-and-topic-recognition-in-video-transcripts">A summary of &lt;em>Sentiment Analysis and Topic Recognition in Video Transcripts&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Lukas Stappen et al. Posted in IEEE Computer Volume 55; Issue 4, 2022 DOI
[0]&lt;/p>
&lt;/blockquote>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-sentiment-analysis-and-topic-recognition-in-video-transcripts">A summary of &lt;em>Sentiment Analysis and Topic Recognition in Video Transcripts&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#related-work">Related Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#dataset-the-muse-topic-subchallenge">Dataset: The MuSe-Topic Subchallenge&lt;/a>&lt;/li>
&lt;li>&lt;a href="#exploratory-analysis">Exploratory Analysis&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#speaker-topics">Speaker Topics&lt;/a>&lt;/li>
&lt;li>&lt;a href="#emotions">Emotions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#prediction-results">Prediction Results&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#speaker-topics-1">Speaker Topics&lt;/a>&lt;/li>
&lt;li>&lt;a href="#emotions-1">Emotions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>It is difficult to extract the sentiment and topic from a video transcript. With
this in mind, researchers developed SenticNet [2], a natural language
processing (NLP) model to identify the sentiment and topic of a transcript with
far less computational resources than previous attempts. The authors were able
to achieve 3% better performance than previous solutions for the MuSe
competition.&lt;/p>
&lt;p>Multi-model sentiment analysis (MSA) is taking a variety of data streams and
information types and extracting sentiment from them. MSA research aims to
understand the sentiment holder, emotional disposition, and the reference
object. MSA typically works on video data as it includes visual (e.g., facial
expressions), audio, and textual (e.g., transcripts) data modalities.
Transcripts have been found to provide the greatest impact in understanding the
topic at hand.&lt;/p>
&lt;p>The authors solution learned a continuous vector space of embeddings from the
symbolic space of words from the transcripts. To identify sentiments, their
solution adheres to the description of of sentiments defined by the &lt;em>Hourglass
of Emotions&lt;/em> [1].&lt;/p>
&lt;h3 id="related-work">Related Work&lt;/h3>
&lt;p>Human communication is a symbolic and naturally ordered within a structured
hierarchy. Current solutions to identifying sentiment from human communication
rely on synsets which are labels that indicate emotion and mood categories.
SenticNet has the largest amount of synsets with 200,000 concepts map words to a
sentiment.&lt;/p>
&lt;p>Automated sentiment and aspect extraction is of interest within the MSA field.
Current solutions involve hand crafted features. The authors applied
&amp;ldquo;commonsense knowledge&amp;rdquo; about topic extraction involving several sentences.&lt;/p>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;p>First, natural world concepts are obtained using SenticNet. Versions 5 and 6 of
SenticNet were used as both extracted different sentics. Stop words were
removed. A linear SVM was applied on the vector word embeddings to predict the
valence, arousal, and topics of the transcript. To improve the generalization
between the feature maps, embedding dropout was used as well as time-step
dropout in order to drop entire embeddings rather than features.&lt;/p>
&lt;h3 id="dataset-the-muse-topic-subchallenge">Dataset: The MuSe-Topic Subchallenge&lt;/h3>
&lt;p>The MuSe-CaR [3] dataset is a large, multi-modal dataset consisting of YouTube
videos of car reviews. The purpose of the dataset is to support MSA research.&lt;/p>
&lt;p>The authors only used the language modality of the dataset and ignored the video
and audio modality. Recent advances in speech to text technologies have resulted
in near human performance.&lt;/p>
&lt;p>For the MuSe-Topic challenge, the weighted score of the combination of the
unweighted average recall and micro F1 measures for each prediction (valence,
arousal, and topic) was reported.&lt;/p>
&lt;h3 id="exploratory-analysis">Exploratory Analysis&lt;/h3>
&lt;h4 id="speaker-topics">Speaker Topics&lt;/h4>
&lt;p>The concepts of semantics were used to identify the contextual information of
the video. These were used to understand the characteristic properties of the
video.&lt;/p>
&lt;h4 id="emotions">Emotions&lt;/h4>
&lt;p>SenticNet was used in an unsupervised fashion to identify the emotions of the
video from the contextual information.&lt;/p>
&lt;h3 id="prediction-results">Prediction Results&lt;/h3>
&lt;h4 id="speaker-topics-1">Speaker Topics&lt;/h4>
&lt;p>The best performance measured achieved a score of 66.16% on the test dataset.
This was better than the LSTM approach the authors also tried. However, Albert
(an end-2-end NLP transformer for supervised NLP tasks) [4] still outperforms
this solution.&lt;/p>
&lt;h4 id="emotions-1">Emotions&lt;/h4>
&lt;p>SenticNet version 6 outperforms version 5 when identifying emotions.&lt;/p>
&lt;hr>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/sentiment-analysis-and-topic-recognition-in-video-transcripts/</description></item><item><title>A summary of Advances in Human Activity Recognition by Gulustan Dogan</title><link>https://nsynovic.dev/summaries/advances-in-human-activity-recognition/</link><pubDate>Sat, 18 Feb 2023 18:51:36 -0600</pubDate><guid>https://nsynovic.dev/summaries/advances-in-human-activity-recognition/</guid><description>&lt;h1 id="a-summary-of-advances-in-human-activity-recognition">A summary of &lt;em>Advances in Human Activity Recognition&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Gulustan Dogan IEEE Computer Volume 55; Issue 4, 2022 DOI [0]&lt;/p>
&lt;/blockquote>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-advances-in-human-activity-recognition">A summary of &lt;em>Advances in Human Activity Recognition&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#this-method-is-great-when-working-on-raw-data-streams-but-there-do-exist-better-algorithms-and-models-to-handle-visual-representations-of-movement">This method is great when working on raw data streams, but there do exist better algorithms and models to handle visual representations of movement.&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Human activity recognition (HAR) involves classifying sequences of accelerometer
data together to identify defined movements. Current solutions involve hand
crafting features (thereby requiring an expert of the space to assist), or by
training machine learning models using decision trees.&lt;/p>
&lt;p>Long short term memory (LSTM) models are currently the most powerful type of
recurrent neural networks (RNNs). LSTMs are great at identifying and predicting
sequential information as they take both time and sequence in to account.
However, LSTMs are computationally expensive.&lt;/p>
&lt;h2 id="the-current-state-of-the-art-devises-an-algorithm-that-takes-in-raw-signal-or-visual-data-and-can-identify-patterns-and-sequences-of-movement-1-this-method-is-great-when-working-on-raw-data-streams-but-there-do-exist-better-algorithms-and-models-to-handle-visual-representations-of-movement">The current state of the art devises an algorithm that takes in raw signal or visual data, and can identify patterns and sequences of movement [1]. This method is great when working on raw data streams, but there do exist better algorithms and models to handle visual representations of movement.&lt;/h2>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/advances-in-human-activity-recognition/</description></item><item><title>A summary of Challenges and Opportunities for Autonomous Micro-UAVs in Precision Agriculture by Xu Liu et al.</title><link>https://nsynovic.dev/summaries/challenges-and-opportunities-for-autonomous-micro-uavs-in-precision-agriculture/</link><pubDate>Sat, 18 Feb 2023 13:08:36 -0600</pubDate><guid>https://nsynovic.dev/summaries/challenges-and-opportunities-for-autonomous-micro-uavs-in-precision-agriculture/</guid><description>&lt;h1 id="a-summary-of-challenges-and-opportunities-for-autonomous-micro-uavs-in-precision-agriculture">A summary of &lt;em>Challenges and Opportunities for Autonomous Micro-UAVs in Precision Agriculture&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Xu Liu et al. IEEE Computer Volume 55; Issue 4, 2022 DOI [0]&lt;/p>
&lt;/blockquote>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-challenges-and-opportunities-for-autonomous-micro-uavs-in-precision-agriculture">A summary of &lt;em>Challenges and Opportunities for Autonomous Micro-UAVs in Precision Agriculture&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#uav-hardware-and-autonomy">UAV Hardware and Autonomy&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#uav-platforms-and-autonomy">UAV Platforms and Autonomy&lt;/a>&lt;/li>
&lt;li>&lt;a href="#sensor-configuration">Sensor Configuration&lt;/a>&lt;/li>
&lt;li>&lt;a href="#challenges">Challenges&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#object-detection-and-segmentation">Object Detection and Segmentation&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#image-based-2-d">Image-Based (2-D)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#lidar-based-3-d">LiDAR-Based (3-D)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#challenges-1">Challenges&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#robot-localization-and-mapping">Robot Localization and Mapping&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#semantic-localization-and-mapping">Semantic Localization and Mapping&lt;/a>&lt;/li>
&lt;li>&lt;a href="#challenges-2">Challenges&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Unmanned ground and areal vehicles (UGVs and UAVs respectfully) are utilized for
precision agriculture. UGVs can carry a larger payload and can run longer, but
suffer from only being operational within 2D space. UAVs have the benefit of
being able to navigate rougher terrains in 3D space, but suffer from limited
flight times and smaller payloads. This article aims to survey the recent
advances in UAV technologies applied to precision farming and to present
opportunities for improvement.&lt;/p>
&lt;p>There are many agriculture issues that exist in our world today. Over 700
million people are malnourished, 70% of fresh water is utilized by agriculture
domains, and advancements made on micro scales do not scale to the macro
environment. Having more data from autonomous sensors and vehicles will help
improve the realizations of scientific advancements in scale.&lt;/p>
&lt;p>Current UGV technologies suffer from being able to only see points of interest
close to vehicle, they can not survey large areas quickly, and they are unable
to navigate rough agriculture environments (i.e.,, rice fields). UAVs do not
suffer from these drawbacks, however, to work for precision agriculture, UAV
technologies need to work close to crops (under canopy flight), must have
reliable relative coordinate reporting, handle a dynamic environment in PD space
(i.e.,, wind, rocks, hills), and must be able to map dense environments. There
have been some under canopy tests that have been performed to limited success.
However, these were only applied in small scale environments and haven&amp;rsquo;t been
tested in larger environments.&lt;/p>
&lt;h3 id="uav-hardware-and-autonomy">UAV Hardware and Autonomy&lt;/h3>
&lt;p>Current autonomous UAV technologies are available, but their usage is limited.
They currently are reliant upon GPS, which requires an open canopy, making under
canopy flight currently impossible. Additionally, complex tasks such as
segmentation of fruits or trees is not possible at this time.&lt;/p>
&lt;h4 id="uav-platforms-and-autonomy">UAV Platforms and Autonomy&lt;/h4>
&lt;p>Several different autonomous systems have been proposed for under-canopy
autonomous flight.&lt;/p>
&lt;p>These include:&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;&amp;hellip; A stereo visual-inertial odometry (VIO) algorithm was used for state
estimation, a 2-D light detection and ranging (LiDAR) mounted on a nodding
gimbal was used for mapping and obstacle avoidance, and a search-based motion
planner was used for motion primitives plans collision-free and dynamically
feasible trajectories,&amp;rdquo; [1]&lt;/li>
&lt;li>A vision based solution that can navigate in both structured and moderately
unstructured environments (i.e., a collapsed building) [2]&lt;/li>
&lt;li>A system of using many UAVs that work together and coordinate through a
simultaneous localization and mapping (SLAM) scheme with loop closure that
utilized trees as landmarks was tested [3], but could fail when operating
within a dense forest&lt;/li>
&lt;/ul>
&lt;p>A limitation with all of these systems so far is that they are unable to work in
long range agricultural missions and they are unable to identify objects in real
time and at scale.&lt;/p>
&lt;h4 id="sensor-configuration">Sensor Configuration&lt;/h4>
&lt;p>Sensors on UAVs are used for both autonomy and for collecting mission specific
data. The most common sensors on UAVs are cameras, inertial measurement units
(IMUs), LiDARs, and global navigation satellite systems (GNSS). These sensors
must be lightweight because multi-rotor UAVs consume 100 - 200 W/kg.&lt;/p>
&lt;p>Cameras and IMUs are great for navigation and obstacle avoidance. However,
cameras are easily susceptible to changes in lightness and darkness. Thus
obstacle avoidance becomes difficult when lighting is patchy.&lt;/p>
&lt;p>LiDAR sensors can be used to reduce this issue, however, current LiDAR
technology is expensive, heavy, and still under much research.&lt;/p>
&lt;p>GNSS technologies (i.e., GPS) allow for geospatial positioning. However, if
there are obstacles in the way, the accuracy decreases. There do exists
optimizations to improve geospatial coordination and positioning, such as GPS
ground stations (DGPS) and real time kinematics (RTK). However, these solutions
must be both real time and reliable to resolve accuracy concerns.&lt;/p>
&lt;h4 id="challenges">Challenges&lt;/h4>
&lt;p>Detection of small obstacles is difficult with conventional camera systems. Thus
a forward facing, solid state LiDAR solution has been proposed to mitigate this.
However, the 360 degree view that LiDAR provides is lost because all of the
LiDAR beams are focused at the front of the device in order to gain resolution.&lt;/p>
&lt;p>Smaller UAVs can be more nimble, however, there is a weight to power and a
weight to flight time concern with these devices.&lt;/p>
&lt;p>Running deep neural network (DNN) algorithms on board a UAV is critical for low
latency, real time data collection, estimation, and understanding. However,
current DNN algorithms are computationally expensive to run. It is predicted
that more efficient algorithms, as well as the usage of AI accelerators, will
help mitigate this problem.&lt;/p>
&lt;h3 id="object-detection-and-segmentation">Object Detection and Segmentation&lt;/h3>
&lt;p>Object detection and segmentation are critical to precise agriculture as plant
or fruit specific data can be captured and acted upon.&lt;/p>
&lt;h4 id="image-based-2-d">Image-Based (2-D)&lt;/h4>
&lt;p>RGB, multi and hyper spectral imaging, thermal, and near-infrared imaging have
been used to perform object detection on plants. Previous methods involved using
K-Means algorithms and SVMs to solve detection and segmentation problems.
Recently, DNN based solutions are becoming more popular and additional sensor
data from the ground is also inputted into these algorithms to provide more
accurate results.&lt;/p>
&lt;h4 id="lidar-based-3-d">LiDAR-Based (3-D)&lt;/h4>
&lt;p>LiDAR based CV solutions are relatively new to the agriculture space. To
represent the problem domain, LiDAR captures data in the forms including a voxel
grid, point clouds, and multi-view and/or spherical images. It has been found
that voxel grid based convolutional neural networks (CNNs) are susceptible to
noise, whereas point clouds are not as affected. It is possible to join LiDAR
point cloud data and a spherical range image together and pass the union of this
data into a CNN to reduce information loss [4].&lt;/p>
&lt;h4 id="challenges-1">Challenges&lt;/h4>
&lt;p>It is difficult to acquire large, high-quality agriculture specific datasets to
train models on object detection and segmentation. Furthermore, occlusion
(e.g.,, not being able to see the plant or fruit) is still a problem that is
trying to be solved.&lt;/p>
&lt;h3 id="robot-localization-and-mapping">Robot Localization and Mapping&lt;/h3>
&lt;p>Mapping refers to the act of creating an understanding of an environment for an
autonomous robot to adhere to. Potential solutions require the input of
knowledge about the structure of the field.&lt;/p>
&lt;h4 id="semantic-localization-and-mapping">Semantic Localization and Mapping&lt;/h4>
&lt;p>Semantic features allows the robot to generate a meaningful map of the
environment and assist in pose estimation. The usage of locating and
representing trees as points of interests has been studied and found to be
useful for identifying local regions.&lt;/p>
&lt;h4 id="challenges-2">Challenges&lt;/h4>
&lt;p>SLAM is able to performing mapping quite well in man made environments. However,
new technologies must be developed to assist with the mapping of natural
environments. Active mapping (where an autonomous agent maps out its environment
in real time) is difficult to do in an agriculture context as fields can be
quite large and contain a dense information mapping.&lt;/p>
&lt;hr>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/challenges-and-opportunities-for-autonomous-micro-uavs-in-precision-agriculture/</description></item><item><title>A summary of Efficient Computer Vision for Embedded Systems by George K. Thiruvathukal and Yung-Hsiang Lu</title><link>https://nsynovic.dev/summaries/efficient-computer-vision-for-embedded-systems/</link><pubDate>Sat, 18 Feb 2023 10:41:18 -0600</pubDate><guid>https://nsynovic.dev/summaries/efficient-computer-vision-for-embedded-systems/</guid><description>&lt;h1 id="a-summary-of-a-summary-of-efficient-computer-vision-for-embedded-systems-0">A summary of &lt;em>A summary of Efficient Computer Vision for Embedded Systems&lt;/em> [0]&lt;/h1>
&lt;blockquote>
&lt;p>George K. Thiruvathukal and Yung-Hsiang Lu IEEE Computer Volume 55; Issue 4,
2022 &lt;a href="https://doi.org/10.1109/MC.2022.3145677">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-a-summary-of-efficient-computer-vision-for-embedded-systems-0">A summary of &lt;em>A summary of Efficient Computer Vision for Embedded Systems&lt;/em> [0]&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#lpcvc">LPCVC&lt;/a>&lt;/li>
&lt;li>&lt;a href="#lpcvc-research">LPCVC Research&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;blockquote>
&lt;p>Disclosure: Both George K. Thiruvathukal and Yung-Hsiang Lu are mentors,
colleagues, and friends of mine. Disclosure: I am a student organizer of the
2023 Low Power Computer Vision competition.&lt;/p>
&lt;/blockquote>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Computer Vision (CV) research and development is encouraged through competitions
that measure the accuracy of models for cash prizes. However, as more CV
technologies are being pushed towards the edge, power and computational
efficiency of these models become increasingly more important. Therefore, the
&lt;em>IEEE Low Power Computer Vision Challenge&lt;/em> (LPCVC) [1], formerly the &lt;em>Low
Power Image Recognition Challenge&lt;/em>, was created to encourage researchers to
develop efficient low power solutions.&lt;/p>
&lt;h3 id="lpcvc">LPCVC&lt;/h3>
&lt;blockquote>
&lt;p>&lt;em>Why did you participate in LPCVC?&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>Researchers participate to develop and formalize techniques for running machine
learning on the edge. This could be designing new hardware to accelerate CV
models and/or validating optimization and software techniques developing new CV
models. Additionally, this challenge encourages the discovery of new research
topics to pursue.&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>How is LPCVC relevant to activities in the IEEE Computer Society&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>The development and support of CV research is crucial the Computer Society&amp;rsquo;s
mission. The technical community, &lt;em>Technical Community on Pattern Analysis and
Machine Intelligence&lt;/em> [2], exists to promote the development of research and
solutions to problems surrounding and involving CV. Furthermore, the &lt;em>Computer
Vision and Pattern Recognition&lt;/em> (CVPR) conference [3] is currently IEEE&amp;rsquo;s most
influential conference as ranked by &lt;em>Guide2Research&lt;/em> [4].&lt;/p>
&lt;h3 id="lpcvc-research">LPCVC Research&lt;/h3>
&lt;blockquote>
&lt;p>&lt;em>Why is research in LPCV important?&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>As AI on the edge becomes more ubiquitous and desired by consumers, academic
research my provide industry with potential solutions to implement on the edge.
Furthermore, there exists a hardware challenge alongside the software challenge
of deploying LPCV solutions on edge. Thus hardware focused research must occur
to assist in LPCV optimizations. Hardware devices such as GPUs, CPUs, and Neural
Processing Units (NPUs) need to be designed and optimized (w.r.t hardware and
software) to support CV applications on the edge. Therefore, LPCV research
involves the union of power efficient designs and optimizations of both the
deployment hardware and solution software.&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>Can you describe one (or several) &amp;ldquo;grand challenges&amp;rdquo; using CV; the solutions
will significantly change the world, but are they far beyond today&amp;rsquo;s
technologies?&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>Better smart systems (e.g.,, smart homes, retail, factory, transportation,
farming, etc.) and prediction of natural disasters and phenomenon by utilizing
many data points can be possible through LPCV.&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>If you have unlimited resources, what would you like to see in the area of
LPCV?&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>Researchers in the space are looking for new datasets, challenges, and problem
specific competitions to advance research in LPCV.&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/efficient-computer-vision-for-embedded-systems/</description></item><item><title>A summary of The Loudest Duck by Laura Liswood</title><link>https://nsynovic.dev/summaries/the-loudest-duck/</link><pubDate>Tue, 07 Feb 2023 13:19:03 -0600</pubDate><guid>https://nsynovic.dev/summaries/the-loudest-duck/</guid><description>&lt;h1 id="a-summary-of-the-loudest-duck">A summary of &lt;em>The Loudest Duck&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Written by Laura Liswood&lt;/p>
&lt;/blockquote>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-the-loudest-duck">A summary of &lt;em>The Loudest Duck&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#chapter-summaries">Chapter Summaries&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#acknowledgments">Acknowledgments&lt;/a>&lt;/li>
&lt;li>&lt;a href="#introduction">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#chapter-1-beware-of-noahs-ark">Chapter 1: Beware of Noah&amp;rsquo;s Ark&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#objections-to-diversity">Objections to Diversity&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#chapter-2-the-elephant-and-the-mouse">Chapter 2: The Elephant and the Mouse&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#combining-forces">Combining Forces&lt;/a>&lt;/li>
&lt;li>&lt;a href="#point-of-view">Point of View&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#chapter-3-tell-your-grandma-to-go-home">Chapter 3: Tell Your Grandma to Go Home&lt;/a>&lt;/li>
&lt;li>&lt;a href="#necessary-but-not-sufficient">Necessary but not Sufficient&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#preconceived-notions-have-roots">Preconceived Notions Have Roots&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#chapter-4-whats-easy-for-you-is-hard-for-me-and-how-to-navigate-the-differences">Chapter 4: What&amp;rsquo;s Easy for You Is Hard for Me and How to Navigate the Differences&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#getting-noticed-in-noahs-ark">Getting Noticed in Noah&amp;rsquo;s Ark&lt;/a>&lt;/li>
&lt;li>&lt;a href="#getting-out-of-your-own-comfort-zone">Getting Out of Your Own Comfort Zone&lt;/a>&lt;/li>
&lt;li>&lt;a href="#critical-feedback">Critical Feedback&lt;/a>&lt;/li>
&lt;li>&lt;a href="#who-apologizes-and-who-interrupts">Who Apologizes and Who Interrupts&lt;/a>&lt;/li>
&lt;li>&lt;a href="#mentoring">Mentoring&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#chapter-5-unwritten-rules">Chapter 5: Unwritten Rules&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#subtle-inequities">Subtle Inequities&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#chapter-6-we-hire-for-difference-and-fire-because-they-are-not-the-same">Chapter 6: We Hire for Difference and Fire Because They Are Not the Same&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#the-danger-of-unconscious-thinking-speaking-and-acting">The Danger of Unconscious Thinking, Speaking, and Acting&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#chapter-7-the-tools-in-your-toolbox">Chapter 7: The Tools in Your Toolbox&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#think-about-the-people-on-your-team">Think About the People on Your Team&lt;/a>&lt;/li>
&lt;li>&lt;a href="#learn-to-recognize-other-peoples-grandmas">Learn to Recognize Other People&amp;rsquo;s &amp;ldquo;Grandmas&amp;rdquo;&lt;/a>&lt;/li>
&lt;li>&lt;a href="#fair-and-equal-with-access-knowledge-and-feedback">Fair and Equal With Access, Knowledge, and Feedback&lt;/a>&lt;/li>
&lt;li>&lt;a href="#be-careful-with-your-words-and-how-you-interpret-the-words-of-others">Be Careful With Your Words, and How You Interpret the Words of Others&lt;/a>&lt;/li>
&lt;li>&lt;a href="#the-silent-have-something-to-say">The Silent Have Something to Say&lt;/a>&lt;/li>
&lt;li>&lt;a href="#results-should-be-the-determinant">Results Should be the Determinant&lt;/a>&lt;/li>
&lt;li>&lt;a href="#conclusion">Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="chapter-summaries">Chapter Summaries&lt;/h2>
&lt;blockquote>
&lt;p>Chapter specific summaries&lt;/p>
&lt;/blockquote>
&lt;h3 id="acknowledgments">Acknowledgments&lt;/h3>
&lt;p>&amp;ldquo;No book is an island&amp;rdquo;. In other words, it takes many authors and contributors
who, while not the primary author(s), deserve to be recognized for their
contributions. However, all errors in the book are the primary author&amp;rsquo;s
responsibility.&lt;/p>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>The need for a diverse workforce has only grown as we as a species have become
increasingly more interconnected. However, diversity efforts often look like
Noah&amp;rsquo;s Ark: two of each kind of diversity. This only makes the problem look like
its solved and yet it hasn&amp;rsquo;t.&lt;/p>
&lt;p>Diversity often gets eye rolled as an annoyance. It also gets a bad rep as
people think that diversity efforts favor some over others. However, diversity
itself isn&amp;rsquo;t the problem. It is the unconscious biases that we bring into the
workplace that are the problem.&lt;/p>
&lt;p>Changes with respect to diversity start from the top of the organization and
move downward. Leaders can&amp;rsquo;t expect diversity changes without recognizing and
addressing their own unconscious biases.&lt;/p>
&lt;p>Corporate power composition is changing. There are slowly more non-white male
leaders entering and taking the lead at companies internationally. And for
companies working within the global market, the workplace is now a bag of many
individuals from different backgrounds.&lt;/p>
&lt;p>True diversity requires changes to the unconscious bias. &lt;em>Dominant groups&lt;/em> with
respect to diversity refer to those who have traditionally held power.
&lt;em>Non-dominant groups&lt;/em> are those that have been typically under represented, feel
less entitled to their position, and need to navigate the workplace differently
than those in dominant groups.&lt;/p>
&lt;p>Diversity is more than skin color. It is a reflection of someone&amp;rsquo;s upbringing.
Two white men with the same views about everything but favor rival sports teams
are a diverse group. This seemingly insignificant difference can result in an
unconscious bias about the other between the two men. Because of issues like
these, where skin color is not the issue but cultural, age, gender, and
religious biases are, we must move beyond the concept of the Noah&amp;rsquo;s Ark of
diversity.&lt;/p>
&lt;p>Most people within the dominant group believe that the workplace is a
meritocracy. Whereas the non-dominant group members differ and believe that
because of their differences between them and the dominant group that they
didn&amp;rsquo;t receive the same merits as often. They rather felt like poster children
for diversity issues. As white men are typically the dominant group, they are
often surprised to hear this. That is why moving beyond diversity 1.0 (Noah&amp;rsquo;s
Ark) to diversity 2.0 is so important.&lt;/p>
&lt;p>Unlike men, women often find that their lives often don&amp;rsquo;t have much control and
that, &amp;ldquo;things happen&amp;rdquo;. Mary Catherine Bateson is anthropologist and resident of
the Institute for Intercultural Studies [3] whose book, &lt;em>Composing a Life&lt;/em>
[1] talks about this topic.&lt;/p>
&lt;p>Conscious leadership does matter. Psychologist and Harvard professor Howard
Gardner&amp;rsquo;s book, &lt;em>Leading Minds&lt;/em> [2] describes great leaders of having the
following four traits:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>1. A True North that serves as an internal compass of values
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>2. The willingness to challenge authority
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>3. The practical skills to communicate to others
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>4. The ability to travel outside their own worldview
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This framework, as well as the additional qualities coupled from the Center for
American Woman and Politics at Rutgers University [4] kick started the
research for this book.&lt;/p>
&lt;p>Women politicians operate differently than their male counterparts with respect
to interacting with constituents, speaking in committee meetings, and bill
proposals. If this is the case, what would a woman American president look like?
With this in mind, interviews with each of the 15 (fifteen) women world leaders
of the world were scheduled and whose discussions would be used to help compile
this book. Through these meetings, the Liswood found out that women&amp;rsquo;s voice
often rise at the end of sentences. Because of this small pattern change that
men didn&amp;rsquo;t do, women are often treated differently then men. Margaret Thatcher
actually went to a speech therapist to drop this speech pattern. However in
doing so she lost a bit of her diverse self by conforming to what other&amp;rsquo;s
expected out of her.&lt;/p>
&lt;p>Diversity is not the problem. Dominant and non-dominant groups are not at fault
for failing to make diversity work. Rather, it is the unconscious handling of
diversity that is at fault and causes many of the issues that we experience with
respect to diversity.&lt;/p>
&lt;h3 id="chapter-1-beware-of-noahs-ark">Chapter 1: Beware of Noah&amp;rsquo;s Ark&lt;/h3>
&lt;p>Too many leaders think that if you take two of each kind of diversity, that you
have achieved diversity. This incentives creating a numbers game surrounding
diversity, rather than incorporating diverse workers and leaders in the
leadership, management, and execution stages of corporations.&lt;/p>
&lt;p>A Noah&amp;rsquo;s Ark will result in clashes between diverse groups because of
unconscious and conscious biases, and results in the problems that diversity
gets a bad reputation for. Problems will continue to arise in the workplace
until we understand our own biases about others. Only then can we reap the
benefits of diversity.&lt;/p>
&lt;p>The benefits of diversity include increased creativity, better ideas, and
multiple perspectives. This was explained by journalist James Surowiecki in his
book &lt;em>The Wisdom of Crowds&lt;/em> [5]. Introducing a heterogeneous member to a
homogeneous group will result in the group not bonding at first, however, the
cognitive diversity and creative output will be much greater than the original
homogeneous group&amp;rsquo;s once the full potential has been reached.&lt;/p>
&lt;p>The Washington Post reported in it&amp;rsquo;s survey article, &lt;em>Most Diversity Training
Ineffective, Study Finds&lt;/em> [6] that compulsory diversity training resulted in
diverse people leaving companies or leadership positions. Voluntary diversity
training that furthered company goals resulted in better diversity practices.
Changes to improve better diversity practices need to involve addressing the
biases of others. These changes often start with the unthinkable, then the
impossible, and eventually become inevitable. This is difficult though because
diversity requires change. To kick off better diversity, it only takes a small
few to start the inevitable.&lt;/p>
&lt;p>Diversity 2.0 can only start once Noah&amp;rsquo;s Ark has been built. But what
conflicting views can exist within the Ark? Views can include the opinions of
the national origins of coworkers, age, culture, religion, gender, sexual
orientation, socioeconomic or class, marital status, family, language,
positions, titles, seniority, work location within an organization, hobbies,
and/or physical appearance.&lt;/p>
&lt;p>National origins refer to what one nationality thinks of another. The
stereotypes and ideas about who one based on where they are from by coworkers of
different nationalities can cause rifts.&lt;/p>
&lt;p>Age refers to the expectations that we place on people to accomplish a task
based on the general skill level, trend, or ideas that a particular age group is
stereotyped as having.&lt;/p>
&lt;p>Culture refers to the cultural differences between coworkers. This could include
formalities, dining, rituals, habits, and other differences based on where
someone grew up. These can be best detected by thoughts along the line of,
&amp;ldquo;That&amp;rsquo;s not what we do here&amp;rdquo;.&lt;/p>
&lt;p>Religion is often polarizing. However, it also implicitly and explicitly affects
how someone&amp;rsquo;s culture is defined. Furthermore, holidays are another factor that
needs to be considered when working towards a truly diverse work place.&lt;/p>
&lt;p>Socioeconomic or class refer to the financial status of individuals. Rich and
poor people have different problems, goals, and management strategies with
respect to their finances.&lt;/p>
&lt;p>Marital status is often hand in hand with gender politics.&lt;/p>
&lt;p>Family status is often difficult to handle. For managers, questions about when
the individual is leaving, who to replace them with, and what the environment
will be like while they are pregnant or planning a pregnancy are both
reasonable, but bordering judgmental, questions. They also can be used against
an employee, even if not meant to intentionally to cause harm.&lt;/p>
&lt;p>Languages and language dialects can be used to classify individuals.
Additionally, as English is the predominant language in the workplace, the more
English that a non-English speaker knows, the &amp;ldquo;smarter&amp;rdquo; they appear to some.
This is because they are finally able to communicate their already smart ideas
in a different language.&lt;/p>
&lt;p>Access to managers and leaders often disparages those who don&amp;rsquo;t have access to
those individuals. Those that do often receive the promotion, insider
information, or other merit. This is because the manager or leader has gotten a
chance to become familiar and trust the individual, and thus feels comfortable
handing this position to them.&lt;/p>
&lt;p>There are other types of diversity in the Ark too that are not discussed in this
book. Regardless, efforts must be made to identify and address the biases with
respect to these diverse groups.&lt;/p>
&lt;p>Sometimes we go looking for a trait in someone. For example, building a leader
from nothing simply because they meet physical definition of a leader.&lt;/p>
&lt;p>Confirmation bias is best characterized as, &amp;ldquo;I know what I know because I know
it&amp;rdquo;. But organizations will not benefit from hiring for diversity as long as
they continue to hire what they know is best.&lt;/p>
&lt;h4 id="objections-to-diversity">Objections to Diversity&lt;/h4>
&lt;p>The following objections are common to hear from the dominant group when
presented with diversity related decisions:&lt;/p>
&lt;ul>
&lt;li>Reverse discrimination&lt;/li>
&lt;li>Defies the meritocracy of the organization&lt;/li>
&lt;li>Coddling the diverse individuals&lt;/li>
&lt;li>Lack of evidence&lt;/li>
&lt;li>Rocking the boat&lt;/li>
&lt;li>It&amp;rsquo;s all about the law&lt;/li>
&lt;li>Don&amp;rsquo;t blame me for the success that I&amp;rsquo;ve had&lt;/li>
&lt;/ul>
&lt;h3 id="chapter-2-the-elephant-and-the-mouse">Chapter 2: The Elephant and the Mouse&lt;/h3>
&lt;p>In power structures their are elephants and mice. Elephants don&amp;rsquo;t need to know
much about mice when they walk into the room. Mice need to know everything about
elephants in order to make sure they aren&amp;rsquo;t crushed. Dominant groups are
elephants; non-dominant groups are mice and build a skill set around being a
successful mouse. Because of this, elephants think that the world operates the
same way for everyone as it does for them.&lt;/p>
&lt;p>However leaders need both the skills that elephants and mice have. Leaders need
to learn just as much about their constituents and team as the team learns about
them. Emotional intelligence is key for leaders to learn. Being able to perceive
how they as well as others are feeling when they walk into the room is a
characteristic mouse technique but is critical for leadership. Additionally,
agile and adaptive skills are extremely useful. With respect to diversity, they
need to understand that what they experience might not reflect what others are
experiencing around them. However, it is still crucial for leaders to be
confident, responsible, and able to move forward with tough decisions.&lt;/p>
&lt;p>If an elephant abuses their position of power, the mice might revolt. Small
players can easily take down a larger player by biding their time and using
their emotional intelligence to know when to strike.&lt;/p>
&lt;p>If there is an asymmetrical power dynamic, the power goes to those who are more
independent. It is possible to look at the different diversity types from
&lt;a href="#chapter-1-beware-of-noahs-ark">Chapter 1&lt;/a> and identify different elephant and
mouse positions.&lt;/p>
&lt;h4 id="combining-forces">Combining Forces&lt;/h4>
&lt;p>Being a leader requires flexibility and the desire to learn new qualities to
better lead. For example, if a leader can&amp;rsquo;t communicate, they can&amp;rsquo;t lead. It is
important to relate to people, and to have others perceive leaders as one of
their own.&lt;/p>
&lt;h4 id="point-of-view">Point of View&lt;/h4>
&lt;p>Someone&amp;rsquo;s point of view of their working style can be seen as the correct way
and other&amp;rsquo;s as wrong because they don&amp;rsquo;t do this. Being able to step outside of
yourself and identify that your point of view may not be the correct way or that
others aren&amp;rsquo;t experiencing the world as you do is crucial for leadership. These
biases towards a particular viewpoint need to be understood and addressed to
achieve diversity 2.0.&lt;/p>
&lt;h3 id="chapter-3-tell-your-grandma-to-go-home">Chapter 3: Tell Your Grandma to Go Home&lt;/h3>
&lt;p>Grandma refers to societies implicit and explicit teachings that leaders carry
with them. Experiences, the media, myths and legends, as well as our belief
system make up Grandma. Additionally, the lessons that we learned from family or
mentors also makes up Grandma.&lt;/p>
&lt;p>Grandma is different from nation to nation, or more broadly, culture to culture.
In America the saying, &amp;ldquo;The squeaky wheel gets the grease,&amp;rdquo; means that speaking
up and being assertive is the best. However in Japan the equally popular saying,
&amp;ldquo;The nail that sticks out gets hit on the head,&amp;rdquo; means to be punished if
assertive. Likewise, in China they have the saying, &amp;ldquo;The loudest duck gets
shot&amp;rdquo;. Furthermore, women in America are taught that, &amp;ldquo;If you cant say anything
nice, dont say anything at all,&amp;rdquo; which means to not be assertive if they don&amp;rsquo;t
have an agreeing opinion.&lt;/p>
&lt;p>So if a Wheel, Nail, Duck, and Nice were in a strategy meeting with their boss,
the Wheel would dominate the conversation and potential receive a merit because
of their outspoken-ess whereas everyone else could be subtly or overtly punished
for their lack of contributions. However, that was only because the Nail, Duck,
and Nice learned from their Grandma, &amp;ldquo;proper etiquette,&amp;rdquo; which was wrong in this
case. This is an example of how diversity failed as the cognitive diversity that
was initially desired.&lt;/p>
&lt;h3 id="necessary-but-not-sufficient">Necessary but not Sufficient&lt;/h3>
&lt;p>Many corporations make strong business cases for diversity, such as the war for
talent, higher profitability, the global nature of the firm, and/or the legal
scrutiny and laws for diversity in many countries. They have additionally put
the building blocks in place for diversity, such as diversity training,
senior-level rhetoric, and diversity related communities. While these are
foundational, they are not enough for true diversity as we still bring Grandma,
and thus all of our conscious and unconscious biases to work.&lt;/p>
&lt;h4 id="preconceived-notions-have-roots">Preconceived Notions Have Roots&lt;/h4>
&lt;p>Harvard social psychologist Mahzarin Banaji has been studying the irreplaceable
notions that are ingrained in the human brain. She has found that there exists
certain unconscious reactions that your brain gives off to different faces.
Additionally, she found that this preference starts around the age of 6 (six),
when we start to hang out with others who are more like us. As we age, different
faces become naturally associated with positive and negative emotions.&lt;/p>
&lt;p>Our Unconscious thinking comes from the following sources:&lt;/p>
&lt;ul>
&lt;li>Our parents, as they lay out expectations and guide them to the best of their
knowledge&lt;/li>
&lt;li>Our experiences, as they tell us not only how the world works but also how it
should work&lt;/li>
&lt;li>Our peers, as they show us what to do to fit in&lt;/li>
&lt;li>Myths, fairy tales, fables, and the media as they carry long standing
archetypes throughout the generations&lt;/li>
&lt;/ul>
&lt;h3 id="chapter-4-whats-easy-for-you-is-hard-for-me-and-how-to-navigate-the-differences">Chapter 4: What&amp;rsquo;s Easy for You Is Hard for Me and How to Navigate the Differences&lt;/h3>
&lt;p>Most manager&amp;rsquo;s would say that an underappreciated, hardworking employee gets
left behind, that they should speak up. However, what if that employee is a
diverse individual? Thus the better solution would be for the manager to become
more perceptive to the employee and the employee overcoming some of the cultural
norms. Thus leaders need to have a greater degree of awareness and set of tools
than merely suggesting what the employee should or shouldn&amp;rsquo;t do.&lt;/p>
&lt;h4 id="getting-noticed-in-noahs-ark">Getting Noticed in Noah&amp;rsquo;s Ark&lt;/h4>
&lt;p>To be successful at work, employees must get their work noticed. However, as
diverse individuals, they might believe that the opposite (being unseen, working
hard, and not making a fuss) will lead to success. This could&amp;rsquo;ve been learned
through Grandma.&lt;/p>
&lt;p>Thus the problem with corporate diversity is that while everyone wants
diversity, only the Wheels will be heard and the Nails, Ducks, and Nices will be
forgotten or suffer. To address, this managers should ask for others ideas or
inputs. Additionally, informing employees that ideas are looking to be shared at
the meeting is another way of preparing employees for questions.&lt;/p>
&lt;p>Management is responsible for getting the benefits of diversity out of their
team.&lt;/p>
&lt;h4 id="getting-out-of-your-own-comfort-zone">Getting Out of Your Own Comfort Zone&lt;/h4>
&lt;p>Managers can&amp;rsquo;t take everything on themselves. To some degree, employees are
expected to take it into their own hands to adjust to the company&amp;rsquo;s style, but
not in a compliance-like manner. In other words, to adjust to the company&amp;rsquo;s
style while maintaining their diversity.&lt;/p>
&lt;p>Power issues arise regardless of gender. Men will typically dominate a mixed
gender group, whereas women will be more quiet. However, in an all women group
some women will be more assertive than others. This is not a gender issue then,
it is a power issue.&lt;/p>
&lt;p>There are clear power dynamics in for-profit companies. Larger offices at the
top, cubicles at the bottom; fewer higher paying jobs at the top, more lower
paying jobs at the bottom. The issue is not that there aren&amp;rsquo;t enough low-tier
employees talking to their managers, its that in a diverse company not everyone
is &lt;em>comfortable&lt;/em> doing so. Thus when a merit comes about, the manager gives it
to those that have kept them informed.&lt;/p>
&lt;p>Gender roles are very devise in the office. More man assume that they can run
for office than women. And when women do run for office, it is often for a
smaller role that they feel ready for. Whereas men will run for larger roles
that they may not be immediately qualified for. Additionally, women estimate
fair pair 4% less than men early in their careers, and 23% less then men at the
peak of their careers. Furthermore, in a study involving counting dots until
participants felt like they could stop, women counted 22% longer than men and
32% more dots. Women also get labeled as bitchy and pushy when being assertive
and that Grandma taught them to not be assertive. The takeaway here is that
gender plays into the pursuit of merits. Overtime in a diverse culture, the
lifetime earnings of diverse individuals will be less then men should they not
follow the same path as men. That doesn&amp;rsquo;t mean to conform to how men operate,
but that being assertive and getting out of your comfort zone does pay off,
however it can be an uphill battle if your company doesn&amp;rsquo;t support diverse
initiatives.&lt;/p>
&lt;h4 id="critical-feedback">Critical Feedback&lt;/h4>
&lt;p>Managers need to give employees critical feedback, stretch assignments, and
mentoring in order for the employees to prosper. Typically though, only those in
the dominant group are given these benefits. In a study of employee performance
between managers and the employees themselves, there was a large disparity of
what managers thought the skill level of employees from the non-dominant group
vs the employees themselves.&lt;/p>
&lt;p>However, managers dealing with diverse employees worry about their critical
feedback not being received positively. Common concerns involve being called
racist, sexist, or subject to litigation. So managers scrape by by telling their
employees that they are fine. Fine could very well mean, &amp;ldquo;Feelings Inside Not
Expressed&amp;rdquo; though.&lt;/p>
&lt;p>Positive allusion is a sociological trait that men have developed where they
hear the positives about themselves first. Women have developed negative
allusion which is the converse of the above statement. This causes men to give
other men direct, critical feedback, and women to approach the same subject in
an indirect relational fashion. The best way to clearly give feedback is to ask,
&amp;ldquo;What did you hear me say?&amp;rdquo; as this then allows the two parties to make sure
that they are on the same page.&lt;/p>
&lt;p>Men typically are allowed to get angry in the workplace and express frustration.
Whereas women are labeled as &amp;ldquo;bitchy&amp;rdquo; for the same behavior. However, if they
don&amp;rsquo;t, their frustrations are never heard. Thus this double bind position that
women are in (damned if they do, damned if they don&amp;rsquo;t) prevents women and other
non-dominant groups from prospering.&lt;/p>
&lt;h4 id="who-apologizes-and-who-interrupts">Who Apologizes and Who Interrupts&lt;/h4>
&lt;p>Women and other non-dominant groups have been taught by Grandma to perform
ritual apologies even if they did nothing wrong. Interrupting others is also
viewed differently depending on what group you are from. Managers must be aware
of these dynamics as well as to be able to support others when their voices get
left out. However, employees must also be able to stand up for themselves and
their voices when interrupted.&lt;/p>
&lt;h4 id="mentoring">Mentoring&lt;/h4>
&lt;p>Some mentoring pitfalls include:&lt;/p>
&lt;ul>
&lt;li>Negative stereotypes about the mentee based on race, gender, religion, etc..&lt;/li>
&lt;li>Mirroring, or seeing themselves in a mentee will result in that mentee getting
special treatment.&lt;/li>
&lt;li>Getting close or potentially inappropriate relationships between mentors and
mentees are highly discouraged. Furthermore, even the appearance of an
inappropriate relationship is discouraged. For example, a male mentor taking a
female mentee out to dinner.&lt;/li>
&lt;li>Risk in the mentee failing and ruining both of their relationships and vice
versa. This is especially true when mentoring someone in the non-dominant
group.&lt;/li>
&lt;li>Resentment by others when mentoring those who are different from you,
particularly with gender roles.&lt;/li>
&lt;/ul>
&lt;h3 id="chapter-5-unwritten-rules">Chapter 5: Unwritten Rules&lt;/h3>
&lt;p>Organizations and group have unwritten rules and customs that everyone is
expected to follow, regardless of if everyone knows them. These rules can be
cultural, implemented informally, or be how a boss wants others to act without
explicitly saying so. If they are violated, it could cause an individual to be
demerited for something that they may not be aware of. Thus having a mentor to
guide mentees into understanding these unspoken rules is crucial.&lt;/p>
&lt;h4 id="subtle-inequities">Subtle Inequities&lt;/h4>
&lt;p>Subtle inequities are small, insignificant, and potentially undetectable
inequities that build up over time that end up haunting the organization. Some
subtle inequities involve:&lt;/p>
&lt;ul>
&lt;li>Giving one employee a longer introduction than another&lt;/li>
&lt;li>Leaders only talking to those that they personally know when first arriving at
divisional offices&lt;/li>
&lt;li>Thinking that when non-dominant groups ask questions, that they are perceived
to be lacking knowledge or confidence&lt;/li>
&lt;li>Expecting that one individual to speak for all individuals who are similar to
them&lt;/li>
&lt;li>Not pronouncing foreign names correctly&lt;/li>
&lt;li>Seating your buddies next to you at meetings&lt;/li>
&lt;li>Making eye contact with some people but not others&lt;/li>
&lt;li>Apologizing to women if you swear&lt;/li>
&lt;li>Giving certain people bigger and better projects than others&lt;/li>
&lt;li>Always favorably commenting on the input of specific people, but not others&lt;/li>
&lt;/ul>
&lt;p>To reduce these, those in dominant groups or positions of power need to step
outside of themselves, critically evaluate their actions, and implement changes
to level the playing field for everyone. Additionally, making small changes to
reduce these subtle inequities can go a long way in improving the working
relationship and culture of an organization.&lt;/p>
&lt;h3 id="chapter-6-we-hire-for-difference-and-fire-because-they-are-not-the-same">Chapter 6: We Hire for Difference and Fire Because They Are Not the Same&lt;/h3>
&lt;h4 id="the-danger-of-unconscious-thinking-speaking-and-acting">The Danger of Unconscious Thinking, Speaking, and Acting&lt;/h4>
&lt;p>Words are the best way to get yourself into and out of trouble quickly. Thus
conscious, intentional, and mindful speaking is critical to thrive.&lt;/p>
&lt;p>Verbal Judo is using words to get someone to comply with or complete an action
or task. It is useful for diffusing difficult situations. Rather than order
someone to do something during a tough situation, talk to them calmly and give
them options. Instead of, &amp;ldquo;Stop that, it&amp;rsquo;s unsafe!&amp;rdquo;, a better option would be,
&amp;ldquo;For your safety and mine, please stop doing that or [insert consequence]&amp;rdquo;.&lt;/p>
&lt;p>Speaking consciously means that each sentence is intentional. To American, &amp;ldquo;How
are you doing?&amp;rdquo; means basically nothing. However, to a foreigner, that is a
meaningful question and when Americans don&amp;rsquo;t listen, we are perceived as rude.&lt;/p>
&lt;p>Men and women communicate information differently. Men typically prefer shorter
communication style. Hearing every single detail of a story is not as important
to them as the key takeaways. Additionally, men engage in verbal banter in a one
up/one down style where there is a winner or loser in a conversation. This
sparring is uncomfortable for some women, but is common for many men.&lt;/p>
&lt;p>Direct speaking (getting to the point in the most straightforward manner) is
common for those in dominant groups. Whereas indirect speaking (telling a story
to get to a point) is common for those in non-dominant groups. However, speaking
types aren&amp;rsquo;t characteristic of the entire group. But a problem arises when a
speaking type is expected of someone, but isn&amp;rsquo;t the case as this can demerit the
speaker to the listener.&lt;/p>
&lt;h3 id="chapter-7-the-tools-in-your-toolbox">Chapter 7: The Tools in Your Toolbox&lt;/h3>
&lt;p>The more tools that we have to handle diversity, the better leaders we will be
within diverse organizations.&lt;/p>
&lt;p>Managers need to be aware of their playing fields, or spheres of influence. They
need to be aware of who is in the dominant and non-dominant groups, whose quite
and whose loud, and what the career goals and accomplishments of individuals
are. They need to make sure everyone is heard, and that promotions are given out
based on merit and not personal relationships.&lt;/p>
&lt;h4 id="think-about-the-people-on-your-team">Think About the People on Your Team&lt;/h4>
&lt;p>If you are a manager, make sure that the events that you plan are accessible to
everyone.&lt;/p>
&lt;p>If you are an employee with unequal access to the manager, find ways to get to
know each other. This could involve suppressing your Grandma.&lt;/p>
&lt;h4 id="learn-to-recognize-other-peoples-grandmas">Learn to Recognize Other People&amp;rsquo;s &amp;ldquo;Grandmas&amp;rdquo;&lt;/h4>
&lt;p>If you are a manager, ask, listen, and be aware of who everyone is on your
field. Try to understand the power dynamics between team members. Understand how
you give feedback to others like and unlike yourself.&lt;/p>
&lt;p>If you are an employee, find ways to make your voice and yourself heard to your
manager. Staying silent is not always the best answer.&lt;/p>
&lt;h4 id="fair-and-equal-with-access-knowledge-and-feedback">Fair and Equal With Access, Knowledge, and Feedback&lt;/h4>
&lt;p>If you are a manager, understand that spending time with some employees means
that you don&amp;rsquo;t naturally spend time with other employees. Additionally, not
everyone will appreciate, understand, or get used to your speaking style.&lt;/p>
&lt;p>If you are an employee, understand the value and usefulness of having access to
your managers.&lt;/p>
&lt;h4 id="be-careful-with-your-words-and-how-you-interpret-the-words-of-others">Be Careful With Your Words, and How You Interpret the Words of Others&lt;/h4>
&lt;p>Understand your own unconscious biases and work towards reducing them when
others speak to you or when you speak to to others.&lt;/p>
&lt;h4 id="the-silent-have-something-to-say">The Silent Have Something to Say&lt;/h4>
&lt;p>If you are a manager, make sure everyone has the ability to speak to you during
meetings or privately. An open-door policy works best for this.&lt;/p>
&lt;p>If you are an employee, if you don&amp;rsquo;t speak, your ideas won&amp;rsquo;t be heard. Make sure
that you either speak up when possible, or ask to speak.&lt;/p>
&lt;h4 id="results-should-be-the-determinant">Results Should be the Determinant&lt;/h4>
&lt;p>Make sure that rewards are given out based on the quality and (sometimes)
quantity of the results of others, rather than on relationships.&lt;/p>
&lt;h4 id="conclusion">Conclusion&lt;/h4>
&lt;p>Once you create a Noah&amp;rsquo;s Ark of diversity, then you can start reaping the
rewards of diversity. But to do so, you must be conscious of your words and
actions. Furthermore, the company&amp;rsquo;s culture must change as well to go beyond
accepting, tolerating, including, and referencing diversity. Company&amp;rsquo;s must
understand the power dynamics, subtle actions, unwritten rules, unconscious
perceptions, and backgrounds of many different groups.&lt;/p>
&lt;hr>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/the-loudest-duck/</description></item><item><title>A summary of On Being a Scientist: A Guide To Responsible Conduct In Research by the National Academy of Science, National Academy of Engineering and Institute of Medicine of the National Academies</title><link>https://nsynovic.dev/summaries/on-being-a-scientist/</link><pubDate>Wed, 01 Feb 2023 12:48:53 -0600</pubDate><guid>https://nsynovic.dev/summaries/on-being-a-scientist/</guid><description>&lt;h1 id="a-summary-of-on-being-a-scientist-a-guide-to-responsible-conduct-in-research">A summary of &lt;em>On Being a Scientist: A Guide To Responsible Conduct In Research&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Written by the National Academy of Science, National Academy of Engineering
and Institute of Medicine of the National Academies&lt;/p>
&lt;/blockquote>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-on-being-a-scientist-a-guide-to-responsible-conduct-in-research">A summary of &lt;em>On Being a Scientist: A Guide To Responsible Conduct In Research&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#chapter-summaries">Chapter Summaries&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#the-national-academies-advisors-to-the-nation-on-science-engineering-and-medicine">The National Academies Advisors to the Nation on Science, Engineering, and Medicine&lt;/a>&lt;/li>
&lt;li>&lt;a href="#preface">Preface&lt;/a>&lt;/li>
&lt;li>&lt;a href="#a-note-on-using-on-being-a-scientist">A Note on Using &lt;em>On Being a Scientist&lt;/em>&lt;/a>&lt;/li>
&lt;li>&lt;a href="#introduction-to-the-responsible-conduct-of-research">Introduction to the Responsible Conduct of Research&lt;/a>&lt;/li>
&lt;li>&lt;a href="#advising-and-mentoring">Advising and Mentoring&lt;/a>&lt;/li>
&lt;li>&lt;a href="#treatment-of-data">Treatment of Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#mistakes-and-negligence">Mistakes and Negligence&lt;/a>&lt;/li>
&lt;li>&lt;a href="#research-misconduct">Research Misconduct&lt;/a>&lt;/li>
&lt;li>&lt;a href="#responding-to-suspected-violations-of-professional-standards">Responding to Suspected Violations of Professional Standards&lt;/a>&lt;/li>
&lt;li>&lt;a href="#human-participants-and-animal-subjects-in-research">Human Participants and Animal Subjects in Research&lt;/a>&lt;/li>
&lt;li>&lt;a href="#laboratory-safety-in-research">Laboratory Safety in Research&lt;/a>&lt;/li>
&lt;li>&lt;a href="#sharing-of-research-results">Sharing of Research Results&lt;/a>&lt;/li>
&lt;li>&lt;a href="#authorship-and-the-allocation-of-credit">Authorship and the Allocation of Credit&lt;/a>&lt;/li>
&lt;li>&lt;a href="#intellectual-property">Intellectual Property&lt;/a>&lt;/li>
&lt;li>&lt;a href="#competing-interests-commitments-and-values">Competing Interests, Commitments, and Values&lt;/a>&lt;/li>
&lt;li>&lt;a href="#the-researcher-in-society">The Researcher in Society&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="chapter-summaries">Chapter Summaries&lt;/h2>
&lt;blockquote>
&lt;p>Chapter specific summaries&lt;/p>
&lt;/blockquote>
&lt;h3 id="the-national-academies-advisors-to-the-nation-on-science-engineering-and-medicine">The National Academies Advisors to the Nation on Science, Engineering, and Medicine&lt;/h3>
&lt;p>The &lt;strong>National Academy of Science&lt;/strong> (NAS) (created in 1863) is a private,
non-profit, self-perpetuating society of distinguished scholars that are
required to advise the federal government on scientific and technical matters.&lt;/p>
&lt;p>The &lt;strong>National Academy of Engineers&lt;/strong> (NAE) (created in 1964) is a parallel
organization to NAS that also advises the federal government.&lt;/p>
&lt;p>The &lt;strong>Institute of Medicine&lt;/strong> (created in 1970) is a subsidiary of NAS aimed at
advising the government about public health policy matters.&lt;/p>
&lt;p>The &lt;strong>National Research Council&lt;/strong> was created in 1916 to broadly advise on
research matters. It is now the primary concern of both NAS and NAE.&lt;/p>
&lt;h3 id="preface">Preface&lt;/h3>
&lt;p>Science is built upon trust. When trust is misplaced and professional standards
are violated, researchers feel attacked and that the base of their profession
has been undermined.&lt;/p>
&lt;p>Many new researchers are not learning about how to conduct themselves as
scientists (a broad definition that includes anyone who is involved with the
pursuit of knowledge). This task used to be placed on more senior researchers.
However, as the speed of scientific developments, this task is often placed on
the back burner. Furthermore, exciting opportunities keep appearing faster than
science can keep up, further contributing to the problem of not educating new
researchers on best practices.&lt;/p>
&lt;p>Senior researchers have a special responsibility of upholding and promulgating
high standards in science. They should be role models, teachers, and mentors to
junior researchers.&lt;/p>
&lt;h3 id="a-note-on-using-on-being-a-scientist">A Note on Using &lt;em>On Being a Scientist&lt;/em>&lt;/h3>
&lt;p>This book explores options for handling situations, not defining what exactly to
do in a particular situation. This book should be read by a group of researchers
that discuss the topics within and figure out how to apply the lessons to their
practice.&lt;/p>
&lt;p>The material within this book is not exhaustive, and many texts and websites now
exist to help supplement this material.&lt;/p>
&lt;p>The existence of professional standards implies that there is a proper way to go
about a task or field. However, this book doesn&amp;rsquo;t aim to cover specifics. To
address specific concerns, a committee consisting of a diverse group of
individuals from varying experience levels should be formed to identify and
discuss what is the professional standards. Furthermore, if your professional
has an established code of conduct or professional standards, those should be
referred to for specifics.&lt;/p>
&lt;h3 id="introduction-to-the-responsible-conduct-of-research">Introduction to the Responsible Conduct of Research&lt;/h3>
&lt;p>Researchers pride themselves on observing or explaining phenomenon which haven&amp;rsquo;t
been seen or explained previously. However, researchers are often under a great
deal of personal and professional pressure. Failure to properly collect, report,
or handle data, experiments, or papers can result in lost time, money, and
respect.&lt;/p>
&lt;p>Researchers have thus created professional standards over many centuries for
many different fields. It is therefore expected that researchers will adhere to
these standards.&lt;/p>
&lt;p>Thus there is an obligation to honor the trust that their colleagues place in
them and vice versa. Irresponsible actions of others could potentially undermine
an entire field.&lt;/p>
&lt;p>There is also an obligation for researchers to conduct themselves in a
responsible manner or else face potentially permanent damage to their
reputation.&lt;/p>
&lt;p>Additionally, researchers have an obligation to the public. It is their results
that effect public policy which affects everyone. Additionally, their results
may have a butterfly effect that affects international peoples or policy.&lt;/p>
&lt;p>A &lt;em>scientific standard&lt;/em> is the application of the ethical values of honesty,
fairness, objectivity, openness, trustworthiness, and respect for others.&lt;/p>
&lt;p>&lt;em>Scientific misconduct&lt;/em> is defined as, &amp;ldquo;fabrication, falsification or plagiarism
in proposing, performing, or reviewing researcher, or in reporting research
results,&amp;rdquo; by the US government.&lt;/p>
&lt;p>Those that operate with &lt;em>scientific misconduct&lt;/em> are said to work with
&lt;em>questionable research practices&lt;/em>.&lt;/p>
&lt;p>The &lt;em>scientific standard&lt;/em> proliferates throughout the entirety of science, but
&lt;em>scientific practices&lt;/em> (application of standards) differs from field to field.&lt;/p>
&lt;h3 id="advising-and-mentoring">Advising and Mentoring&lt;/h3>
&lt;p>It is often best to build a diverse community of mentors as not one person has
the same experience, education, or background as another.&lt;/p>
&lt;p>Being a mentor is not a one way street where the mentee only benefits. As a
mentor, you experience the benefits of being exposed to new ideas, building a
strong research program and collaboration network, and gain new friends plus
respect from beginning researchers. However, mentors have influence over the
start of a junior researcher&amp;rsquo;s career, and must therefore be careful not to
abuse it. The main role of the mentor is to help the mentee move along a
productive and successful career trajectory.&lt;/p>
&lt;p>Mentees also have responsibilities towards their mentors. They need to develop
clear expectations regarding meeting times and availability with their mentors,
and need to seek out their mentors, rather than expect a mentor to be provided
to them. For more information on this subject, please read &lt;em>Adviser, Teacher,
Role Model, Friend: On Being a Mentor to Students in Science and Engineering&lt;/em> by
Philip A. Grififths [1].&lt;/p>
&lt;h3 id="treatment-of-data">Treatment of Data&lt;/h3>
&lt;p>Researchers who maliciously manipulate data to deceive others violate the basic
values and accepted professional standards of science. In doing so they mislead
colleagues, potentially impede progress in their field, undermine their own
authority and trustworthiness, and they introduce misinformation into both the
scientific record and the public sphere.&lt;/p>
&lt;p>To ensure the integrity of science, researchers have developed and continually
improve upon methods such as statistical tests of significance, double-blind
trials, and proper survey phrasing ensured by the academic&amp;rsquo;s relevant Internal
Review Board (IRB). Because it is paramount that the integrity of the work is
valid, papers must include a description of the methodology used to generate the
data. This is because if the methodology to generate the data is incorrect or
not present, then the integrity of the data and the work can be called into
question. Furthermore, this helps ensure the reproducibility of science.&lt;/p>
&lt;p>New researchers are not trained on how to generate, share, store, and publish
data. Additionally, new technologies to store, transmit, generate, and analyze
data are constantly being created. Thus researchers face a dilemma of how to
store data permanently so that it can be used in future works (either by
themselves or with others). However, when academic works are released the data
must be made available. If a researcher refuses to share the data, then they
fail to maintain the standards of science.&lt;/p>
&lt;h3 id="mistakes-and-negligence">Mistakes and Negligence&lt;/h3>
&lt;p>All science is susceptible to error. However, researchers have an obligation to
be as accurate and as careful as possible both for their profession and the
public.&lt;/p>
&lt;p>To make progress, researchers must believe the works of other researchers. It is
well known that by, &amp;ldquo;looking for an answer&amp;rdquo;, or believing that there exists an
answer prior to running experiments, that researchers bias themselves to believe
that something exists when it doesn&amp;rsquo;t. However, these inaccuracies and mistakes
must be remedied in the scientific record as soon as they are identified so as
to allow the continue proliferation of science.&lt;/p>
&lt;h3 id="research-misconduct">Research Misconduct&lt;/h3>
&lt;p>Scientific misconduct, as defined by the U.S. Office of Science and Technology
Policy, defines it as, &amp;ldquo;fabrication, falsification, or plagiarism in proposing,
performing, or reviewing research, or in reporting research results&amp;rdquo;.&lt;/p>
&lt;p>The specific definitions for fabrication, falsification, and plagiarism (FFP)
are listed below (and taken from the text):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>- Fabrication is &amp;#34;making up data or results.&amp;#34;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Falsification is &amp;#34;manipulating research materials, equipment, or processes, or
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>changing or omitting data or results such that the research is not accurately
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>represented in the research record.&amp;#34;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Plagiarism is &amp;#34;the appropriation of another person&amp;#39;s ideas, processes,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>results, or words without giving appropriate credit.&amp;#34;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Furthermore, misconduct actions significantly depart from accepted practices,
that they were intended, or knowingly, or recklessly, and have to be proven by a
preponderance of information. However, identifying the intent to deceive is
difficult to do, as honest mistakes could also result in the same generated
data. Thankfully, differences of opinion are not scientific misconduct.&lt;/p>
&lt;p>To reduce scientific misconduct, co-authors should validate each others work, as
well as have third parties review their results prior to publication.&lt;/p>
&lt;p>Misconduct can set back individuals, institutions, and research fields. It can
also draw the eye of the public and policy makers to make decisions that are
negative for the progression of science.&lt;/p>
&lt;h3 id="responding-to-suspected-violations-of-professional-standards">Responding to Suspected Violations of Professional Standards&lt;/h3>
&lt;p>Science typically self-regulates itself. It is the research community that sets
the standards and practices that researchers should follow. However, as it is
self-regulating, confrontation does occur where results are questioned.&lt;/p>
&lt;p>When raising these questions, retaliation can occur. However, this is prohibited
by the laws do exist to prevent this.&lt;/p>
&lt;p>All research institutions that receive federal funding must have methods to
investigate and report misconduct. Anyone who is aware of misconduct must follow
these procedures.&lt;/p>
&lt;p>All scientists and research institutions should discourage questionable research
practices (QRPs). Additionally, they need to take responsibility in determining
what QRPs warren reprimands from the institution. However, not all fields define
QRPs the same. The act of salami-publishing (deliberately dividing research
results across multiple papers) is viewed differently from field to field.&lt;/p>
&lt;p>To raise suspicions, ask neutral questions rather than be accusatory. Another
option is to talk to a friend or advisor who can keep the situation
confidential. While this won&amp;rsquo;t necessarily result in changes, this will at least
structure your thinking in what to do next. Another option is to talk to the
research integrity officer who must be appointed if the institution receives
federal funding. However, keep in mind your own motivations and biases prior to
raising questions so as to be courteous and kind.&lt;/p>
&lt;h3 id="human-participants-and-animal-subjects-in-research">Human Participants and Animal Subjects in Research&lt;/h3>
&lt;p>Any research done on humans or animals is subject to federal, state, local, and
institutional regulations.&lt;/p>
&lt;p>The U.S. federal regulations on experiments that involve humans is known as the
Common Rule. The Common Rule specifies exactly what falls under the domain of
human participation in experiments. Furthermore, any research institution
receiving federal grants must have an IRB. The job of the IRB is to ensure that
all research involving human participants follows the Common Rule. Many private
institutions also have IRBs.&lt;/p>
&lt;p>In some cases where human participation is required, formal training in
bioethics is needed to identify if the experiment is ethical towards the
participants. These issues are often evaluated by the committee&amp;rsquo;s such as the
President&amp;rsquo;s Council on Bioethics in the United States.&lt;/p>
&lt;p>Animals also have federal protections with regard to animal participation in
experiments. The U.S. federal Animal Welfare Act seeks, &amp;ldquo;to insure that animals
intended for use in research facilities &amp;hellip; are provided humane care and
treatment.&amp;rdquo; The U.S. Public Health Service&amp;rsquo;s &lt;em>Policy on the Human Care and Use
of Laboratory Animals&lt;/em> applies to all research supported by the National
Institute of Health (NIH). This requires institutions, &amp;ldquo;to establish and
maintain proper measures to ensure the appropriate care and use of all animals
involved in research, research training, and biological testing.&amp;rdquo; The policy
requires adherence to both the Animal Welfare Act and NIH&amp;rsquo;s continually updated
&lt;em>Guide for the Care and Use of Laboratory Animals&lt;/em>. This requires that all
animal testing follows the three R&amp;rsquo;s:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>- Reduction in the number of animals used,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Refinement of techniques and procedures to reduce pain and distress,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Replacement of conscious living higher animals with insentient material
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Overseeing the ethical experimentation on animals is an institution&amp;rsquo;s
Institutional Animal Care and Use Committees (IACUCs).&lt;/p>
&lt;h3 id="laboratory-safety-in-research">Laboratory Safety in Research&lt;/h3>
&lt;p>Researchers should review safety information and procedures at least once a
year.&lt;/p>
&lt;p>A shortlist of topics to review includes:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>- Appropriate usage of protective equipment and clothing,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Safe handling of materials in laboratories,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Safe operation of equipment,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Safe disposal of materials,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Safety management and accountability,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Hazard assessment processes,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Safe transportation of materials between laboratories,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Safe design of facilities,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Emergency responses,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Safety education of all personnel before entering the laboratory,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Applicable government regulations
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="sharing-of-research-results">Sharing of Research Results&lt;/h3>
&lt;p>In the 17th century, researchers kept their results secret in order to protect
their results from others claiming them. To solve this, Henry Oldenburg (the
secretary for the Royal Society of London) started to guarantee that not only
researchers could have their results rapidly published, but that the Royal
Society would defend authors integrity. He furthermore invented the modern
review practice of sending out manuscripts to experts prior to publication for
review.&lt;/p>
&lt;p>To publish in a peer-reviewed journal is often the most important way to share
information among researchers. This is because the first to publish often gets
the credit. Once results are published, others can cite the author in their
work. Unpublished manuscripts under review are to be treated as confidential.&lt;/p>
&lt;p>Citations need to be scrutinized during the peer-review process in order to
assess their validity and usefulness. Researchers have a duty to thoroughly
search the available literature on a subject and attribute all relevant authors.&lt;/p>
&lt;p>Information can be disseminated through seminars, conferences, abstracts, and
posters as well as peer-reviewed journal papers. Often considered to be
preliminary in nature. Spreading data, results, or manuscripts that haven&amp;rsquo;t been
peer-reviewed online is possible as well. However, by bypassing traditional
quality assurance measures, researchers take on a risk that their work won&amp;rsquo;t
serve science well.&lt;/p>
&lt;p>Publishing or announcing results prior to peer-review is dangerous. Without the
peer-review checks, it is possible for misinformation to be misinterpreted by
the public, policy makers, and other researchers as fact. Additionally, by
posting information prior to a journal submission online, it can be considered
to be a &amp;ldquo;prior publication&amp;rdquo;, thus making it unusable for publishing within the
journal.&lt;/p>
&lt;p>Publication abuses are possible. For example, researchers may publish the &amp;ldquo;least
publishable unit&amp;rdquo; of information. This is when the smallest number of results
are collected or written about to be published, rather than undergoing an
intensive research process. This wastes the time and energy of reviewers and can
give the researcher a reputation of producing shoddy or incomplete work.&lt;/p>
&lt;h3 id="authorship-and-the-allocation-of-credit">Authorship and the Allocation of Credit&lt;/h3>
&lt;p>When a paper is published, all of those that contributed to the paper are
considered authors and should be listed as such. Authorship conventions and
practices differ from field to field, and even lab to lab. Open and honest
conversations about how authorship is handled is paramount to ensure that
everyone knows the rules for the author list order.&lt;/p>
&lt;p>Honorary authors (those that contributed zero to little actual content to the
research) should not be added in the author list. They can be acknowledged in an
&amp;ldquo;Acknowledgment&amp;rdquo; section however. Additionally, ghost-writers should not be used
to write a paper.&lt;/p>
&lt;p>All authors on a paper must take responsibility for the content of the paper and
must be able to answer questions about the research.&lt;/p>
&lt;h3 id="intellectual-property">Intellectual Property&lt;/h3>
&lt;p>Researchers should be aware of the potential monetary value that their
intellectual property has. Intellectual property is a legal right to control the
application of an idea within a specific context (through a patent) or control
the expression of an idea (through a copyright).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>- Patent owners can protect [their] intellectual property rights by excluding
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>others from making, using, or selling an invention so long as the patent owner
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>provides a full description of how the invention is made, is used, and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>functions.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Copyrights protect the expression or presentation of ideas, but they do not
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>protect the ideas themselves.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Most research institutions have policies that detail how intellectual policy
rights should be handled. Additionally, patent and copyright laws differ from
country to country. Publication of work should not be delayed because of a
potential patent though.&lt;/p>
&lt;h3 id="competing-interests-commitments-and-values">Competing Interests, Commitments, and Values&lt;/h3>
&lt;p>Conflict of interest refers to situations where researchers have personal,
intellectual, financial, and professional interests that conflict with the
ongoing research. Conflicts of financial interests undergo much scrutiny. Should
research be done with a known conflict of financial interest, it severely
damages the researcher&amp;rsquo;s credibility and integrity.&lt;/p>
&lt;p>Personal relationships can also conflict with research. Often journals and
funding agencies ask for personal relationships in order to identify potential
conflict of personal interests. Romantic relationships often count as a personal
conflict of interest.&lt;/p>
&lt;p>Timely and full disclosure of conflict of interests must be made to governing
bodies of science when requested.&lt;/p>
&lt;p>Conflicts of commitment are different than conflict of interests. This can cause
significant strain on a researcher&amp;rsquo;s life and should be identified early.&lt;/p>
&lt;p>The values that a researcher has can cause a conflict of interest as well.
Religious, philosophical, cultural, or political beliefs can all cause conflicts
of interests. Values cannot and should not be separated from science, but should
be acknowledged and understood while conducting science.&lt;/p>
&lt;h3 id="the-researcher-in-society">The Researcher in Society&lt;/h3>
&lt;p>Researchers have the duty to ensure that not only does their work further
science, but also improves the public&amp;rsquo;s welfare. They are put into an
interesting situation within the public sphere and may have to wear many hats.
However, it is paramount that they remain honest, fair, collegial, and open in
society as these are all core scientific tenants that researchers should follow.&lt;/p>
&lt;hr>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/on-being-a-scientist/</description></item><item><title>A summary of ORI Introduction to the Responsible Conduct of Research by Nicholas H. Steneck</title><link>https://nsynovic.dev/summaries/ori-introduction-to-the-responsible-conduct-of-research/</link><pubDate>Fri, 13 Jan 2023 19:43:16 -0600</pubDate><guid>https://nsynovic.dev/summaries/ori-introduction-to-the-responsible-conduct-of-research/</guid><description>&lt;h1 id="a-summary-of-ori-introduction-to-the-responsible-conduct-of-research">A summary of &lt;em>ORI Introduction to the Responsible Conduct of Research&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Written by Nicholas H. Steneck&lt;/p>
&lt;/blockquote>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-ori-introduction-to-the-responsible-conduct-of-research">A summary of &lt;em>ORI Introduction to the Responsible Conduct of Research&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#chapter-summaries">Chapter Summaries&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#foreward">Foreward&lt;/a>&lt;/li>
&lt;li>&lt;a href="#preface">Preface&lt;/a>&lt;/li>
&lt;li>&lt;a href="#part-i">Part I&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#chapter-1-rules-of-the-road">Chapter 1: Rules of the Road&lt;/a>&lt;/li>
&lt;li>&lt;a href="#chapter-2-research-misconduct">Chapter 2: Research Misconduct&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#part-ii">Part II&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#chapter-3-the-protection-of-human-subjects">Chapter 3: The Protection of Human Subjects&lt;/a>&lt;/li>
&lt;li>&lt;a href="#chapter-4-the-welfare-of-laboratory-animals">Chapter 4: The Welfare of Laboratory Animals&lt;/a>&lt;/li>
&lt;li>&lt;a href="#chapter-5-conflicts-of-interest">Chapter 5: Conflicts of Interest&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#part-iii">Part III&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#chapter-6-data-management-practices">Chapter 6: Data Management Practices&lt;/a>&lt;/li>
&lt;li>&lt;a href="#chapter-7-mentor-and-trainee-responsibilities">Chapter 7: Mentor and Trainee Responsibilities&lt;/a>&lt;/li>
&lt;li>&lt;a href="#chapter-8-collaborative-research">Chapter 8: Collaborative Research&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#part-iv">Part IV&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#chapter-9-authorship-and-publication">Chapter 9: Authorship and Publication&lt;/a>&lt;/li>
&lt;li>&lt;a href="#chapter-10-peer-review">Chapter 10: Peer Review&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#part-v">Part V&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="chapter-summaries">Chapter Summaries&lt;/h2>
&lt;blockquote>
&lt;p>Chapter specific summaries&lt;/p>
&lt;/blockquote>
&lt;h3 id="foreward">Foreward&lt;/h3>
&lt;p>This book is published by the Office of Research Integrity (ORI) as supplemental
material to assist in the teaching of the responsible conduct of research. The
ORI oversees and directs the Public Health Service (PHS) research integrety on
behalf of the Secretary of Health and Human Services as well as the American
public. This book stems from the ethics that were raised in biology related
fields in 1989 and has sinced expanded to become more general and applicable to
other disciplines.&lt;/p>
&lt;h3 id="preface">Preface&lt;/h3>
&lt;p>As the importance of science and technology increases, so too has the public&amp;rsquo;s
demand for the responsible conduct of research. Simply put, this means that good
citizenship is applied to a researcher&amp;rsquo;s professional life. However, as each
field is different, so too does what &amp;ldquo;good citizenship&amp;rdquo; mean within each field.
This book was written to briefly introduce researchers to what is generally
accepted as responsible conduct regardless of their working field.&lt;/p>
&lt;hr>
&lt;h3 id="part-i">Part I&lt;/h3>
&lt;h4 id="chapter-1-rules-of-the-road">Chapter 1: Rules of the Road&lt;/h4>
&lt;p>There aren&amp;rsquo;t clear rules laid out to for researcher&amp;rsquo;s to easily understand what
is and isn&amp;rsquo;t responsible research conduct. Rather, there are many organizing
bodies, both official and non-official, that govern what is and isn&amp;rsquo;t
responsible. Furthermore, a researcher&amp;rsquo;s personal judgement must also solve this
ethical calculus. In order of importance of whose rules to listen too, it goes:
Professional Codes, Federal, State, institutional policies, and personal
judgement. Not all professions have Professional Codes. Federal regulations have
to first be made available for public comment in the &lt;em>Federal Register&lt;/em>, and
once accepted, are incorporated into the &lt;em>Code of Federal Regulations&lt;/em> and must
be followed. Researchers are advised to seek help in understanding Federal
rules.&lt;/p>
&lt;h4 id="chapter-2-research-misconduct">Chapter 2: Research Misconduct&lt;/h4>
&lt;p>Research misconduct policies provide guidance on responsible conduct in these
three areas:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>1. Establish definitions for misconduct in research,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>2. Outline procedures for reporting and investigating misconduct, and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>3. Provide protection for whistleblowers and person accused of misconduct.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Federal regulations for research misconduct technically only apply at the
Federal level, but many institutions incorporate and expand it within their own
policies. The Office of Science and Technology Policy&amp;rsquo;s (OSTP), a Federal
agenct, definition of research misconduct is, &amp;ldquo;fabrication, falsification, or
plagiarism in proposing, performing, or reviewing research, or in reporting
research results&amp;rdquo;. For an action to be considered research misconduct, it must
meet the following criteria:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>1. Represent a, &amp;#34;significant departure from accepted practices&amp;#34;,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>2. Have been, &amp;#34;committed intentionally, or knowingly, or recklessly&amp;#34;; and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>3. Be, &amp;#34;proven by a prponderance of evidence&amp;#34;.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The federal policy puts the responsibility of research misconduct reporting and
investigation on both &lt;em>researchers&lt;/em> and &lt;em>their research institutions&lt;/em>. Research
misconduct puts a researcher&amp;rsquo;s career on the line. Additionally, a whistleblower
runs the risk of illegal retaliation. Furthermore, the accused researcher(s)
only face punishment when proven guilty.&lt;/p>
&lt;p>The violation of Federal rules, abuse of confidentiality, authorship and
publication violations, failure to report misconduct, obstruction of misconduct
investigations, and other institutional level policies can be classified as
research misconduct. It is assumed that research misconduct is under reported.
Regardless, this misconduct taints the public image of research, wastes donated
or tax-funded dollars, and harms both the professional reputations of
researchers and institutions.&lt;/p>
&lt;hr>
&lt;h3 id="part-ii">Part II&lt;/h3>
&lt;h4 id="chapter-3-the-protection-of-human-subjects">Chapter 3: The Protection of Human Subjects&lt;/h4>
&lt;p>Human subjects are used in order to find improvements to the human experience.
This could involve the creation of new medicines, surgeries, or experiments.&lt;/p>
&lt;p>Researchers who use human subjects are subject to Federal regulations. Both the
Nuremberg Code and the Declaration of Helsinki are guidelines that are to be
followed when testing on human subjects. Additionally, the Federal regulation 45
CFR 46 Subparts A - D, known as the &amp;ldquo;Common Rule&amp;rdquo;, are expected to be followed
as well. Exempt research from the &amp;ldquo;Common Rule&amp;rdquo; include:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>- Research conducted in established or commonly accepted educational settings,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Research involving the use of educational tests,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Research involving the collection or study of existing data, documents,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>records, pathological specimens, or diagnostic specimens, if unidentifiable or
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>publicly availible,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Research and demonstration projects which are conducted by or subject to the
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>approval of department or agency heads; or
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Taste and food quality evaluation and consumer acceptance studies.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Any institution that performs human research must have a Federally mandated
Internal Review Board (IRB) consisting of at least 5 people, one of which is a
scientist, one of which is a non-scientist, and one who is not affiliated with
the institution. Furthermore, all researchers working with human subjects must
undergo National Institutes of Health (NIH) training.&lt;/p>
&lt;p>All human subjects must have to informed consent and the right to withdraw.
Furthermore, there is an ethical concern in human subjects research that some
experiments have risks without any benefits.&lt;/p>
&lt;h4 id="chapter-4-the-welfare-of-laboratory-animals">Chapter 4: The Welfare of Laboratory Animals&lt;/h4>
&lt;p>Research on animal subjects is done to benefit humans without experimenting on
humans. The 1966 Animal Welfare Act and the 1985 Health and Research Extension
Act both act as Federal regulations that researcher&amp;rsquo;s experimenting on human
subjects must abide by. The United States Department of Agriculture (USDA) is
responsible for the transportation, care, and use of animals. The usage of
animals for biomedical and behavioral research is subject to the Secretary of
Health and Human Services (HHS), acting through the National Institutes of
Health (NIH) and the Office of Laboratory Animal Welfare (OLAW), NIH.
Furthermore, activists have put together guidelines through the Animal Care
Panel (ACP) commonly reffered to as th Guide. The &lt;em>Guide&lt;/em> is widely accepted
both in public and private institutions as well as Federal laboratories.&lt;/p>
&lt;p>The definition of an animal from the PHS Policy is, &amp;ldquo;any live, vertebrate
animals used or inteded for use in research, research training, experimentation,
or biological testing or for related purposes&amp;rdquo;. the definition of an animal from
the Common Rule that implements the Animal Welfare Act includes warm blooded
animals (mammals) but excludes, &amp;ldquo;Birds, rats of the genus Rattus and mice of the
genus Mus bred for use in research, and horses not used for research purposes
and other farm animals&amp;rdquo;.&lt;/p>
&lt;p>Institutions testing on animals must create an Institutional Animal Care and Use
Committee (IACUC) which acts similarly to an IRB but for animals. These, along
with the OLAW, USDA, and accrediation programs, are responsible for the proper
care and respect of animal subjects. Prior to using animal subjects, researchers
must defend the usage of animal subjects against the &amp;ldquo;three R&amp;rsquo;s&amp;rdquo;:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>1. Replacement - using non-animal models such asmicroorganisms or cell culture
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>techniques, computer simulations, or species lower on the phulogenetic scale,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>2. Reduction - using methods aimed at reducing the numbers of animals such as
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>minimization of variability, appropriate selection of animal model, minimization
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>of animal loss, and careful experimental design, and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>3. Refinement - the elimination or reduction of unnecessary pain and distress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[in animal subjects].
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="chapter-5-conflicts-of-interest">Chapter 5: Conflicts of Interest&lt;/h4>
&lt;p>Researchers can experience three different types of conflicts of interest:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>1. Financial gain,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>2. Work commitments, and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>3. Intellectual and personal matters.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Financial gains involve reciving money from an institute that the researcher has
a vested in. This could involve having stock in the company, or being a member
of the company.&lt;/p>
&lt;p>Work commitments involve colluding multiple working positions into one in order
to serve a singular purpose. In other words, if a position at work is being used
to further a research objective or vice versa, it is a conflict of interest.&lt;/p>
&lt;p>Intellectual and personal matter conflicts of interest occur when the idealogy,
beliefs, or viewpoint of a researcher could interfere with their research
objectives. As an example (not mentioned in this book; purely my own thought
probably taken from someone else) is a pro-life reseacher working on
abortificants.&lt;/p>
&lt;hr>
&lt;h3 id="part-iii">Part III&lt;/h3>
&lt;h4 id="chapter-6-data-management-practices">Chapter 6: Data Management Practices&lt;/h4>
&lt;p>Research is not possible without data. Because of this, there are
responsibilities that researchers have to the proper ownership, collection,
storage, and sharing of data.&lt;/p>
&lt;p>Data can be owned by one or many sources. This includes the funders of the
research, the institution where the research was conducted, and the original
source of the data. Typically, government funded research allows the data to be
shared for the public good, privately funded research retain the right to
disseminate the data, and philinthropic funding sources retain or give away
their data ownership rights depending on their motive.&lt;/p>
&lt;p>Data must be collected responsibly. This means that data collection method is
appropriate and reliable. If the method is garbage, the result will be garbage
(garbage in, garbage out). The data collected must be detailed enough to support
the results. It must also be authorized for collection, and when recorded,
proper metadata must be captured to include the date, time, source, and other
important facts about the source of the data.&lt;/p>
&lt;p>After collecting the data, data must be stored properly so as to prevent the
theivery or loss of data. This includes securing lab notebooks and regularly
backing up data files. Additionally, any data that includes personal identifying
information or is otherwise confidential, must go through a process of
anonymizing the data so as to respect the source&amp;rsquo;s privacy. Furthermore, some
data may be subject to a period of retention and if not, data should otherwise
be retained indefinetly.&lt;/p>
&lt;p>In the case of sharing data with other researchers or the general public, data
must be fully validated, not be preliminary data, and can be confidential until
publication.&lt;/p>
&lt;p>Finally, future considerations about the data must be taken into account. For
example, the generated data of an experiment can be larger than the amount of
usable computer storage. Furthermore, there is now a concern that released data
can contribute to the terrorism or foreign entities that wish to do harm to the
American public. Ethical concerns about the release of such data must be taken
into consideration prior to releasing said data.&lt;/p>
&lt;h4 id="chapter-7-mentor-and-trainee-responsibilities">Chapter 7: Mentor and Trainee Responsibilities&lt;/h4>
&lt;p>Trainee refers to anyone learning to be a researcher learning from a senior
researcher. To establish the basic understandings of the relationship, trainees
must know from their mentors:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>- How much time they are expected to put into research,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- How performance is judged,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- How responsibilities are shared or divided,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Standard operating procedures, and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- How credit and authorship are assigned.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Whereas mentors expect from their trainees that they:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>- Do work in a conscientious way,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Respect the authority of others,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Follow research regulations and protocols, and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Live by agreements established for authorship and ownership
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To make sure these expectations are clear, the mentor should lay them out
initally. Furthermore, mentors have are expected to supervise the work of a
trainee. This results in the monitoring, support, and review of a trainees work,
as well as imparting the ethos of being a researcher onto them.&lt;/p>
&lt;p>The research environment that they mentor creates and the trainee learns in
should be one of:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>- Equal treatment free from discrimination,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Professional, and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Follow the responsible conduct of research
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Postdoctoral researchers have a rough time in the lab as they have the skills to
be an independent researcher, but since they are no longer students nor faculty,
they have little rights. Additionally, since they are working under someone
else&amp;rsquo;s funding, they have little freedom to pursue their own research. The
National Postdoctoral Association has been created to ensure that postdocs have
the rights that they deserve with respect to transitioning to independent
researchers.&lt;/p>
&lt;h4 id="chapter-8-collaborative-research">Chapter 8: Collaborative Research&lt;/h4>
&lt;p>Collaborative research involves any time when two or more researchers
collaborate on the same project. This could involve researchers from the same
institution or different insititutions. This work is led by a primary
investigator (PI) who heads the project and is responsible for the compliance of
rules and responsible research conduct, financials, and training.&lt;/p>
&lt;p>Collaborative research projects need to initially solve many problems with
respect to:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>- The goals of the project and anticipated outcomes,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Everyone&amp;#39;s role,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- How data will be collected, stored, and shared,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- How changes in the research design will be made,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Who is responsible for drafting publications,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- The criteria to identify and rank authorship,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Who is responsible for submitting reports and meeting other requirements,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Who is responsible for publicly speaking about the work,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- How intellectural property rights and ownership issues will be resolved, and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- How collaboration can be changed and when it comes ot an end.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With respect to compliance, it is best practice to work under the compliance
guidelines of the institution or state that has the strictest policies to ensure
that all researchers are following proper protocol.&lt;/p>
&lt;hr>
&lt;h3 id="part-iv">Part IV&lt;/h3>
&lt;h4 id="chapter-9-authorship-and-publication">Chapter 9: Authorship and Publication&lt;/h4>
&lt;p>All forms of publication should present:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>- A full and fair description of the work done,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- An accurate report of the results,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- An honest and open assessment of the findings,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- The methods of the researcher,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- What the results are, and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- A discussion of the results to better understand their impact.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Authorship serves to let others know who conducted the research and should get
credit for it. It is generally limited to those who were:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>- Intimately involved in the conception and design of the research,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Assumed responsibility for data collection and interpretation,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Participated in the drafting of the publication, and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Approved the final version of the publication.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>There is disagreement as to whether a researcher needs to adhere to all of these
rules in order to be consider an author. The International Committee of Medical
Journal Editors (ICMJE) have created the &lt;em>Uniform Requirements for Manuscripts
Submitted to Biomedical Journals&lt;/em> which lists out the following criteria for
authorship:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>1. Substantial contributions to conception and design, or acquisition of data,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>or analysis and interpretation of data,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>2. Drafting the article or revising it critically for important intellectual
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>content, and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>3. Final approval of the version to be published
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Authors should meet conditions 1, 2, and 3.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The ICMJE guidelines are strict and are not followed by all disciplines.&lt;/p>
&lt;p>The first and last author on a paper carry weight. What this weight means is
dependent upon the field that it is published in.&lt;/p>
&lt;p>The corresponding or primary author of the paper is the individual who can
intimately describe the work and is the public relations individual of the work.&lt;/p>
&lt;p>A manuscript should include:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>- An abstract to summarize the content of the paper,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- A methods section to describe the research methods to create the results,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- A results section to talk about what was found,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- A discussion section to describe what the results mean, and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- A notes, bibliography, and acknowledgments section to acknowledge the work of
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>previous researchers or minor authors to the paper
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The following authorship practices should be avoided:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>- Honorary authorship,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Salami publications (using the same data/ study in two seperate papers),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Duplicate publications, and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Premature public statments
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="chapter-10-peer-review">Chapter 10: Peer Review&lt;/h4>
&lt;p>Peer reviewed is done by fellow researchers (typically within the same field) to
asses:&lt;/p>
&lt;ul>
&lt;li>Grant reviews,&lt;/li>
&lt;li>Manuscript reviews,&lt;/li>
&lt;li>Personnel reviews, and&lt;/li>
&lt;li>Literature reviews and expert testimony&lt;/li>
&lt;/ul>
&lt;p>Peer reviews must be:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>- Timely,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Thorough,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Constructive,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Free from personal bias, and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Respectful of the need for confidentiality
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Peer reviews assess the quality of the work by:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>- Assessing whether the research methods are appropriate,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Checking calculations and/or confirming the logic of important arguments,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Making sure the conclusions are supported by the evidence presented, and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Confirming that the relevant literature has bee consulted and cited
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Research quality can be comprimised if the peer reviewer(s) find:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>- Careless mistakes made in reporting data and/or listing citations,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- The deliberate favrication and falsification of data,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Plagarism
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Inaccurate reporting of conflicts of interest, contributors/ authors, and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- The failure to mention important prior work, either by others of by the
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>researcher submitting a paper for publication.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Peer reviews judge the importance of the work by answering the following
questions:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>- Assuming a researcher could carry out a proposed research project, is it
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>important to do so?
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Are these research results important enough to publish?
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Has a researcher made important contributions to a field of study?
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Is this evidence important enough to be used in setting policy?
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>By juding importance, a reviewers personal bias is involved. To mitigate this,
reviewers should write transparent reviews in which they lay out their bias as
well as any documents or citations that they found that supports their bias.&lt;/p>
&lt;p>Confidentiality is important in protecting peers from attacks from one another.
There exists a double-blind review in which both the reviewers and the authors
are anonymous from one another. It is not appropriate to:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>- Ask students or anyone else to conduct a review you were asked to do,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Use an idea or information contained in a grant proposal or unpublished
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>manuscript before it becomes publicly available,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Discuss grant proposals or manuscripts you are reviewing with colleagues in
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>your department or at a professional meeting,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Retain a copy of the reviewed mateial, and
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Discuss personnel and hiring decisions with colleagues who are not part of the
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>review process
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;h3 id="part-v">Part V&lt;/h3>
&lt;p>It is well understood that many people early in their career have irresponsible
research conduct. Additionally, what might be considered responsible conduct in
one field may not apply to all fields. None the less, there is no justification
for comprimising the core tennants of research: honesty, accuracy, efficiency,
and objectivity. Upholding these is the responsibility of all researchers.&lt;/p>
Citations availible at https://nsynovic.dev/summaries/ori-introduction-to-the-responsible-conduct-of-research/</description></item><item><title>A summary of Joint Head Pose Estimation and Face Alignment Framework Using Global and Local CNN Features by Xiang Xu and A. Kakadiaris</title><link>https://nsynovic.dev/summaries/joint-head-pose-estimation-and-face-alignment-framework-using-global-and-local-cnn-features/</link><pubDate>Sun, 04 Dec 2022 22:32:45 -0600</pubDate><guid>https://nsynovic.dev/summaries/joint-head-pose-estimation-and-face-alignment-framework-using-global-and-local-cnn-features/</guid><description>&lt;h1 id="a-summary-of-joint-head-pose-estimation-and-face-alignment-framework-using-global-and-local-cnn-features">A summary of &lt;em>Joint Head Pose Estimation and Face Alignment Framework Using Global and Local CNN Features&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Xiang Xu and A. Kakadiaris IEEE FG, 2017
&lt;a href="https://doi.org/10.1109/FG.2017.81">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-joint-head-pose-estimation-and-face-alignment-framework-using-global-and-local-cnn-features">A summary of &lt;em>Joint Head Pose Estimation and Face Alignment Framework Using Global and Local CNN Features&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>The problem is two fold:&lt;/p>
&lt;ol>
&lt;li>To build a face detector that is robust to occlusion and &amp;ldquo;in the wild&amp;rdquo;
images.&lt;/li>
&lt;li>To incorporate facial landmarks via global and local features of the image to
better isolate not only where the face(s) are within the image, but also the
eyes, jaw, mouth, etc.&lt;/li>
&lt;/ol>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Because the solution proposed allows for the identification and isolation of
facial features of an input image.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is a face detection paper as well as CV model architecture paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is related to works involving face and facial landmark detection.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their contributions are a methodology for leveraging the relationship between
head pose and face landmarks to find the best shape for initialization and the
usage of global and local CNN features for pose estimation and face alignment in
a cascading manner.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done to detect faces and to identify the facial landmarks of a
detected face.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>Nearly all of the figures, charts, and tables are clear. Figure 4 is lacking
description for which set of images correspond to what model.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is well written.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors compared their solution (JFA) against other SOTA models including
ERT on pose and facial landmark detection datasets.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>That global features can capture fine grain detail of an image.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>Not really, as global features are intended to look at the feature map as a
whole to identify very large or prominent features (such as an entire face). To
the authors credit, they do discuss this at the end of the paper. But as it
stands, the usage of global features to identify fine grain detail (such as
facial landmarks) is a waste of computational resources that only attempts to
mimic the local feature analysis that their algorithm also performs.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d like to see if I could optimize their solution by removing the global
feature analysis for facial landmarks component. The idea behind optimizing
their solution is to implement it on a mobile or low powered system.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>As the solution proposed involves a database lookup to identify areas of
interest for facial landmarks, how robust is that database to rotation, scale,
lighting, and occlusion variance?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>This is an interesting solution to the problem of identifying facial landmarks,
however&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Joint Head Pose Estimation and Face Alignment Framework Using Global
and Local CNN Features&lt;/em> by Xiang Xu and A. Kakadiaris [1] describes a solution
to not only detect faces, but identify facial landmarks using a cascading CNN
architecture. The reference model for the architecture was able to attain SOTA
results using an off-the-shelf face detector (Dlib) and could accurately
identify facial landmarks. It is able top detect facial landmarks by analyzing
both the global and local features generated from the face detectors feature
maps. A global feature detector is used for pose estimation. This is then passed
into a database of poses used for estimating where facial features are most
likely to be in the input image. A local feature detector than analyzes the
feature map to identify all of the facial landmarks within the image.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/joint-head-pose-estimation-and-face-alignment-framework-using-global-and-local-cnn-features/</description></item><item><title>A summary of A Convolutional Neural Network Cascade for Face Detection by Haoxiang Li et al.</title><link>https://nsynovic.dev/summaries/a-convolutional-neural-network-cascade-for-face-detection/</link><pubDate>Sun, 04 Dec 2022 21:43:17 -0600</pubDate><guid>https://nsynovic.dev/summaries/a-convolutional-neural-network-cascade-for-face-detection/</guid><description>&lt;h1 id="a-summary-of-a-convolutional-neural-network-cascade-for-face-detection">A summary of &lt;em>A Convolutional Neural Network Cascade for Face Detection&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Haoxiang Li et al. CVPR, 2015 &lt;a href="https://doi.org/10.1109/CVPR.2015.7299170">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-a-convolutional-neural-network-cascade-for-face-detection">A summary of &lt;em>A Convolutional Neural Network Cascade for Face Detection&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>The problem addressed in this paper is that previous works relied on Haar
features which work for when faces are looking at the camera head on. However,
these features are not enough to capture faces at angles and are hand crafted.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>This paper proposes a model architecture for face detection that relies on a
cascade of CNNs to detect not only if there is a face, but also where a face is
within an image.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is a face detection and model architecture paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is most closely related to work that involves face detection.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors contributions are a CNN cascade for fast face detection, a bounding
box calibration method for fine tuning the localization of the model, a
multi-resolution CNN architecture for face detection, and a SOTA model
implementation of the architecture for face detection.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done previously to use neural networks for face detection.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the figures, charts, and tables are clear and easy to read.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is well written.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors measured the recall of their model and others on the Annotated Face
(Landmarks) in the Wild datasets.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The major assumption that the authors made was that a cascade of CNNs could
learn face detection better than an end to end model.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>Not really. As more and more modern literature is released about computer
vision, the trend is to train a model end to end on a particular task, rather
than use a cascade of models.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I think a reimplementation would be cool to do. However, I&amp;rsquo;d like to compare
this model to end to end solutions as well.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>Why were VGA sized images (800 x 600 pixels) used instead of transforming the
images into smaller inputs such as 224 x 224?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>This is a good paper. However, I would encourage the authors to explore
implementing this model on mobile or low powered systems if it is that efficient
to run on a CPU or GPU.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>A Convolutional Neural Network Cascade for Face Detection&lt;/em> by
Haoxiang Li et al. [1] presents a CNN based solution for face detection. Their
solutions aims to be robust against faces that are not looking directly at the
camera. Their model architecture involves a cascade of 6 CNNs that aim to first
identify if a first is in the image using dense features (first 3 CNNs), then
isolate and locate faces within the scene (last 3 CNNs). The general
architecture of the solution is: 12 layers, 12 layers + calibration, 24 layers,
24 layers + calibration, 48 layers, 48 layers + calibration. This architecture
was able to achieve SOTA performance on the Annotated Face (Landmarks) in the
Wild datasets.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/a-convolutional-neural-network-cascade-for-face-detection/</description></item><item><title>A summary of Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers by Sixiao Zheng et al.</title><link>https://nsynovic.dev/summaries/rethinking-semantic-segmentation-from-a-sequence-to-sequence-perspective-with-transformers/</link><pubDate>Sun, 04 Dec 2022 14:12:14 -0600</pubDate><guid>https://nsynovic.dev/summaries/rethinking-semantic-segmentation-from-a-sequence-to-sequence-perspective-with-transformers/</guid><description>&lt;h1 id="a-summary-of-rethinking-semantic-segmentation-from-a-sequence-to-sequence-perspective-with-transformers">A summary of &lt;em>Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Sixiao Zheng et al. CVPR, 2021
&lt;a href="https://doi.org/10.1109/CVPR46437.2021.00681">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-rethinking-semantic-segmentation-from-a-sequence-to-sequence-perspective-with-transformers">A summary of &lt;em>Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>This paper aims to develop a model architecture based off of the usage of the
Transformer architecture for the purposes of semantic segmentation.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Because the reference model implemented in this paper achieved SOTA results on a
variety of datasets as well as achieving first place on the ADE20K test server
leader board the day that it was released.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is a semantic segmentation model architecture paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is most closely related to works within the CV domain that use the
Transformer architecture as well as papers that develop SOTA architectures for
semantic segmentation.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors main contribution are a CV model architecture based on Transformers
that achieves SOTA performance on several datasets.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done to develop semantic segmentation models that rely on FCN
style architectures, as well as on CV models that rely on the Transformer
architecture to perform image classification.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The figures and tables are clear and easy to understand.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>This is a well written paper.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ol start="2">
&lt;li>Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan
Salakhutdinov. Transformer-XL: Attentive language models beyond a
fixed-length context. In ACL, 2019.&lt;/li>
&lt;li>Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan
Salakhutdinov. Transformer-XL: Attentive language models beyond a
fixed-length context. In ACL, 2019.&lt;/li>
&lt;/ol>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>They evaluated their model against other SOTA models on the Cityscapes, ADE20K,
and Pascal Context datasets.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>Their assumption is that the FCN architecture is not the best performing
architecture for semantic segmentation tasks.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>No. While their model (SETR) does achieve SOTA performance on semantic
segmentation tasks using the Transformer architecture, this alone does not mean
that the FCN architecture is dead. Rather, this paper just shows that
Transformers can be used for this task.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>Before working with this paper, I need to better understand what a Transformer
is and how to use one.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>It was mentioned that Transformers have a quadratic complexity. This, to me,
sounds like they are unusable on low powered devices. Can you compress or
quantize these types of models?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>I liked this paper. I thought it was interesting and exciting to hear that there
is a potential for a paradigm shift. If you are going to report that you were
first on the leader board for some competition, it would also be good to know
how long your model held onto that spot.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Rethinking Semantic Segmentation from a Sequence-to-Sequence
Perspective with Transformers&lt;/em> by Sixiao Zheng et al. presents the idea of using
the Transformer architecture for the purposes of semantic segmentation. In order
to do this, the authors describe in detail how to transform an image into a
suitable format for a transformer to understand. Additionally, they evaluate
their model (SETR) on three different semantic segmentation datasets and
achieved SOTA scores on all of them. However, SETR doesn&amp;rsquo;t achieve SOTA overall.
SETR was first on the ADE20K test server leader board the day that it was
released though.&lt;/p>
&lt;p>This paper challenges the usage of FCN&amp;rsquo;s as the primary architecture for
semantic segmentation.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/rethinking-semantic-segmentation-from-a-sequence-to-sequence-perspective-with-transformers/</description></item><item><title>A summary of by SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation by Vijay Badrinarayanan et al.</title><link>https://nsynovic.dev/summaries/segnet-a-deep-convolutional-encoder-decoder-architecture-for-image-segmentation/</link><pubDate>Sun, 04 Dec 2022 13:06:29 -0600</pubDate><guid>https://nsynovic.dev/summaries/segnet-a-deep-convolutional-encoder-decoder-architecture-for-image-segmentation/</guid><description>&lt;h1 id="a-summary-of-segnet-a-deep-convolutional-encoder-decoder-architecture-for-image-segmentation">A summary of &lt;em>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Vijay Badrinarayanan et al. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE
INTELLIGENCE, 2017 &lt;a href="https://doi.org/10.1109/TPAMI.2016.2644615">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-segnet-a-deep-convolutional-encoder-decoder-architecture-for-image-segmentation">A summary of &lt;em>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>A model architecture for image segmentation of indoor and outdoor scenes that is
memory and computationally efficient does not exist.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Because it presents a model architecture and reference model that is memory and
computationally efficient for the purposes of image segmentation.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is a computer vision neural network architecture paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is most closely related to works involving image segmentation
solutions that rely on an encoder-decoder style network.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The author&amp;rsquo;s main contributions are a model architecture that is memory and
computationally efficient for the purposes of image segmentation, a reference
model of this architecture called SegNet, and an advancement of the
encoder-decoder style network architecture.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done to develop image segmentation models using encoder-decoder
networks.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The tables are clear for the most part. There are some where trying to
understand which model was the top performer is unclear. The segmentation
figures are not clear to me. It is difficult to tell the qualitative difference
between the output of different models.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is dense. I&amp;rsquo;d say that about half of this paper is well written, but
technical sections of the paper mentions acronyms or domain specific knowledge
that are not defined.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ol start="2">
&lt;li>M. Ranzato, F. J. Huang, Y. Boureau, and Y. LeCun, Unsupervised learning of
invariant feature hierarchies with applications to object recognition, in
Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2007&lt;/li>
&lt;/ol>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors evaluated the performance of their architecture in both a simplified
and fully instanced bench marking.&lt;/p>
&lt;p>In the simplified bench marking, they evaluated their SegNet-basic model (4
encoders, 4 decoders) against FCN-basic which mirrored their encoder design.
They bench marked the two against each other on the Pascal VOC 2012 challenge.
The metrics captured where the global and class average, mean intersection over
union, and semantic contour measure.&lt;/p>
&lt;p>In the fully instanced bench marking, they evaluated SegNet against other models
on road and indoor scene segmentation. They compared SegNet against, FCN, FCN
(learned deconvolution), DeepLab-LargeFOV, DeepLab-LargeFOV-denseCRF, and
DeconvNet. They used the mean intersection over union and semantic contour
measure metrics to compare the models.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors assumed that less attention is being placed on smaller, more
computationally and memory efficient models for the purposes of semantic
segmentation. Additionally, that engineers want models of this type when
selecting from a pool of models.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>To me, yes. The smaller the model, or the more efficient it is, allows for the
engineers to evaluate models not just by performance, but by power consumption
or memory constrains which might be constrained.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d like to try and build SegNet and quantize it from float-16 to int-8 just to
see what would happen.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>Why wasn&amp;rsquo;t this model quantized? If memory efficiency was a concern, or
increasing the computation speed was desired, why utilize floating point values
everywhere and not just where it mattered most? Where are the comparisons of
memory usage and inference timing of different models compared to SegNet?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is dense. There is a lot of information covered here. Because of
that, I would encourage a review of the paper to address spelling mistakes when
referencing different models, as well as enhancing figures by pointing out
specific areas where SegNet outperforms or is outperformed by other models.
Also, please report comparisons of memory usage and inference timings if you are
claiming that the SegNet architecture is more efficient that other
architectures.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image
Segmentation&lt;/em> by Vijay Badrinarayanan et al. [1] introduces a novel model
architecture for semantic segmentation based off of an encoder-decoder design.
This architecture is designed around being computationally and memory efficient,
while accurate enough to be used in practice. They created a reference model
that implemented this model called SegNet that they then evaluated against
larger and SOTA segmentation models on road and indoor scene segmentation. While
SegNet is not always more accurate than the SOTA, the architecture it implements
significantly reduces memory usage and therefore increases inference time. No
metrics with respect to memory or inference time was reported in this work.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/segnet-a-deep-convolutional-encoder-decoder-architecture-for-image-segmentation/</description></item><item><title>A summary of U-Net: Convolutional Networks for Biomedical Image Segmentation by Olaf Ronneberger et al.</title><link>https://nsynovic.dev/summaries/u-net-convolutional-networks-for-biomedical-image-segmentation/</link><pubDate>Sat, 03 Dec 2022 15:05:16 -0600</pubDate><guid>https://nsynovic.dev/summaries/u-net-convolutional-networks-for-biomedical-image-segmentation/</guid><description>&lt;h1 id="a-summary-of-u-net-convolutional-networks-for-biomedical-image-segmentation">A summary of &lt;em>U-Net: Convolutional Networks for Biomedical Image Segmentation&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Olaf Ronneberger et al. arXiv, 2015 &lt;a href="http://arxiv.org/abs/1505.04597">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-u-net-convolutional-networks-for-biomedical-image-segmentation">A summary of &lt;em>U-Net: Convolutional Networks for Biomedical Image Segmentation&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>The problem addressed in this paper is to develop a fully convolutional semantic
segmentation model capable of learning from a small number of training examples
in a short period of time.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this paper because it builds off of the ideas of a fully
convolutional neural network for the purpose of semantic segmentation, but
introduces training techniques to reduce the number of training examples needed
as well as a novel architecture to support training on few examples.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is a semantic segmentation computer vision work.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This work is most closely related to papers that discuss the fully convolutional
solutions to the semantic segmentation task.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contributions are a novel architecture designed built around semantic
segmentation and a fully convolutional architecture. Additionally, they discuss
how they utilized data augmentation and elastic deformation to increase the
amount of training data.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done to develop fully convolutional neural networks, to develop
fully convolutional semantic segmentation models, and the development of data
augmentation techniques.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The figures are clear and easy to read.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>This is a well written paper that is easy to follow.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The author&amp;rsquo;s compare their semantic segmentation model against other models on
the ISBI EM Challenge.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The author assumes that data augmentation is enough to make a model robust
against rotation, shift, deformations, and gray value variations.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>I disagree with their assumptions as, while yes, data augmentation is necessary
to make a model or system robust against the different variance mentioned, also
exposing the model to more data should further increase the robustness of it.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d like to try and run the proposed model architecture on semantic segmentation
datasets that aren&amp;rsquo;t ones for cellular imagery.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>Just how much data was the model trained on?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>This is a fine paper. It is fairly short and is lacking technical details that
would be appreciated. For example, how much data was augmented? As well as the
metrics values proposed
&lt;a href="fully-convolutional-networks-for-semantic-segmentation.md">here&lt;/a>.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>U-Net: Convolutional Networks for Biomedical Image Segmentation&lt;/em> by
Olaf Ronneberger et al. [1] which describes a model architecture and training
details for semantic segmentation. The model architecture is known as a U-Net. A
U-Net is composed of two halves. The first half is a standard convolutional
network that encodes the feature map; the second half up-samples and decodes the
first and is then a convolutional neural network. The training methodology
proposed involves data augmentation to train this network on a small dataset.
The authors were able to achieve SOTA semantic segmentation performance on the
dataset the they were using.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/u-net-convolutional-networks-for-biomedical-image-segmentation/</description></item><item><title>A summary of Fully Convolutional Networks for Semantic Segmentation by Jonathan Long et al.</title><link>https://nsynovic.dev/summaries/fully-convolutional-networks-for-semantic-segmentation/</link><pubDate>Sat, 03 Dec 2022 14:24:59 -0600</pubDate><guid>https://nsynovic.dev/summaries/fully-convolutional-networks-for-semantic-segmentation/</guid><description>&lt;h1 id="a-summary-of-fully-convolutional-networks-for-semantic-segmentation">A summary of &lt;em>Fully Convolutional Networks for Semantic Segmentation&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Jonathan Long et al. arXiv, 2015 &lt;a href="http://arxiv.org/abs/1411.4038">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-fully-convolutional-networks-for-semantic-segmentation">A summary of &lt;em>Fully Convolutional Networks for Semantic Segmentation&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>That previous SOTA models for semantic segmentation did not utilize fully
convolutional neural networks.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Because it presents a methodology for using existing SOTA convolutional neural
networks for semantic segmentation.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is a deep learning computer vision semantic segmentation paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is related to works that involve the development of semantic
segmentation techniques.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contribution is an analysis of the usage of fully convolutional
neural networks for the purposes of semantic segmentation.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done to develop semantic segmentation models as well as developing
convolutional neural networks for the purposes of image classification.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The charts and figures are clear and easy to read.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is well written.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ol start="2">
&lt;li>F. Ning, D. Delhomme, Y. LeCun, F. Piano, L. Bottou, and P. E. Barbano.
Toward automatic phenotyping of developing embryos from videos. Image
Processing, IEEE Transactions on, 14(9):13601371, 2005.&lt;/li>
&lt;li>C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning hierarchical
features for scene labeling. Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 2013.&lt;/li>
&lt;li>P. H. Pinheiro and R. Collobert. Recurrent convolutional neural networks for
scene labeling. In ICML, 2014.&lt;/li>
&lt;/ol>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors compared a variety of semantic segmentation model using VGG,
GoogLeNet, and ResNet on several different datasets. These datasets include
NYUDv2, PASCAL VOC, and SIFT Flow. The metrics captured were pixel accuracy,
mean accuracy, mean IU, and frequency weighted IU.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors are reliant upon image classification models that have fully
connected layers for the purposes of classification. This is because these fully
connected layers are converted into fully convolutional layers.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>This reliance seems valid as most SOTA models for image classification (that I&amp;rsquo;m
aware of) utilize fully connected layers for the purposes of classification.
However, the method that the authors propose may not be transferable to present
or future convolutional networks that no longer rely on fully connected layers
for the purposes of classification.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d like to take the ideas and methodology proposed in this paper and apply them
to one shot object detection models to see if it is possible to create something
like a YOLO-Segmentation model.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>Is it possible to incorporate a fully connected layer for the purposes of
classification and additionally convolutional layers for the purposes of
semantic segmentation within the same layer by using a complicated branching
architecture (in the vein of GoogLeNet)?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>This is a good paper. I am concerned that the methodology presented may not be
transferable to models of the future that may not rely upon fully convolutional
layers to accomplish image classification.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Fully Convolutional Networks for Semantic Segmentation&lt;/em> by Jonathan
Long et al. [1] describes a methodology for converting an existing image
classification network into a semantic segmentation network. This is done by
replacing the fully connected layers at the head of the classification network
with one or more convolutional layers. This thereby makes semantic segmentation
networks full convolutional in terms of architecture design.&lt;/p>
&lt;p>However, not all models rely on this pattern and different architectures need to
be tested for each model conversion. For example, GoogLeNet has a different
architecture for semantic segmentation than VGG.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/fully-convolutional-networks-for-semantic-segmentation/</description></item><item><title>A summary of Focal Loss for Dense Object Detection by Tsung-Yi Lin et al.</title><link>https://nsynovic.dev/summaries/focal-loss-for-dense-object-detection/</link><pubDate>Sat, 03 Dec 2022 12:01:03 -0600</pubDate><guid>https://nsynovic.dev/summaries/focal-loss-for-dense-object-detection/</guid><description>&lt;h1 id="a-summary-of-focal-loss-for-dense-object-detection">A summary of &lt;em>Focal Loss for Dense Object Detection&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Tsung-yi Lin et al. arXiv, 2018 &lt;a href="http://arxiv.org/abs/1708.02002">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-focal-loss-for-dense-object-detection">A summary of &lt;em>Focal Loss for Dense Object Detection&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>This paper aims to address the problem that one stage object detectors (i.e.,
YOLO, SSD) face when trying to match the performance of SOTA two stage object
detectors which is class imbalance.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Because it introduces a new loss function that addresses the issue of class
imbalance when training dense, one stage object detectors. Additionally, the
authors released an example model implementing this loss known as Detectron/
RetinaNet.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is an algorithms and CV object detection paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is related to papers demonstrating or working on one stage object
detection models.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The author&amp;rsquo;s main contribution is a new loss function aimed at training one
stage object detection models that reduces the problem of class imbalance
between identifying objects in the foreground and background. Furthermore, the
authors have released an example model that was trained on this loss function
known as Detectron/RetinaNet.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done in developing classic object detectors, one and two stage
detectors, reducing class imbalance, and robust estimation techniques.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The figures are clear and understandable&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is well written and is dense with technical information.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ol start="2">
&lt;li>P. Doll  ar, Z. Tu, P. Perona, and S. Belongie. Integral channel features.
In BMVC, 2009.&lt;/li>
&lt;li>P. F. Felzenszwalb, R. B. Girshick, and D. McAllester. Cascade object
detection with deformable part models. In CVPR, 2010.&lt;/li>
&lt;li>T. Hastie, R. Tibshirani, and J. Friedman. The elements of statistical
learning. Springer series in statistics Springer, Berlin, 2008.&lt;/li>
&lt;li>W. Liu, D. Anguelov, D. Erhan, C. Szegedy, and S. Reed. SSD: Single shot
multibox detector. In ECCV, 2016.&lt;/li>
&lt;/ol>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>They compared their RetinaNet model against other SOTA object detectors on the
COCO dataset. Additionally, they compare the performance of models trained using
their Focal Loss and the Online Hard Example Mining (OHEM) technique.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>Their assumption is that one stage object detectors are the future.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>While having more options as to what type of object detector to choose from (one
or two stage), it is important to keep in mind that inference speed, accuracy,
recall, other metrics, and domain need all play an important role in what model
is selected for a particular task.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d like to implement Focal Loss in both a traditional YOLO network and a YOLO
network following the MobileNet architecture.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>Why was the COCO dataset chosen and not the ImageNet or Pascal VOC dataset for
training?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>This is a good paper. I would say that the size of the network is certainly
larger than previous one stage object detectors such as YOLO. Could it be
possible to reduce the size of the network to be comparable to these smaller
networks while maintaining the accuracy or achieving a better accuracy?&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Focal Loss for Dense Object Detection&lt;/em> by Tsung-Yi Lin et al. [1]
describes a new loss function aimed at improving the performance of one shot
object detection models that rely on region proposals. The problem that one shot
object detection models face compared against traditional two stage models that
utilize region proposals and object detection is that of class imbalance. Class
imbalance is simply that the region proposal network detects too many regions
where an object might be. This affects the performance of the object detection
component of the model as it might infer that an object is in a location that it
isn&amp;rsquo;t.&lt;/p>
&lt;p>To reduce this error, the authors of this paper propose the Focal Loss function,
a loss function aimed at reducing class imbalance. The function is
&lt;code>FL(pt) = (1  pt)^ log(pt)&lt;/code> where &lt;code> &amp;gt;= 0&lt;/code> They then trained a model
(RetinaNet) with this loss function on the COCO dataset and found that it
performed better than other on stage methods with respect to average precision.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/focal-loss-for-dense-object-detection/</description></item><item><title>A summary of You Only Look Once: Unified, Real-Time Object Detection by Joseph Redmon et al.</title><link>https://nsynovic.dev/summaries/you-only-look-once-unified-real0time-object-detection/</link><pubDate>Fri, 02 Dec 2022 21:50:54 -0600</pubDate><guid>https://nsynovic.dev/summaries/you-only-look-once-unified-real0time-object-detection/</guid><description>&lt;h1 id="a-summary-of-you-only-look-once-unified-real-time-object-detection">A summary of &lt;em>You Only Look Once: Unified, Real-Time Object Detection&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Joseph Redmon et al. CVPR, 2016 &lt;a href="https://doi.org/10.1109/CVPR.2016.91">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-you-only-look-once-unified-real-time-object-detection">A summary of &lt;em>You Only Look Once: Unified, Real-Time Object Detection&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>The authors wanted to create a very fast object detection network that handles
object detection using both region proposals and class probability maps in one
unified model.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Because this model was the fastest object detection model of its time with being
able to infrence at 45 FPS or 155 FPS.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is an object detection computer vision paper&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is related to work in real time object detection.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contribution is a model architecture (implemented with VGG) that is
very fast at performing object detection in real time (45 FPS) or super fast
(155 FPS) at the cost of accuracy.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done to develop real time object detection systems as well as
region proposal based object detection models.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The figures and tables in this paper are clear and easy to understand.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is well written.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors tested variations of their model on the PASCAL VOC dataset against
other SOTA models and measured the MAP percentage of the results. Furthermore,
they measured the real time object detection performance of their model
variations as well.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>YOLO trades accuracy for speed which the author&amp;rsquo;s argue in their Introduction is
applicable to the domain of self driving cars.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>While infrence speed is a necessity within that domain, I would argue that
accuracy is more important, as I would want a system that could detect a stop
sign in front of the car accurately but slower, than quickly but innacurately.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>This work has undergone many revisions since its initial publication. I&amp;rsquo;d like
to review the enhancements that were made to the model since this initial
publication and see what was changed and understand why.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>Is this model bottlenecked by the number of classes that it has to look at
and/or understand?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>This was a really good paper. I would encourage further work in this field and
specifically to test this model out on low powered devices.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>You Only Look Once: Unified, Real-Time Object Detection&lt;/em> by Joseph
Redmon et al. [1] describes an object detection strategy that aims to
outperform previous methods &lt;strong>in terms of infrence speed measured in FPS&lt;/strong> by
creating a unified model that can perform region proposals and class probability
mapping to a source image in parallel. This model is called YOLO.&lt;/p>
&lt;p>The authors of this paepr were able to accomplis this by using the following
technique:&lt;/p>
&lt;ol>
&lt;li>Divide the source image into many sub-sections.&lt;/li>
&lt;li>In parallel, compute the regions of interest as well as the class probability
mapping for each sub-section.&lt;/li>
&lt;li>Predict the class label and bounding boxes at the final layer by analyzing
the overlap between the class probability mapping and regions of interest.&lt;/li>
&lt;/ol>
&lt;p>Their methodology was implemented using the VGG architecture + extra layers to
accomodate for the parallel nature of the design.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/you-only-look-once-unified-real0time-object-detection/</description></item><item><title>A summary of Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks by Shaoqing Ren et al.</title><link>https://nsynovic.dev/summaries/faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks/</link><pubDate>Thu, 01 Dec 2022 21:54:52 -0600</pubDate><guid>https://nsynovic.dev/summaries/faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks/</guid><description>&lt;h1 id="a-summary-of-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks">A summary of &lt;em>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Shaoqing Ren et al. Posted in the IEEE Transactions on Patterns Analysis and
Machine Intelligence, 2017 &lt;a href="https://doi.org/10.1109/TPAMI.2016.2577031">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks">A summary of &lt;em>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>Fast R-CNN is able to perform fast inferencing on an image for object detection
&lt;em>after the region proposal step has been complete&lt;/em>. Therefore, if one could
improve the speed at which region proposal occurs at, then Fast R-CNN should
become even faster. Additionally, region proposal methods rely on handcrafted
features that are slow to use and biased towards their implementer. Furthermore,
while work regarding region proposal is typically done on the CPU,
re-engineering region proposal methods on the GPU only masks the underlying
algorithmic problem and implementation, and does not resolve the issue.
Therefore, a new approach must be taken to solve this issue.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this work as it proposes a Region Proposal Network (RPN)
that relies on deep learning (DL) techniques to identify the regions of
interest. Additionally, the authors were able to pass these into Fast R-CNN
almost cost free and were therefore able to improve both the training and
inferencing time.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is an algorithms and CV paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is related to works regarding object detection and region proposal
networks.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The author&amp;rsquo;s main contributions are the development of the Region Proposal
Network (RPN), methodologies for including proposed regions within Fast R-CNN
from an RPN almost cost free, and a system that can detect objects at 5 to 17
FPS.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done to hand create algorithms and filters for region proposal, as
well as DL techniques for identifying regions of interest. Additionally, work
has been done with respect to the shared computation of convolutions.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The tables and figures are clear.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is understandable.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ol start="2">
&lt;li>J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders. Selective
search for object recognition. IJCV, 2013.&lt;/li>
&lt;/ol>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors compare the number of proposals made as well as the inference rate
(measured in FPS) of RPN + Fast R-CNN on both the VGG and ZF models.
Additionally, they compared RPN against the Selective Search (SS) region
proposal technique on VGG. They also compared the one stage and two stage object
detection using RPN on the ZF model.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>Their assumption that RPN is a ubiquitous technique regardless of model
architecture when combined with Fast R-CNN.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>Maybe? The biggest concern that I see with that RPNs are reliant upon Fast R-CNN
to operate properly. This sounds like a limitation of the network for a
particular task and may be difficult to incorporate in future works.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d like to explore RPN usage in model architectures that do not utilize Fast
R-CNN.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>VGG was used as the base model for both this work and Fast R-CNN paper. Why is
that? Additionally, this paper calls out re-engineering region proposal
solutions to target the GPU to not solve the underlying problem, but the authors
did not specify if they made their solution run a GPU. Are RPNs more performant
on GPUs?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>This was a good paper. I found the writing style to be very terse which I wasn&amp;rsquo;t
a fan of as I find it a little boring. This is a personal preference and not a
demerit on the content of the paper nor the explanations given.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper, &lt;em>Faster R-CNN: Towards Real-Time Object Detection with Region
Proposal Networks&lt;/em> by Shaoqing Ren et al. [1] introduces the Region Proposal
Network (RPN) architecture for finding image region proposals for the purposes
of object detection, specifically with the &lt;a href="fast-r-cnn.md">Fast R-CNN&lt;/a> method.
RPNs are a convolutional neural network (CNN) that learns region proposals end
to end. The output of this network can be fed into an object detection model or
be incorporated as a one shot method directly with an existing model.&lt;/p>
&lt;p>To do so:&lt;/p>
&lt;ol>
&lt;li>An RPN is trained having the same number of convolutional layers as the
target model on the same dataset.&lt;/li>
&lt;li>A CNN using Fast R-CNN is trained on object detection using the regions
proposed from the RPN.&lt;/li>
&lt;li>The detection network initializes the RPN network, but the RPN&amp;rsquo;s
convolutional layers are fixed. This then only trains the RPN&amp;rsquo;s unique
layers.&lt;/li>
&lt;li>The detection network fixes its CNN layers, and fine tunes its unique layers.&lt;/li>
&lt;/ol>
&lt;p>By doing this, both the RPN and the Object Detection network now share the same
convolutional layers. This forms a unified network that allows for more accurate
region proposals.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks/</description></item><item><title>A summary of MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Andrew G. Howard et al.</title><link>https://nsynovic.dev/summaries/mobilenets-efficient-convolutional-neural-networks-for-mobile-vision-applications/</link><pubDate>Thu, 01 Dec 2022 01:02:34 -0600</pubDate><guid>https://nsynovic.dev/summaries/mobilenets-efficient-convolutional-neural-networks-for-mobile-vision-applications/</guid><description>&lt;h1 id="a-summary-of-mobilenets-efficient-convolutional-neural-networks-for-mobile-vision-applications">A summary of &lt;em>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Andrew G. Howard et al. arXiv, 2017 &lt;a href="http://arxiv.org/abs/1704.04861">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-mobilenets-efficient-convolutional-neural-networks-for-mobile-vision-applications">A summary of &lt;em>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>More and more CV models are getting better performance by &amp;ldquo;going deeper&amp;rdquo;.
However, these models can not be run efficiently on mobile and low powered
devices. Therefore, a new class of models (MobileNets) needs to be developed in
order to accommodate for these low powered and resource constrained devices.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this paper as it introduces a new class of CV models
targeted at mobile and low powered devices (MobileNets) as well as a method for
creating these models to meet specific latency and size specifications through
the usage of two hyper parameters.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is a deep learning CV paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This work is most similar to papers focusing on CV model architectures and low
powered computer vision.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The author&amp;rsquo;s main contributions are a new class of models for low powered
devices (MobileNets) and a methodology to generate these models targeting
specific sizes and latency requirements through the usage of two hyper
parameters.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>There has been work in developing small and efficient CV models previously.
Additionally, work has been done to develop depth wise separable convolutions
which are the backbone of this model class. Furthermore, work has been done in
the area of model compression and optimization.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the figures are clear. However, not all of the charts are made clear on
their own. some of the charts use an unlabeled log scale that is described in
the captions of the figure.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is well written.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ol start="2">
&lt;li>F. N. Iandola, M. W. Moskewicz, K. Ashraf, S. Han, W. J. Dally, and K.
Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and
1mb model size. arXiv preprint arXiv:1602.07360, 2016.&lt;/li>
&lt;li>J. Jin, A. Dundar, and E. Culurciello. Flattened convolutional neural
networks for feed forward acceleration. arXiv preprint arXiv:1412.5474, 2014.&lt;/li>
&lt;li>M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnornet: Imagenet
classification using binary convolutional neural networks. arXiv preprint
arXiv:1603.05279, 2016.&lt;/li>
&lt;li>M. Wang, B. Liu, and H. Foroosh. Factorized convolutional neural networks.
arXiv preprint arXiv:1608.04337, 2016.&lt;/li>
&lt;li>J. Wu, C. Leng, Y. Wang, Q. Hu, and J. Cheng. Quantized convolutional neural
networks for mobile devices. arXiv preprint arXiv:1512.06473, 2015.&lt;/li>
&lt;li>L. Sifre. Rigid-motion scattering for image classification. PhD thesis, Ph.
D. thesis, 2014.&lt;/li>
&lt;/ol>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors trained and tested variations of their MobileNets on tasks such as
fine grained recognition, large scale geolocation, face attributes, object
detection, and face embeddings. They then evaluated the performance of their
models against VGG and Inception V2 and V3. Comparisons were made by measuring
the number of billion multiply-adds and million parameters, as well as the mean
average precision.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The largest assumption that I see is that MobileNets would be applicable for
mobile and low powered hardware. The reason I say this is that the paper doesn&amp;rsquo;t
describe a test of running this model on a mobile device.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>No they do not. The name seems misleading without a proper test.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d like to test variations of MobileNets on modern smartphones, low powered
devices (i.e., Raspberry Pis, NVIDIA Jetson Nanos), and more powerful devices
(i.e., laptops) in order to see how far I can push a MobileNet to give real-time
(minimum 24 FPS) inference without compromising on accuracy.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>Why wasn&amp;rsquo;t a MobileNet variation tested on a mobile device?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>Overall this was a good paper. I found the description of the depth wise
separable convolutions a little confusing. Additionally, the naming of the
family models presented (MobileNets) is misleading to me as these models were
not tested on mobile devices.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision
Applications&lt;/em> by Andrew G. Howard et al. introduces a new class of models that
are designed to be small and fast (low latency) called MobileNets. MobileNets
are CNN based DL models that rely on depth wise separable convulsions to
inference data. A depth wise separable convolution occurs in two steps:&lt;/p>
&lt;ol>
&lt;li>Each channel of the input data structure is analyzed using a 3 x 3 x 1
filter. This is different than traditional CNN building blocks that analyze
all of the channels in one pass using a 3 x 3 x N filter (with N being the
number of filters).&lt;/li>
&lt;li>The values from the aforementioned filters are multiples together and summed
with a point wise convolution that analyzes all of the channels. This point
wise convolution uses a 1 x 1 x N filter (with N being the number of
filters).&lt;/li>
&lt;/ol>
&lt;p>By using depth wise separable filters, MobileNets decrease their latency due to
the fewer number of multiply-add operations that they need to perform on the
data.&lt;/p>
&lt;p>The authors tested the performance of MobileNets on a variety of tasks against
the VGG and Inception class of CV models, and found that MobileNets are both
smaller, faster, and nearly as accurate as these much larger models.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/mobilenets-efficient-convolutional-neural-networks-for-mobile-vision-applications/</description></item><item><title>A summary of Deep Residual Learning for Image Recognition by Kaiming He et al.</title><link>https://nsynovic.dev/summaries/deep-residual-learning-for-image-recognition/</link><pubDate>Thu, 01 Dec 2022 00:29:40 -0600</pubDate><guid>https://nsynovic.dev/summaries/deep-residual-learning-for-image-recognition/</guid><description>&lt;h1 id="a-summary-of-deep-residual-learning-for-image-recognition">A summary of &lt;em>Deep Residual Learning for Image Recognition&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Kaiming He et al. arXiv, 2015 &lt;a href="http://arxiv.org/abs/1512.03385">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-deep-residual-learning-for-image-recognition">A summary of &lt;em>Deep Residual Learning for Image Recognition&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>SOTA deep learning (DL) computer vision (CV) models are becoming increasingly
more common. The trend with these models is to make them deeper to get better
results. However, the deeper a model is, the harder it is to train due to
vanishing gradients, and the longer it takes to train it as there are more
parameters. Additionally, as the network gets deeper, the accuracy starts to
degrade. This paper aims to address the later two issues with training very
large networks for the purposes of image recognition.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Because it introduces a deep residual learning framework. This framework is
simple to implement, but allows for models to become far deeper than before due
to its usage of skip connections. These skip connections simply add the identity
of the previous residual learning building block to the output of the current
building block.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is a DL CV paper. It is also a CV framework and model paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is related to works that aim to create better CV models.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contributions is the deep residual learning framework and the SOTA
ResNet suite of models, as well as an understanding of how residual learning
works via shortcuts.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done to explore skip connections and residual networks, as well as
creating other DL CV models.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the figures are properly labeled and are clear to read.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is well written.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ol start="2">
&lt;li>F. Perronnin and C. Dance. Fisher kernels on visual vocabularies for image
categorization. In CVPR, 2007.&lt;/li>
&lt;li>R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks.
arXiv:1505.00387, 2015.&lt;/li>
&lt;li>R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep networks.
1507.06228, 2015.&lt;/li>
&lt;li>V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann
machines. In ICML, 2010.&lt;/li>
&lt;/ol>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors compared their ResNet family of models against CV models that did
not adhere to the residual learning framework, along with other SOTA models, on
image classification on the ImageNet and CIFAR-10 datasets, as well as object
detection on the PASCAL and MS COCO datasets.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors assumed that better models can be made by just adding more layers.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>Not entirely. Better architected models rely on more layers yes. However, better
models need more data to perform better as well. Additionally, the usage of
neural compressors or optimizers can adjust the performance of a CV model as
well.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d like to see how far I can push the ResNet model family with respect to
quantization and optimization in order to run these extremely deep models on
mobile or low powered hardware.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>What was the system specification that they used to train the ResNet model
family on?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>This was a very clear and concise paper on a model architecture.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper, &lt;em>Deep Residual learning for Image Recognition&lt;/em> by Kaiming He et al.
[1] introduced the ResNet family of DL CV models. These models are based on a
deep residual learning framework that makes usage of shortcut connections
(taking the identity function of the previous deep residual block and summing it
to the output of the current residual learning block) and the ReLU activation
function to reduce the total number of parameters within the model, while
dramatically increasing the depth in comparison to former SOTA methods (i.e.,
VGG). The authors found that their deep models achieve SOTA performance on both
Object Detection (using the Faster-RCNN method) and Image Recognition compared
to existing SOTA models. Furthermore, they found that the deeper the network,
the more accurate and less error prone it is with respect to image recognition.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/deep-residual-learning-for-image-recognition/</description></item><item><title>A summary of Large-Scale Image Retrieval with Attentive Deep Local Features by Hyeonwoo Noh et al.</title><link>https://nsynovic.dev/summaries/large-scale-image-retrieval-with-attentive-deep-local-features/</link><pubDate>Wed, 30 Nov 2022 16:14:49 -0600</pubDate><guid>https://nsynovic.dev/summaries/large-scale-image-retrieval-with-attentive-deep-local-features/</guid><description>&lt;h1 id="a-summary-of-large-scale-image-retrieval-with-attentive-deep-local-features">A summary of &lt;em>Large-Scale Image Retrieval with Attentive Deep Local Features&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Hyeonwoo Noh et al. ICCV, 2017 &lt;a href="https://doi.org/10.1109/ICCV.2017.374">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-large-scale-image-retrieval-with-attentive-deep-local-features">A summary of &lt;em>Large-Scale Image Retrieval with Attentive Deep Local Features&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>The authors intend to address the problem of image retrieval when images are
occuleded or have objects blocking the subject by taking a weakly supervised
Deep Learning (DL) approach. Additionally, they propose a large scale dataset
that would assist the image retrieval community in creating new SOTA models.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this paper because it proposes a SOTA method for generating
robust image features using a DL approach. Additionally, it is the paper that
proposes the large scale Google-Landmarks dataset.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is a deep learnign computer vision paper as well as a datasets
release paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This work is related to image feature extraction, image retrieval, computer
vision, and deep learning computer vision.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contributions are a SOTA deep learning computer vision model for
image retireval as well as a large scale dataset for training similar image
retrieval models.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done to create both hand crafted and DL solutions to image
retrieval and image feature extraction.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All figures are properly labeled and well explained.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is well written.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ol start="2">
&lt;li>A. Gordo, J. Almazan, J. Revaud, and D. Larlus. Deep Image Retrieval:
Learning Global Representations for Image Search. In Proc. ECCV, 2016.&lt;/li>
&lt;li>F. Radenovi  c, G. Tolias, and O. Chum. CNN Image Retrieval Learns from BoW:
Unsupervised Fine-Tuning with Hard Examples. In Proc. ECCV, 2016.&lt;/li>
&lt;li>U. Buddemeier and H. Neven. Systems and Methods for Descriptor Vector
Computation, 2012. US Patent 8,098,938.&lt;/li>
&lt;li>H. Neven, G. Rose, and W. G. Macready. Image Recognition with an Adiabatic
Quantum Computer I. Mapping to Quadratic Unconstrained Binary Optimization.
arXiv:0804.4457, 2008.&lt;/li>
&lt;li>K. M. Yi, E. Trulls, V. Lepetit, and P. Fua. LIFT: Learned Invariant Feature
Transform. In Proc. ECCV, 2016.&lt;/li>
&lt;li>R. Arandjelovi  c, P. Gronat, A. Torii, T. Pajdla, and J. Sivic. NetVLAD:
CNN Architecture for Weakly Supervised Place Recognition. In Proc. CVPR,
2016.&lt;/li>
&lt;li>H. J  egou, M. Douze, C. Schmidt, and P. Perez. Aggregating Local
Descriptors into a Compact Image Representation. In Proc. CVPR, 2010.&lt;/li>
&lt;li>H. J  egou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and C. Schmid.
Aggregating Local Image Descriptors into Compact Codes. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 34(9), 2012.&lt;/li>
&lt;/ol>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors compared variations of DELF against SOTA image retrieval techniques
including CONGAS [4, 5], DIR [2], siaMAC [3] and LIFT [6] and graphed
precision vs recall on the Google Landmarks dataset and the accuracy of the same
methods on smaller datasets.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>That GPS coordinates were a useful feature to include in the Google Landmarks
dataset.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>I found it interesting that the authors included the GPS coordinates of images
selected for the Google Landmarks dataset. I&amp;rsquo;m not sure how useful this is for
image retrieval tasks as it relies upon image features, rather than resolving
GPS coordinates. While the authors used it for validating the ground truth
labels for the landmarks, actual usage for the purposes of image retrieval
alludes me.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d like to compare this result to DL models trained on both
&lt;a href="surf-speeded-up-robust-features.md">SURF&lt;/a> and
&lt;a href="distinctive-image-features-from-scale-invariant-keypoints.md">SIFT&lt;/a> feature
descriptions.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>Why was ResNet-50 choosen as the base model? The Google Landmarks dataset has
one million images, but only ~13,000 landmarks (~77 images per landmark). Is
there bias as to where the landmarks are choosne from? Is that enough landmarks
to train a DL model for the purposes of image feature description?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>A solid paper, however the discussion of the Google Landmarks dataset left me
wanting more that, in my opinion, could&amp;rsquo;ve been published in a blog post.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Large-Scale Image Retrieval with Attentive Deep Local Features&lt;/em> by
Hyeonwoo Noh et al. discusses both a new image retrieval method that achieves
SOTA performance based off of ResNet-50 (DELF), and an image retrieval dataset
(Google Landmarks) to create similar models. DELF aims to work as an image
feature descriptor that is robust to occulusion and background clutter. DELF
achieves SOTA performance on the Google Landmarks dataset compared against
previous SOTA methods including CONGAS [4, 5], DIR [2], siaMAC [3] and
LIFT [6]. Additionally, DELF is more accurate the aforementioned methods on
smaller, traditional datasets.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/large-scale-image-retrieval-with-attentive-deep-local-features/</description></item><item><title>A summary of SURF: Speeded Up Robust Features by Herbert Bay et al.</title><link>https://nsynovic.dev/summaries/surf-speeded-up-robust-features/</link><pubDate>Wed, 30 Nov 2022 15:14:56 -0600</pubDate><guid>https://nsynovic.dev/summaries/surf-speeded-up-robust-features/</guid><description>&lt;h1 id="a-summary-of-surf-speeded-up-robust-features">A summary of &lt;em>SURF: Speeded Up Robust Features&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Herbert Bay et al., 2006 &lt;a href="http://link.springer.com/10.1007/11744023_32">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-surf-speeded-up-robust-features">A summary of &lt;em>SURF: Speeded Up Robust Features&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>The authors aim to make a repeatable, robust, and fast image feature extractor
that outperforms previous SOTA feature extractors.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Because this paper builds off of SIFT and allows for even faster feature
extraction without additional computational cost by relying upon Hessian
matrices and integral images.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is an algorithms paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This work is related to papers regarding image retrieval, object recognition,
and image feature extraction.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contributions is a SOTA algorithm for image feature extraction that
is fast due to their usage of the Hessian matrices and the integral image. The
Hessian matrices can be thought of as filters that are slid across the integral
image to identify features.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done to create image feature extractors that are robust and
invariant to scale and rotation.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the figures are clearly labeled and have good captions.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is well written.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ol start="2">
&lt;li>Ke, Y., Sukthankar, R.: PCA-SIFT: A more distinctive representation for local
image descriptors. In: CVPR (2). (2004) 506  513&lt;/li>
&lt;li>Koenderink, J.: The structure of images. Biological Cybernetics 50 (1984) 363
370&lt;/li>
&lt;li>Lowe, D.: Distinctive image features from scale-invariant keypoints, cascade
filtering approach. IJCV 60 (2004) 91  110&lt;/li>
&lt;li>Mikolajczyk, K., Schmid, C.: A performance evaluation of local descriptors.
PAMI 27 (2005) 16151630&lt;/li>
&lt;li>Lowe, D.: Object recognition from local scale-invariant features. In: ICCV.
(1999)&lt;/li>
&lt;li>Mikolajczyk, K., Schmid, C.: Indexing based on scale invariant interest
points. In: ICCV. Volume 1. (2001) 525  531&lt;/li>
&lt;/ol>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors compared the repeatability of the SURF descriptor (Fast-Hessian)
against DoG [6], Harris-Laplace [7], and Hessian-Laplace [7] across
several common bench marking images. Additionally, they measured the precision
vs recall of different algorithms including SURF-128 (a variation of SURF the
authors proposed that results in a higher dimensional feature space), SURF, SIFT
[4], GLOH [5], and PCA-SIFT [2] with respect to object recognition.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>Probably the biggest assumption that I saw in the paper is that a scale and
image rotation invariance are the two biggest features to focus on when
designing a detector.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>No their assumption doesn&amp;rsquo;t seem valid. In their own words, &amp;ldquo;Skew, anisotropic
scaling, and perspective effects are assumed to be second-order effects, that
are covered to some degree by the overall robustness of the descriptor,&amp;rdquo; [1]
however, I would argue that with the advent of fish-eyed lenses on smartphones
and action-cameras, skew is now (and should be) a first-order priority.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>Similar to the future work that I proposed
&lt;a href="distinctive-image-features-from-scale-invariant-keypoints.md">here&lt;/a>, I&amp;rsquo;d like
to train a Deep Learning model on SURF descriptions and see what results I would
get with respect to object recognition. Would it be better than a CNN based
model?&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>How robust is this image on 180 degree images? 360 degree images? Does object
recognition or image retrieval fail at such extreme angles?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>I really liked the work that I read here as well as the results of the paper.
However, as this work is under a patent, I would like to see the source code
open-sourced so that it could be improved upon and freely implemented in modern
solutions.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>A summary of SURF: Speeded Up Robust Features&lt;/em> by Herbert Bay et al.
[1] describes an algorithm for extracting image features that are robust to
scale and rotation while also being faster and more repeatable than previous
SOTA methods. To achieve this, the authors utilized the source image&amp;rsquo;s integral
image (the sum of all the pixels of a rectangular area of the image between the
origin and the pixel) to compute Hessian filters that a then slid across the
image at different scales to identify robust features. It should be noted that
the filters are increased in size and not the image. This allows for robust
features to be identified much faster than if the image was scaled down and the
same filter was slid across the image.&lt;/p>
&lt;p>The authors compare SURF&amp;rsquo;s repeatability by comparing it to other feature
descriptors, as well as its ability at object recognition against other feature
extractors using nearest neighbor algorithms.&lt;/p>
&lt;p>The authors also propose U-SURF (Upright SURF), which is faster to compute as it
doesn&amp;rsquo;t find features that are robust against rotation. Additionally, they
propose SURF-128 which is similar to vanilla SURF, but reports features in a
high dimensionality. This is computed in a similar time as SURF, but takes
longer to match features against as there are more features to account for.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/surf-speeded-up-robust-features/</description></item><item><title>A summary of Distinctive Image Features from Scale-Invariant Keypoints by David G. Lowe</title><link>https://nsynovic.dev/summaries/distinctive-image-features-from-scale-invariant-keypoints/</link><pubDate>Fri, 18 Nov 2022 20:47:39 -0600</pubDate><guid>https://nsynovic.dev/summaries/distinctive-image-features-from-scale-invariant-keypoints/</guid><description>&lt;h1 id="a-summary-of-distinctive-image-features-from-scale-invariant-keypoints">A summary of &lt;em>Distinctive Image Features from Scale-Invariant Keypoints&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>David G. Lowe International Journal of Computer Vision, 2004
&lt;a href="https://doi.org/10.1023/B:VISI.0000029664.99615.94">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-distinctive-image-features-from-scale-invariant-keypoints">A summary of &lt;em>Distinctive Image Features from Scale-Invariant Keypoints&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>This paper aims to address image matching challenges by finding scale invariant
features from images. These features perform well against images that are
subject to blurring and noise.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this paper as it presents an efficient method for
generating image features that are invariant to scale and rotation changes. This
allows for images to be taken in arbitrary locations and at different locations
to then be matched with similar objects in a different image. In other words, it
presents an efficient way of generating image features that can be used to
compare how similar two images are regardless of scale and rotation differences.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is an algorithms paper focused on image matching and retrieval.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This work is most similar to work discussing image feature extraction and image
matching and retrieval techniques.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contributions were a methodology for extracting scale invariant image
features as well as methods for comparing features between images for the
purposes of image matching and retrieval.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done to develop image feature extractors. These extractors were
initially used for stereo and short range motion tracking. However, they are now
capable of more complex tasks. These include image recognition and retrieval.
All of these feature extractors produce a representation of the image that can
be used to compare one image against another.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the figures are clearly labeled. I do find the lines on the line charts
are a bit difficult to distinguish due to the usage of a tight dashed line. But
that&amp;rsquo;s on me, not the paper.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is well written, if a bit dense. There is an argument to be made that
this paper is two papers in one. One about a novel feature extraction technique,
and a second about image retrieval with the usage of feature extractors.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ol start="2">
&lt;li>Brown, M. and Lowe, D.G. 2002. Invariant features from interest point groups.
In British Machine Vision Conference, Cardiff, Wales, pp. 656665.&lt;/li>
&lt;li>Carneiro, G. and Jepson, A.D. 2002. Phase-based local features. In European
Conference on Computer Vision (ECCV), Copenhagen, Denmark, pp. 282296.&lt;/li>
&lt;li>Crowley,J.L.and Parker,A.C.1984.A representation for shapes based on peaks
and ridges in the difference of low-pass transform. IEEE Trans. on Pattern
Analysis and Machine Intelligence, 6(2):156170.&lt;/li>
&lt;li>Fergus, R., Perona, P., and Zisserman, A. 2003. Object class recognition by
unsupervised scale-invariant learning. In IEEE Conference on Computer Vision
and Pattern Recognition, Madison, Wisconsin, pp. 264271.&lt;/li>
&lt;li>Harris,C.and Stephens,M.1988.A combined corner and edge detector. In Fourth
Alvey Vision Conference, Manchester, UK, pp. 147151.&lt;/li>
&lt;li>Koenderink, J.J. 1984. The structure of images. Biological Cybernetics,
50:363396.&lt;/li>
&lt;/ol>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>For their feature extractor, they created a dataset of images and their
features, then performed transformations on images present in the dataset to
generate new features. With these variables, they were able to measure the
performance of their feature extractor and how well it was able to identify
invariant features. With respect to their testing on object recognition and
image retrieval, they utilized K Nearest Neighbor (KNN) algorithms to accomplish
this.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>This work doesn&amp;rsquo;t rely on the usage of Deep Neural Networks (DNNs) to learn the
representation of images. Because of this, their work relies on hand crafted
filters and algorithms to extract features. This could result in algorithmic
bias or generate results that are susceptible to the views of the author.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>Keeping in mind that this paper was published in 2004, this assumption seems
valid for the time. Due to the AI winter as well as the limited usage of GPUs
for the purposes of training DNNs, handcrafting feature extractors was a valid
usage.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>While I can&amp;rsquo;t say that image retrieval is of much interest to me, I would like
to explore how to perform object detection or image recognition using this
feature extractor. Additionally, it would be really cool to see if I could
utilize this feature extractor on low powered devices for the purposes of image
classification.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>The &lt;code>Background&lt;/code> section of this paper mentioned the usage of feature detectors
for motion capture. Is that possible with this feature extractor? What does that
space look like today vs 2004 vs 1990s? What would happen if I trained a Deep
Learning model on SIFT features? Could I get a comparable output to a CNN with
respect to image classification (for example)?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>This is a great paper that introduces a novel technique for performing feature
extraction. However, it is a bit dense and could&amp;rsquo;ve been split into two separate
papers. One being an algorithms paper presenting the feature extractor, and
another being a case study of feature extractor performance on many tasks.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Distinctive Image Features from Scale-Invariant Keypoints&lt;/em> by David
G. Lowe [1] presents a novel image feature extractor called Scale Invariant
Feature Transform (SIFT). SIFT is an algorithm to extract features from an image
that are invariant (do not change) to scale and rotation. These features can be
used to perform image retrieval and object recognition by utilizing nearest
neighbor algorithms such as KNN or ANN.&lt;/p>
&lt;p>This summary was kept short as I have been sitting on this summary for well over
two weeks now with no progress and just want to get something out. Sorry for the
brevity and weak summary.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/distinctive-image-features-from-scale-invariant-keypoints/</description></item><item><title>A summary of Hidden Technical Debt in Machine Learning Systems by D. Sculley et al.</title><link>https://nsynovic.dev/summaries/hidden-technical-debt-in-machine-learning-systems/</link><pubDate>Fri, 11 Nov 2022 09:55:18 -0600</pubDate><guid>https://nsynovic.dev/summaries/hidden-technical-debt-in-machine-learning-systems/</guid><description>&lt;h1 id="a-summary-of-hidden-technical-debt-in-machine-learning-systems">A summary of &lt;em>Hidden Technical Debt in Machine Learning Systems&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>D. Sculley Proceedings of NeurIPS, 2015
&lt;a href="https://proceedings.neurips.cc/paper/2015/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-hidden-technical-debt-in-machine-learning-systems">A summary of &lt;em>Hidden Technical Debt in Machine Learning Systems&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>Machine learning systems incur technical debt like traditional software systems.
However, they also incur additional debt that traditional software systems do
not.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Because this paper aims at establishing definitions for potential technical debt
that can be incurred while developing machine learning systems.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is a language definition paper. In other words, it is suggesting language
to be used when describing the technical debt of machine learning systems.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is related to those that discuss and define technical debt broadly
and in domain specific applications.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contributions are definitions and language to be used when describing
the different technical debt that can be incurred while developing machine
learning systems.&lt;/p>
&lt;p>These debts include:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Boundary Erosion&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Entanglement&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Hidden Feedback Loops&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Undeclared Consumers&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Data Dependencies&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Configuration Issues&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Changes in the External World&lt;/strong>&lt;/li>
&lt;li>&lt;strong>System-Level Anti-Patterns&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Traditional Software System Technical Debt&lt;/strong>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done to identify traditional software technical debt and software
system anti-patterns.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>There is one figure in this paper. While the visualization works, the usage of a
gray scale image makes it difficult to read the text.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is well written. The structure of this paper is quite simplistic,
which I appreciate as it makes it easier to digest.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ol start="2">
&lt;li>J. D. Morgenthaler, M. Gridnev, R. Sauciuc, and S. Bhansali. &lt;em>Searching for
build debt: Experiences managing technical debt at google&lt;/em>. In Proceedings of
the Third International Workshop on Managing Technical Debt, 2012.&lt;/li>
&lt;li>H. B. McMahan, G. Holt, D. Sculley, M. Young, D. Ebner, J. Grady, L. Nie, T.
Phillips, E. Davydov, D. Golovin, S. Chikkerur, D. Liu, M. Wattenberg, A. M.
Hrafnkelsson, T. Boulos, and J. Kubica. &lt;em>Ad click prediction: a view from the
trenches&lt;/em>. In The 19th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD 2013, Chicago, IL, USA, August 11-14, 2013,
2013.&lt;/li>
&lt;li>. Langford and T. Zhang. &lt;em>The epoch-greedy algorithm for multi-armed bandits
with side information&lt;/em>. In Advances in neural information processing systems,
pages 817824, 2008.&lt;/li>
&lt;li>T. M. Chilimbi, Y. Suzue, J. Apacible, and K. Kalyanaraman. &lt;em>Project adam:
Building an efficient and scalable deep learning training system&lt;/em>. In 11th
USENIX Symposium on Operating Systems Design and Implementation, OSDI 14,
Broomfield, CO, USA, October 6-8, 2014., pages 571582, 2014.&lt;/li>
&lt;li>B. Dalessandro, D. Chen, T. Raeder, C. Perlich, M. Han Williams, and F.
Provost. &lt;em>Scalable hands free transfer learning for online advertising&lt;/em>. In
Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 15731582. ACM, 2014.&lt;/li>
&lt;li>M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V. Josifovski, J.
Long, E. J. Shekita, and B. Su. &lt;em>Scaling distributed machine learning with
the parameter server&lt;/em>. In 11th USENIX Symposium on Operating Systems Design
and Implementation, OSDI 14, Broomfield, CO, USA, October 6-8, 2014., pages
583598, 2014.&lt;/li>
&lt;li>D. Sculley, M. E. Otey, M. Pohl, B. Spitznagel, J. Hainsworth, and Y. Zhou.
&lt;em>Detecting adversarial advertisements in the wild. In Proceedings of the 17th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining&lt;/em>,
San Diego, CA, USA, August 21-24, 2011, 2011&lt;/li>
&lt;/ol>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The author&amp;rsquo;s look to traditional software and machine learning system examples
for anti-patterns and&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>As this paper proposes potential areas of technical debt for machine learning
systems, nearly all of it is assuming that something will go wrong when working
with a machine learning system.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>Given the type of paper that this is, yes, the assumptions seem valid.
Furthermore, the justifications for each type of technical debt are well
explained and made clear in the literature.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>As I&amp;rsquo;m now starting to study ML dependencies, this work is a great springboard
to drill deeper into what both ML and software engineers consider technical debt
and dependents.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>Are there identifiable cases of ML models that have one or more of the technical
debts described here?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>Overall this was a solid paper. I&amp;rsquo;d appreciate more real world examples of
models that have dealt with technical debt. Additionally, a survey of engineers
about whether they think each type of technical debt is worthy of considerations
would have been appreciated.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Hidden Technical Debt in Machine Learning Systems&lt;/em> by D. Sculley et
al. [1] presents potential technical debt considerations that ML engineers
need to consider when developing ML systems. These considerations stem both from
experience as well as traditional software engineering technical debt.
Furthermore, they pose several questions that engineers should ask when taking
on technical debt.&lt;/p>
&lt;p>These questions are:&lt;/p>
&lt;ul>
&lt;li>How easily can an entirely new algorithmic approach be tested at full scale?&lt;/li>
&lt;li>What is the transitive closure of all data dependencies?&lt;/li>
&lt;li>How precisely can the impact of a new change to the system be measured?&lt;/li>
&lt;li>Does improving one model or signal degrade others?&lt;/li>
&lt;li>How quickly can new members of the team be brought up to speed?&lt;/li>
&lt;/ul>
&lt;p>The authors defined several areas where technical debt can accrue:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Model Boundary Erosion&lt;/strong>
&lt;ul>
&lt;li>&lt;em>Entanglement&lt;/em>: CACE principle (Changing Anything Changes Everything) which
applies to both data and model training&lt;/li>
&lt;li>&lt;em>Correction Cascades&lt;/em>: Transfer learning/ fine tuning a PTM creates a new
model (B) which is now dependent upon the original model&amp;rsquo;s (A) weights and
architecture&lt;/li>
&lt;li>&lt;em>Undeclared Consumers&lt;/em>: Similar to visibility debt [2]; creates tight and
hidden coupling to the model which if changed, could affect the wider system&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Data Dependencies&lt;/strong>: Static analysis of data dependencies could resolve this
[3]
&lt;ul>
&lt;li>&lt;em>Unstable Data Dependencies&lt;/em>: Some data signals change qualitatively or
quantitatively over time. Therefore, improving input signals to the system
could harm the output signals of the model&lt;/li>
&lt;li>&lt;em>Underutilized Data Dependencies&lt;/em>: Similar to underutilized dependencies in
traditional software engineering; data signals that the model is trained on
but have little to no effect on the output signal. Removing these signals
post-training, however, could greatly affect the quality of the model.
Examples include:
&lt;ul>
&lt;li>&lt;code>Legacy Features&lt;/code>&lt;/li>
&lt;li>&lt;code>Bundled Features&lt;/code>: Features that collectively improve performance that
are bundled together&lt;/li>
&lt;li>&lt;code>-Features&lt;/code>: Adding features to marginally improve accuracy&lt;/li>
&lt;li>&lt;code>Correlated Features&lt;/code>: Models struggle to distinguish between two
correlated features, one of which is causal (important) and the other
non-causal (not important)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Feedback Loops&lt;/strong>
&lt;ul>
&lt;li>&lt;em>Direct Feedback Loops&lt;/em>: A model may directly influence the selection if its
own future training data based on the decisions it makes&lt;/li>
&lt;li>&lt;em>Hidden Feedback Loops&lt;/em>: Two systems may influence each other indirectly by
affecting the sources of their data&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>ML-System Anti-Patterns&lt;/strong>
&lt;ul>
&lt;li>&lt;em>Glue Code&lt;/em>: Trying to stitch together two incompatible components of a
system results in significant code overhead&lt;/li>
&lt;li>&lt;em>Pipeline Jungles&lt;/em>: Data pre-processing&lt;/li>
&lt;li>&lt;em>Dead Experimental Codepaths&lt;/em>&lt;/li>
&lt;li>&lt;em>Abstraction Debt&lt;/em>: There is a lack of standardized abstractions for ML
systems components&lt;/li>
&lt;li>&lt;em>Common Smells&lt;/em>:
&lt;ul>
&lt;li>&lt;code>Plain-Old-Data Type Smell&lt;/code>&lt;/li>
&lt;li>&lt;code>Multiple-(Programming) Language Smell&lt;/code>&lt;/li>
&lt;li>&lt;code>Prototype Smell&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Configuration Debt&lt;/strong>
&lt;ul>
&lt;li>Taken verbatim from the paper:&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-md" data-lang="md">&lt;span style="display:flex;">&lt;span> It should be easy to specify a configuration as a small change from a previous configuration.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> It should be hard to make manual errors, omissions, or oversights.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> It should be easy to see, visually, the difference in configuration between two models.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> It should be easy to automatically assert and verify basic facts about the configuration:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>number of features used, transitive closure of data dependencies, etc.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> It should be possible to detect unused or redundant settings.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Configurations should undergo a full code review and be checked into a repository
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>&lt;strong>A Changing External World&lt;/strong>
&lt;ul>
&lt;li>&lt;em>Fixed Threshold in Dynamic Systems&lt;/em>: Manually selecting a decision
threshold that a system has to abide by.&lt;/li>
&lt;li>&lt;em>Monitoring and Testing&lt;/em>: &amp;ldquo;Comprehensive live monitoring of system behavior
in real time combined with automated response is critical for long-term
system reliability&amp;rdquo;
&lt;ul>
&lt;li>&lt;code>Prediction Bias&lt;/code>&lt;/li>
&lt;li>&lt;code>Action Limits&lt;/code>&lt;/li>
&lt;li>&lt;code>Up-Stream Producers&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>Data Testing Debt&lt;/em>&lt;/li>
&lt;li>&lt;em>Reproducibility Debt&lt;/em>: It is difficult to reproduce results from ML
research due to randomized algorithms, non-determinism inherent in parallel
learning, initial conditions, and interactions with the external world&lt;/li>
&lt;li>&lt;em>Process Management Debt&lt;/em>: How does one handle a system with many models?&lt;/li>
&lt;li>&lt;em>Cultural Debt&lt;/em>: Difficulty re-using academic research in industry&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/hidden-technical-debt-in-machine-learning-systems/</description></item><item><title>A summary of Fast R-CNN by Ross Girshick</title><link>https://nsynovic.dev/summaries/fast-r-cnn/</link><pubDate>Thu, 10 Nov 2022 15:18:13 -0600</pubDate><guid>https://nsynovic.dev/summaries/fast-r-cnn/</guid><description>&lt;h1 id="a-summary-of-fast-r-cnn">A summary of &lt;em>Fast R-CNN&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Ross Girshick, IEEE International Conference on Computer Vision, 2015;
&lt;a href="https://openaccess.thecvf.com/content_iccv_2015/html/Girshick_Fast_R-CNN_ICCV_2015_paper.html">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-fast-r-cnn">A summary of &lt;em>Fast R-CNN&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>The problem addressed in this paper is that there exists a method that is better
at performing object detection and semantic segmentation within region proposals
that is not implemented in either the original R-CNN model or the SPPNet model.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Because it provides an updated model architecture for performing object
detection and semantic segmentation within region proposals, thereby speeding up
inference time and reducing computational cost.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is a CNN paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>Papers that discuss CNN models with respect to region proposals.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>An updated R-CNN model that is substantially faster than the original R-CNN
model and the competing SPPNet model. This Fast R-CNN model achieves SOTA mean
average precision (mAP) on the PASCAL VOC 2007, 2010, and 2012 datasets. Fast
training and testing compared to R-CNN and SPPNet. And that fine tuning ConvNet
layers in VGG 16 improves mAP.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Region proposal convolutional neural networks have been created prior to this
work. Furthermore, this work utilizes techniques that other successful CV DL
models have utilized to achieve SOTA results.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The majority of the figures are clear. Figure 2 is a bit difficult to read due
to how squished the text is to each other. Additionally, the model architecture
in Figure 1 uses an identical image as presented in the seminal R-CNN paper. It
would have been nicer to see a different test image utilized for this paper.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is well written, if a bit technical. However, the technicality is
important as it distinguishes the improvements made to the original R-CNN and
SPPNet models.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>They performed a similar study to their previous paper (R-CNN) where they
compared the mAP of competing models against their model. Additionally, they
performed an analysis of their model where they tested different improvements
and DL techniques used in other models to improve performance.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>They utilized the VGG 16 model as their CNN model. However, other existing
models could&amp;rsquo;ve been used/ re-implemented with their fast region proposal model
to potentially improve performance.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>This assumption makes sense to a degree as VGG 16 is a popular model for
research purposes. However, evaluating other CNN models would have been more
interesting in my opinion.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d like to implement there work on non-VGG 16 models, such as ResNet or on a
MobileNet.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>Why weren&amp;rsquo;t other models implemented with the fast region proposal component?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>Overall good paper. I don&amp;rsquo;t recommend on creating a paper of a third variation
of this model unless there are substantial improvements made. These improvements
can be in further reducing computational or energy cost, an even simpler
architecture, or an substantial overall increase of mAP on the PASCAL VOC
datasets.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Fast R-CNN&lt;/em> by Ross Girshick [1] proposes a new method to perform
region proposal CNN tasks that is significantly faster than the previously
proposed method. To do so, both the region proposals and the image itself are
passed into the CNN layer for analysis. Additionally, many layers of the
previous architectures are collapsed into one to reduce the complexity.
Furthermore, the SVM classifier was replaced with a Softmax classifier which is
both faster and more accurate than the previous SVM classifier.&lt;/p>
&lt;hr>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/fast-r-cnn/</description></item><item><title>A summary of Rich feature hierarchies for accurate object detection and semantic segmentation by Ross Girshick et al.</title><link>https://nsynovic.dev/summaries/rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/</link><pubDate>Thu, 10 Nov 2022 10:05:51 -0600</pubDate><guid>https://nsynovic.dev/summaries/rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/</guid><description>&lt;h1 id="a-summary-of-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation">A summary of &lt;em>Rich feature hierarchies for accurate object detection and semantic segmentation&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Ross Girshick et al. CVPR, 2014
&lt;a href="https://openaccess.thecvf.com/content_cvpr_2014/html/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation">A summary of &lt;em>Rich feature hierarchies for accurate object detection and semantic segmentation&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>This paper aims to achieve SOTA object detection by using a combination of
computer vision and deep learning techniques. This was chosen because object
detection on the PASCAL VOC 2012 dataset results have stagnated for the past two
years prior to publication.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this paper because the author&amp;rsquo;s propose an efficient method
for performing object detection while also being more accurate than previous
results. Furthermore, they introduce the usage of fine-tuning a larger CV model
on a domain specific task in order to achieve better performance.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is both an algorithms paper and a CV model review paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is related to works that talk about SOTA models, as well as papers
that talk about CV models for object detection.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contributions are a proposed algorithm for performing object
detection using DL based region proposals prior to CNN analysis and the usage of
fine-tuning a larger model on a domain specific task to improve accuracy/
performance on the task.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done in creating CNNs for the PASCAL VOC challenge, creating
algorithms for feature extraction for the purposes of object detection, creating
algorithms for region proposal (selective search was used for this paper), and
creating ensemble based DL models.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the figures and tables are easy to read and are clear.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is well written. It goes into great technical length about the
attributes of the model as well as how the model operates via an ablation study.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ol start="2">
&lt;li>N. Dalal and B. Triggs. &lt;em>Histograms of oriented gradients for human
detection&lt;/em>. In CVPR, 2005.&lt;/li>
&lt;li>B. Alexe, T. Deselaers, and V. Ferrari. &lt;em>Measuring the objectness of image
windows&lt;/em>. TPAMI, 2012.&lt;/li>
&lt;li>J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders. &lt;em>Selective search
for object recognition&lt;/em>. IJCV, 2013.&lt;/li>
&lt;li>I. Endres and D. Hoiem. &lt;em>Category independent object proposals&lt;/em>. In ECCV,
2010.&lt;/li>
&lt;li>J. Carreira and C. Sminchisescu. &lt;em>CPMC: Automatic object segmentation using
constrained parametric min-cuts&lt;/em>. TPAMI, 2012.&lt;/li>
&lt;li>P. Arbelaez, J. Pont-Tuset, J. Barron, F. Marques, and J. Malik. &lt;em>Multiscale
combinatorial grouping&lt;/em>. In CVPR, 2014.&lt;/li>
&lt;li>D. Ciresan, A. Giusti, L. Gambardella, and J. Schmidhuber. &lt;em>Mitosis
detection in breast cancer histology images with deep neural networks&lt;/em>. In
MICCAI, 2013.&lt;/li>
&lt;/ol>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors compared their model (R-CNN) with and without bounding boxes against
the previous SOTA models. Additionally, they conducted an ablation study on
their model to understand how it works internally.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors don&amp;rsquo;t seem to make any assumptions.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>There were no assumptions that I could find.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I know that this work is the predecessor to many works that expand and improve
upon the R-CNN algorithm presented here. Therefore, I&amp;rsquo;d like to read those
papers.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>What is the performance of other region proposal algorithms when used in the
R-CNN architecture?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>This is a solid paper. I appreciate the alibation study conducted to understand
the model.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Rich feature hierarchies for accurate object detection and semantic
segmentation&lt;/em> by Ross Girshick et al. [1] introduces and evaluates the Region
Convolutional Neural Network (R-CNN) on variations of the PASCAL VOC dataset for
both object classification and semantic segmentation. They compare this
algorithm against previous SOTA models on that dataset as well. Additionally,
they perform an alibation study on their R-CNN model to understand how it works.&lt;/p>
&lt;p>R-CNN works by using an ensemble approach. First, a DL model identifies regions
of interest of an image. These regions can number up into the tens of thousands.
These regions are then warped to fit a 224 x 224 pixel image. From this new
image, it is passed into a traditional CNN model for object detection or
semantic segmentation. By performing this region analysis first, the authors
reduce the amount of data that the CNN needs to process, and therefore increases
performance. Furthermore, the author&amp;rsquo;s pruned their R-CNN model and found that
94% of parameters could be dropped post-training.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation/</description></item><item><title>A summary of The Random Subspace Method for Constructing Decision Forests by Tin Kam Ho</title><link>https://nsynovic.dev/summaries/the-random-subspace-method-for-constructing-decision-forests/</link><pubDate>Wed, 09 Nov 2022 15:17:53 -0600</pubDate><guid>https://nsynovic.dev/summaries/the-random-subspace-method-for-constructing-decision-forests/</guid><description>&lt;h1 id="a-summary-of-the-random-subspace-method-for-constructing-decision-forests">A summary of &lt;em>The Random Subspace Method for Constructing Decision Forests&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Tin Kam Ho, IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
1998 &lt;a href="https://doi.org/10.1109/34.709601">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-the-random-subspace-method-for-constructing-decision-forests">A summary of &lt;em>The Random Subspace Method for Constructing Decision Forests&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>This paper addresses the problem of decision tree forest construction.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this paper as it compares eight forest construction
algorithms against the author&amp;rsquo;s algorithm on publicly available datasets. This
allows the reader to understand the pros and cons of using a particular
algorithm over another as well as validating the author&amp;rsquo;s claims. Furthermore,
this algorithm can monotonically increase in generalization accuracy while
preserving perfect accuracy.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is an algorithms paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is related to papers that present ways of constructing random
forests.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Her main contributions were:&lt;/p>
&lt;ul>
&lt;li>An efficient algorithm for generating decision trees&lt;/li>
&lt;li>A comparison of 8 forest construction algorithms on publicly available
datasets&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done before to describe what decision trees are, as well as how to
generate many of them for the purposes of classification.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the tables are clear and easy to read. However, all of the line charts
are difficult to read as each line is the same color in my copy of the paper.
Additionally, figure 1 is difficult to tell what is supposed to represented.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>I found this work hard to follow. I think that this is due to me not
understanding the problem domain, rather than her explanations.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ol start="2">
&lt;li>Y. Amit, D. Geman, and K. Wilder, &lt;em>Joint Induction of Shape Features and
Tree Classifiers&lt;/em>, IEEE Trans. Pattern Analysis and Machine Intelligence,
vol. 19, no. 11, pp. 1,300-1,305, Nov. 1997&lt;/li>
&lt;li>L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone, &lt;em>Classification and
Regression Trees&lt;/em>. Belmont, Calif.: Wadsworth, 1984&lt;/li>
&lt;li>D. Heath, S. Kasif, and S. Salzberg, &lt;em>Induction of Oblique Decision Trees&lt;/em>,
Proc. 13th Intl Joint Conf. Artificial Intelligence, vol. 2, pp.
1,002-1,007, Chambery, France, 28 Aug.-3 Sept. 1993.&lt;/li>
&lt;li>T.K. Ho, &lt;em>Random Decision Forests&lt;/em>, Proc. Third Intl Conf. Document
Analysis and Recognition, pp. 278-282, Montreal, Canada, 14-18 Aug. 1995.&lt;/li>
&lt;li>T.K. Ho, &lt;em>C4.5 Decision Forests&lt;/em>, Proc. 14th Intl Conf. Pattern
Recognition, Brisbane, Australia, 17-20 Aug. 1998.&lt;/li>
&lt;/ol>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The author compared the performance of different forest generation methods
against her own generation method. The different forest generation methods were:&lt;/p>
&lt;ul>
&lt;li>Single feature split with best gain ratio&lt;/li>
&lt;li>Distribution mapping&lt;/li>
&lt;li>Class centroids&lt;/li>
&lt;li>Unsupervised clustering&lt;/li>
&lt;li>Supervised clustering&lt;/li>
&lt;li>Central axis projection&lt;/li>
&lt;li>Perceptron&lt;/li>
&lt;li>Support Vector Machine&lt;/li>
&lt;/ul>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The author assumes that the reader has worked with decision trees prior to
reading.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>Yes.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d like to learn more about decision trees and compare them against Deep
Learning models.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>When would I ever use a decision tree over a SVM or DL model?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d appreciate the usage of color to separate different lines on the figures.
Additionally (and this could be due to the limited available citation), please
reduce the number of self-citations in future works.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>The Random Subspace Method for Constructing Decision Forests&lt;/em> by Tin
Kam Ho [1] discusses a method of generating many decision trees efficiently
without affecting accuracy. She validates this method by comparing it against
eight other forest construction methods, all on publicly available datasets. The
benefits of her work is that it is parallelized; meaning that with some tuning
to the algorithm, it can run on multiple CPU cores or threads (potentially even
faster on GPU cores).&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/the-random-subspace-method-for-constructing-decision-forests/</description></item><item><title>A summary of Deep Learning by Yann LeCun et al,</title><link>https://nsynovic.dev/summaries/deep-learning/</link><pubDate>Tue, 08 Nov 2022 12:55:10 -0600</pubDate><guid>https://nsynovic.dev/summaries/deep-learning/</guid><description>&lt;h1 id="a-summary-of-deep-learning">A summary of &lt;em>Deep Learning&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Yann LeCunn et al, Nature, 2015 &lt;a href="https://doi.org/10.1038/nature14539">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-deep-learning">A summary of &lt;em>Deep Learning&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>This paper discusses the usage of deep learning (DL) models and how they have
led to improvements in speech recognition, visual object recognition, object
detection, drug discovery, and genomics. It talks about how these models are
created, what type of models are typically applied to what domains, and the
usage of the backpropagation algorithm to train the model. Additionally, a
discussion about the usage of Recurrent Neural Networks (RNNs) and their
benefits is had.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this paper as it is a review of different DL techniques for
different problem domains.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is a literary review paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>It is closest related to papers that summarize a body of literature for the
purposes of understanding what the current SOTA techniques for a problem are.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contribution is a discussion of DL, its usages, RNNs, and a general
summary of the SOTA DL techniques for different problem domains.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done to develop DL and RNN techniques.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the figures are clear and easy to understand.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is well written.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ol start="2">
&lt;li>Krizhevsky, A., Sutskever, I. &amp;amp; Hinton, G. &lt;em>ImageNet classification with
deep convolutional neural networks.&lt;/em> In Proc. Advances in Neural Information
Processing Systems 25 10901098 (2012).&lt;/li>
&lt;li>Hinton, G. et al. &lt;em>Deep neural networks for acoustic modeling in speech&lt;/em>
recognition. IEEE Signal Processing Magazine 29, 8297 (2012).&lt;/li>
&lt;li>Sutskever, I. Vinyals, O. &amp;amp; Le. Q. V. &lt;em>Sequence to sequence learning with
neural&lt;/em> networks. In Proc. Advances in Neural Information Processing Systems
27 31043112 (2014)&lt;/li>
&lt;li>Glorot, X., Bordes, A. &amp;amp; Bengio. Y. &lt;em>Deep sparse rectifier neural networks.&lt;/em>
In Proc. 14th International Conference on Artificial Intelligence and
Statistics 315323 (2011).&lt;/li>
&lt;li>Hinton, G. E., Osindero, S. &amp;amp; Teh, Y.-W. &lt;em>A fast learning algorithm for deep
belief nets&lt;/em>. Neural Comp. 18, 15271554 (2006).&lt;/li>
&lt;li>Bengio, Y., Lamblin, P., Popovici, D. &amp;amp; Larochelle, H. &lt;em>Greedy layer-wise
training of deep networks.&lt;/em> In Proc. Advances in Neural Information
Processing Systems 19 153160 (2006).&lt;/li>
&lt;li>LeCun, Y. et al. &lt;em>Handwritten digit recognition with a back-propagation
network.&lt;/em> In Proc. Advances in Neural Information Processing Systems 396404
(1990).&lt;/li>
&lt;li>LeCun, Y., Bottou, L., Bengio, Y. &amp;amp; Haffner, P. G&lt;em>radient-based learning
applied to document recognition&lt;/em>. Proc. IEEE 86, 22782324 (1998).&lt;/li>
&lt;li>Hochreiter, S. &amp;amp; Schmidhuber, J. &lt;em>Long short-term memory&lt;/em>. Neural Comput. 9,
17351780 (1997).&lt;/li>
&lt;/ol>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The author&amp;rsquo;s of this paper reviewed literary sources for examples and usage of
DL and RNN techniques applied to different problem domains.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The author&amp;rsquo;s assume that unsupervised learning will become far more important in
the future than supervised learning.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>Potentially. Unsupervised learning presents problems and challenges not explored
in this paper, and is therefore treated as the next logical evolution of
techniques, rather than a series of unknowns and problems that need to be solved
first.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d like to implement the different DL techniques to the suggested problem
domains presented in this paper.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>Why wasn&amp;rsquo;t a discussion about Generative Adversarial Networks (GANs) not had in
this work? What are the performance differences of the presented loss functions?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>Overall, a pretty good paper. A follow up paper on unsupervised learning would
be nice to read.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The review paper &lt;em>Deep Learning&lt;/em> by Yann LeCun et al [1]. discusses the
advances and advantages of deep learning (DL) techniques made up to 2015. The
authors discuss what is DL, how and where it is applied, commercial and academic
usages of DL, the advantages of merging two different architectures together to
solve challenging tasks, and the usage of Recurrent Neural Networks (RNNs) for
handling natural language processing and speech recognition tasks. As their
paper is purely a listing of work that others have done prior to them, their
contributions were mostly the synthesis of such information into a digestible
document. With that said, each section of their work can be summarized, which is
what I have done here.&lt;/p>
&lt;p>DL allows for machine learning to surpass its previous limitations of having to
manually represent data in a suitable internal representation (through feature
extraction) by learning the representation itself. Current DL models are
typically trained using labeled datasets in what is known as supervised
learning. A sub-set of the data is used for training, which when ran through the
model, adjusts the hidden weights. These weights are adjusted using a technique
called stochastic gradient descent (SGD). SGD is accomplished by working
backwards through the model and taking the derivative of each weight which is
then used to adjust the hidden weights. Algorithms to do this include &lt;code>tanh(x)&lt;/code>
and &lt;code>ReLU&lt;/code>. &lt;code>ReLU&lt;/code> is the most popular algorithm for this task which is more
commonly known as backpropagation.&lt;/p>
&lt;p>Convolutional neural networks (ConvNets) are useful for analyzing data
structured as a series of multi-dimensional arrays. A typical application of
ConvNets are for analyzing images. RNNs are useful for analyzing data that is
dependent upon prior understanding. Chat bots, speech recognition, and answering
questions about data (i.e.,, where is a character in a book?) are all problems
that are reliant upon the model having some sort of &amp;ldquo;memory&amp;rdquo;. Memory solutions
include &lt;code>long short-term memory&lt;/code> which has been useful for accomplishing these
tasks.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/deep-learning/</description></item><item><title>A summary of Small World with High Risks: A Study of Security Threats in the npm Ecosystem by Markus Zimmermann et al.</title><link>https://nsynovic.dev/summaries/small-world-with-high-risks-a-study-of-security-threats-in-the-npm-ecosystem/</link><pubDate>Wed, 02 Nov 2022 22:49:58 -0500</pubDate><guid>https://nsynovic.dev/summaries/small-world-with-high-risks-a-study-of-security-threats-in-the-npm-ecosystem/</guid><description>&lt;h1 id="a-summary-of-small-world-with-high-risks-a-study-of-security-threats-in-the-npm-ecosystem">A summary of &lt;em>Small World with High Risks: A Study of Security Threats in the npm Ecosystem&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Markus Zimmermann et al. 28th USENIX Security Symposium; 2019
&lt;a href="https://www.usenix.org/conference/usenixsecurity19/presentation/zimmerman">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-small-world-with-high-risks-a-study-of-security-threats-in-the-npm-ecosystem">A summary of &lt;em>Small World with High Risks: A Study of Security Threats in the npm Ecosystem&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>This paper analyzes the security risks that the &lt;code>npm&lt;/code> package manager exposes
end users to directly and indirectly through dependency analysis.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>The 2016 &lt;code>left-pad&lt;/code> and 2018 &lt;code>eslint-scope&lt;/code> caused many dependent packages to
become exposed to security vulnerabilities after being taken down and
compromised respectfully.&lt;/p>
&lt;p>Additionally (and quoted from the paper):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-md" data-lang="md">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">-&lt;/span> Installing an average npm package introduces an implicit trust on 79
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>third-party packages and 39 maintainers, creating a surprisingly large attack
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>surface.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">-&lt;/span> Highly popular packages directly or indirectly influence many other packages
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>(often more than 100,000) and are thus potential targets for injecting malware.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">-&lt;/span> Some maintainers have an impact on hundreds of thousands of packages. As a
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>result, a very small number of compromised maintainer accounts suffices to
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>inject malware into the majority of all packages.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">-&lt;/span> The influence of individual packages and maintainers has been continuously
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>growing over the past few years, aggravating the risk of malware injection
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>attacks.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">-&lt;/span> A significant percentage (up to 40%) of all packages depend on code with at
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>least one publicly known vulnerability.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is a security paper with particular focus on security analysis of software
supply chains.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>Papers that analyze and quantify the risks to software hosting platforms/
software ecosystems. Additionally, papers that discuss the threat models of
software ecosystems are also related.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contributions can be found in &lt;a href="#motivation">Motivation&lt;/a>. More
generally, they show that &lt;code>npm&lt;/code> is small in that packages are tightly dependent
upon one another, and that a single security vulnerability is enough to
seriously cripple the functionality of the ecosystem. Furthermore, they analyze
the different threat models to &lt;code>npm&lt;/code>, as well as the role of maintainers with
respect to the wider ecosystem. In addition, they propose several different
mitigations for their proposed threat models. These include:&lt;/p>
&lt;ul>
&lt;li>a vetting process to create &amp;ldquo;trusted&amp;rdquo; maintainers&lt;/li>
&lt;li>a vetting process to analyze newly contributed code of specific packages&lt;/li>
&lt;/ul>
&lt;p>If both process were to be created for a single package, that package would be
considered to have, &amp;ldquo;perfect first-party security&amp;rdquo;. And if this was to be
extended to all transitive packages of that sole package, then it would be
considered to have &amp;ldquo;perfect third-party security&amp;rdquo; If both of the considerations
were to be met, then the package would be considered to be a &amp;ldquo;fully secured
package&amp;rdquo;.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done understanding the usage of &amp;ldquo;micro packages&amp;rdquo;, or packages that
accomplish a small functionality.&lt;/p>
&lt;p>Work has been done to understand the server and client security vulnerabilities
in JavaScript.&lt;/p>
&lt;p>Work has been done to understand software ecosystems and to raise questions that
need to be answered with respect to understanding the evolution of the
ecosystems.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the figures are clearly made, as well as well captioned.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is well written and dense. I do wonder if this paper could have been
broken up into potentially two smaller papers. But at the same time, if the
author&amp;rsquo;s were to do that, it might be hard to justify the overall contribution
of the work per paper.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>Revisiting software ecosystems research: A longitudinal literature study [2]&lt;/li>
&lt;li>Challenges in software ecosystems research [3]&lt;/li>
&lt;li>An ecosystem and socio-technical view on software maintenance and evolution
[4]&lt;/li>
&lt;li>A look at the dynamics of the JavaScript package ecosystem [5]&lt;/li>
&lt;li>Structure and evolution of package dependency networks [6]&lt;/li>
&lt;li>An empirical comparison of dependency network evolution in seven software
packaging ecosystems [7]&lt;/li>
&lt;li>The evolution of the R software ecosystem [8]&lt;/li>
&lt;li>The evolution of project inter-dependencies in a software ecosystem: The case
of Apache [9]&lt;/li>
&lt;li>Gentoo package dependencies over time [10]&lt;/li>
&lt;/ul>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The author&amp;rsquo;s used &lt;code>npm&lt;/code> package metadata from 2011 to April of 2018 to generate
several graphs of how packages are related to one another. Following this, they
then utilized graph metrics to measure the potential vulnerabilities &lt;code>npm&lt;/code> is
exposed to, as well as the actual reach of vulnerable packages within &lt;code>npm&lt;/code>.
Additionally, they utilized the package metadata to visualize and understand the
growth of &lt;code>npm&lt;/code> year over year. They utilized these metrics to understand how
potentially dangerous their proposed threat models are to engineers who use
&lt;code>npm&lt;/code>.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The author&amp;rsquo;s assume that all proposed threat models are of the same concern. For
some engineers, different models can be of different levels of concern.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>Yes, as this would have involved a survey of engineers to understand&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>While the study of &lt;code>npm&lt;/code> is useful as it is the world&amp;rsquo;s largest software package
ecosystem, I&amp;rsquo;d like to apply the metrics implemented in this work to
understanding PTM software ecosystems, such as Hugging Face and PyTorch Hub.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>Will the author&amp;rsquo;s perform a survey to understand if developers feel like the
proposed threat models are feasible?&lt;/p>
&lt;p>What is the &lt;code>npm&lt;/code> community&amp;rsquo;s opinion on reducing the number of micro packages
hosted on &lt;code>npm&lt;/code>?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>This work is very interesting and allows for easy expansion and exploration into
other software ecosystems. I suggest to make their graphs publicly available, as
well as to submit the graph to services such as Snyk so that they can further
analyze the data for security concerns (if they haven&amp;rsquo;t already).&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Small World with High Risks: A Study of Security Threats in the npm
Ecosystem&lt;/em> by Markus Zimmermann et al. [1] was a large scale study on &lt;code>npm&lt;/code>
packages and package dependencies taken from 2011 to April 2018. This study was
done to understand the various different threat models that exist on &lt;code>npm&lt;/code> as
well as to understand how &lt;code>npm&lt;/code> has evolved. By studying the evolution of &lt;code>npm&lt;/code>,
the author&amp;rsquo;s were able to analyze the growth of potentially vulnerable software
that can be affected by the proposed threat models. These threat models target
the underlying software package supply chain, and as &lt;code>npm&lt;/code> is considered to be a
small world (packages are tightly coupled to one another often resulting in long
chains), their are high risks involved when a single package is compromised, as
potentially countless more are affected by it.&lt;/p>
&lt;p>The author&amp;rsquo;s main contributions were (taken from the paper):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-md" data-lang="md">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">-&lt;/span> Installing an average npm package introduces an implicit trust on 79
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>third-party packages and 39 maintainers, creating a surprisingly large attack
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>surface.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">-&lt;/span> Highly popular packages directly or indirectly influence many other packages
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>(often more than 100,000) and are thus potential targets for injecting malware.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">-&lt;/span> Some maintainers have an impact on hundreds of thousands of packages. As a
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>result, a very small number of compromised maintainer accounts suffices to
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>inject malware into the majority of all packages.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">-&lt;/span> The influence of individual packages and maintainers has been continuously
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>growing over the past few years, aggravating the risk of malware injection
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>attacks.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#66d9ef">-&lt;/span> A significant percentage (up to 40%) of all packages depend on code with at
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>least one publicly known vulnerability.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In addition, they propose several different mitigations for their proposed
threat models. These include:&lt;/p>
&lt;ul>
&lt;li>a vetting process to create &amp;ldquo;trusted&amp;rdquo; maintainers&lt;/li>
&lt;li>a vetting process to analyze newly contributed code of specific packages&lt;/li>
&lt;/ul>
&lt;p>If both process were to be created for a single package, that package would be
considered to have, &amp;ldquo;perfect first-party security&amp;rdquo;. And if this was to be
extended to all transitive packages of that sole package, then it would be
considered to have &amp;ldquo;perfect third-party security&amp;rdquo; If both of the considerations
were to be met, then the package would be considered to be a &amp;ldquo;fully secured
package&amp;rdquo;.&lt;/p>
&lt;p>The threat models that the author&amp;rsquo;s identified were:&lt;/p>
&lt;ul>
&lt;li>Malicious packages&lt;/li>
&lt;li>Exploiting Unmaintained Legacy Code&lt;/li>
&lt;li>Package Takeover&lt;/li>
&lt;li>Account Takeover&lt;/li>
&lt;li>Collusion Attacks&lt;/li>
&lt;/ul>
&lt;p>They found that:&lt;/p>
&lt;ul>
&lt;li>The number of maintainers on &lt;code>npm&lt;/code> is growing significantly slower than the
number of released packages. In other words, maintainers are creating more and
more packages and are there by creating a larger and larger threat space for
an attacker to execute an Account or Package Takeover attack.&lt;/li>
&lt;li>That packages on &lt;code>npm&lt;/code> have a linear growth of direct dependencies, but a
super linear growth of transitive dependencies&lt;/li>
&lt;li>That the average package reach is growing at an exponential rate year over
year&lt;/li>
&lt;li>That there is growth in implicitly trusting maintainers&lt;/li>
&lt;li>That there is fairly linear growth in the number of unpatched advisories year
over year&lt;/li>
&lt;li>That the rate at which published vulnerabilities per 10,000 packages has been
rapidly increasing year over year.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/small-world-with-high-risks-a-study-of-security-threats-in-the-npm-ecosystem/</description></item><item><title>A summary of "What are Weak Links in the npm Supply Chain?", by Nusrat Zahan et al.</title><link>https://nsynovic.dev/summaries/what-are-weak-links-in-the-npm-supply-chain/</link><pubDate>Mon, 31 Oct 2022 09:50:49 -0500</pubDate><guid>https://nsynovic.dev/summaries/what-are-weak-links-in-the-npm-supply-chain/</guid><description>&lt;h1 id="a-summary-of-what-are-weak-links-in-the-npm-supply-chain">A summary of &lt;em>&amp;ldquo;What are Weak Links in the npm Supply Chain?&amp;rdquo;&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Nusrat Zahan et al.; Proceedings of ICSE-SEIP 2022;
&lt;a href="https://doi.org/10.1145/3510457.3513044">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-what-are-weak-links-in-the-npm-supply-chain">A summary of &lt;em>&amp;ldquo;What are Weak Links in the npm Supply Chain?&amp;rdquo;&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>Supply chain attacks stem from a data-driven approach where malicious actors
analyze signals in packages to identify weak links that can be exploited through
the insertion of malicious code. These attacks propagate through the supply
chain and affect unsuspecting users who rely on packages dependent upon the
compromised package.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Reports indicate that in 2021 supply chain attacks increased by 650%. An example
supply chain attack that crippled significant infrastructure was the SolarWinds
attack, which affected 425 Fortune 500 companies and several U.S. federal
agencies. Attacks on the supply chain stem from a data-driven approach, where
attackers analyze package metadata to identify which packages are most
vulnerable based on several signals, and then insert malicious code into the
package. This malicious code then propagates through the supply chain, thereby
affecting many unsuspecting downstream users. The more signals a package has,
the weaker it is considered to be within the supply chain - thereby earning the
name: &amp;ldquo;weak link&amp;rdquo;. Weak links exposes a package to a higher risk of a supply
chain attack and an attacker can exploit signals to execute a supply chain
attack.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is similar to a bug study in that it proposes classifications for
weak link signals. It also is a survey paper, as 470 &lt;code>npm&lt;/code> package maintainers
were interviewed to identify if weak link signals are of importance.
Additionally, this paper proposes a framework for identifying weak link packages
as well.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is similar to papers that study supply chain vulnerabilities, &lt;code>npm&lt;/code>
analysis papers, MSR papers, and developer survey papers.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors contributions are:&lt;/p>
&lt;ul>
&lt;li>A proposal of six weak link signals based on an empirical study of 1.63
million &lt;code>npm&lt;/code> packages (91% of all packages on &lt;code>npm&lt;/code> as of 2021)&lt;/li>
&lt;li>A survey of how &lt;code>npm&lt;/code> package maintainers perceive the proposed weak link
signals
&lt;ul>
&lt;li>Three of which were considered to be &lt;strong>strong&lt;/strong> signals by the maintainers&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Eight new signals suggested by the surveyed maintainers&lt;/li>
&lt;li>A framework to collect, categorize, and analyze package metadata in &lt;code>npm&lt;/code> to
evaluate weak link signals&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done to define supply chain attacks:&lt;/p>
&lt;ul>
&lt;li>Supply Chain Attack: &amp;ldquo;A supply chain attack is a cyber-attack that aims to
infect organizations and end-users by targeting less-secure components in the
supply chain&amp;rdquo;&lt;/li>
&lt;/ul>
&lt;p>Work has been done to define supply chain attacks:&lt;/p>
&lt;ul>
&lt;li>Malicious package release&lt;/li>
&lt;li>Social Engineering: Getting a maintainer to hand over sensitive information&lt;/li>
&lt;li>Account Takeover: Taking over an account to inject malicious code under the
maintainer&amp;rsquo;s name&lt;/li>
&lt;li>Ownership Transfer: Taking over an abandoned package&lt;/li>
&lt;li>Remote Execution: Taking over a package by compromising its dependencies&lt;/li>
&lt;/ul>
&lt;p>Work has been done to identify that supply chain attacks are a real threat, and
that no proposed framework is all-encompassing to prevent these attacks from
occurring.&lt;/p>
&lt;p>Work has been done to identify that the human element of package management
(maintainer and contributor information) is the most likely vector to attack.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The figures and charts are properly labeled, and are clear to read and
understand.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is clear to read, however, there are numerous grammatical errors that
are throughout the paper (punctuation and capitalization errors).&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>Does distributed development affect software quality? an empirical case study
of windows vista [2]&lt;/li>
&lt;li>Dont touch my code! Examining the effects of ownership on software quality
[3]&lt;/li>
&lt;li>SolarWinds attack explained: And why it was so hard to detect [4]&lt;/li>
&lt;li>The Hijacking of Perl.com [5]&lt;/li>
&lt;li>Towards Measuring Supply Chain Attacks on Package Managers for Interpreted
Languages [6]&lt;/li>
&lt;li>Detecting suspicious package updates [7]&lt;/li>
&lt;li>Anomalicious: Automated Detection of Anomalous and Potentially Malicious
Commits on GitHub [8]&lt;/li>
&lt;li>SolarWinds Orion Security Breach: A Shift In The Software Supply Chain
Paradigm [9]&lt;/li>
&lt;li>Compromised npm Package: event-stream [10]&lt;/li>
&lt;li>Secure at every step: What is software supply chain security and why does it
matter? [11]&lt;/li>
&lt;li>CCleaner Attack TimelineHeres How Hackers Infected 2.3 Million PCs [12]&lt;/li>
&lt;li>The Untold Story of NotPetya, the Most Devastating Cyberattack in History
[13]&lt;/li>
&lt;li>Secure open source collaboration: an empirical study of linus law [14]&lt;/li>
&lt;li>The State of Open Source Security [15]&lt;/li>
&lt;li>Backstabbers knife collection: A review of open source software supply chain
attacks [16]&lt;/li>
&lt;li>CII Best Practices Badge Program [17]&lt;/li>
&lt;li>Open Source Security Metrics [18]&lt;/li>
&lt;li>Security Scorecards for Open Source Projects [19]&lt;/li>
&lt;li>Small world with high risks: A study of security threats in the npm ecosystem
[20]&lt;/li>
&lt;/ul>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The author&amp;rsquo;s identified weak link signals by analyzing potential vulnerabilities
regarding the maintainer and contributor information in a &lt;code>npm&lt;/code> package&amp;rsquo;s
&lt;code>package.json&lt;/code> file. They then validated their weak links by measuring how
frequently they appear in the unique set of the join between the top 10,000 most
popular and the most frequently downloaded packages (with duplicates removed).
This resulted in 14,892 packages to analyze.&lt;/p>
&lt;p>Their proposed weak links were:&lt;/p>
&lt;ul>
&lt;li>Expired Maintainer Email Domain&lt;/li>
&lt;li>Package Installation Script&lt;/li>
&lt;li>Unmaintained Package&lt;/li>
&lt;li>Too Many Maintainers&lt;/li>
&lt;li>Too Many Contributors&lt;/li>
&lt;li>Overloaded Maintainer&lt;/li>
&lt;/ul>
&lt;p>Then they validated the usefulness of their weak links by surveying 470
maintainers about how useful the proposed weak links are as well as other
potential weak links to consider.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>All weak links proposed by the authors, except for the package installation
script, relied upon an assumption.&lt;/p>
&lt;p>The author&amp;rsquo;s assumed that an accounts with an expired email domain don&amp;rsquo;t have
2FA enabled in their analysis. Additionally, their measurement of expired email
domains is flawed as it relied upon checking if a domain was available for
purchase, and had no check to see if the domain was considered to be a
compromised domain.&lt;/p>
&lt;p>The author&amp;rsquo;s were unable to distinguish between a feature complete package and
an unmaintained package in their analysis.&lt;/p>
&lt;p>The author&amp;rsquo;s arbitrarily assigned a number to represent the max number of
maintainers and contributors to a repository. They also assumed the amount of
work that a maintainer can take on.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>These assumptions don&amp;rsquo;t seem valid to me.&lt;/p>
&lt;p>While being unable to check the 2FA status of an account makes sense, their
methodology for checking email domains was described to flawed and a manual
undertaking even by the author&amp;rsquo;s.&lt;/p>
&lt;p>Additionally, they mentioned that it was difficult to distinguish between a
feature complete and an unmaintained package.&lt;/p>
&lt;p>Finally, they were critiqued by their reviewers (to which I agree with on this
point) that assuming that too many maintainers or contributors is an incorrect
weak link as the nature of open source encourages collaboration and a &amp;ldquo;more the
merrier&amp;rdquo; approach to developing software.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d love to expand off of this work and see if the same issues exist within
pre-trained model supply chains. Additionally, the author&amp;rsquo;s propose further work
into the matter in &lt;em>Section 6: LIMITATIONS&lt;/em> that seems promising and
interesting.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>I am curious as to the ethics behind such a study when tooling is built to
analyze potential package vulnerabilities. In other words, is it ethical to
&lt;em>keep&lt;/em> tools to analyze for vulnerabilities, or should they be destroyed after
their intended usage?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>Please run the paper through a grammar checking service. Additionally,
understanding the nature and desired outcome of developing open-source software
could have helped when deciding on weak link criteria.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper, &lt;em>What are Weak Links in the npm Supply Chain?&lt;/em> by Nursat Zahan et al.
[1] was published in 2022 in the proceedings of the 44th International
Conference on Software Engineering: Software Engineering in Practice
(ICSE-SEIP). This paper discusses six potential signals that could identify a
package as a weak link within the &lt;code>npm&lt;/code> supply chain. Additionally, the paper
conducted a survey with 470 &lt;code>npm&lt;/code> maintainers about the proposed weak links to
understand their validity and to additionally find potential signals that could
also identify weak links.&lt;/p>
&lt;p>They drew upon industry experience and understanding when determining the six
weak links. Furthermore, these weak links relied upon maintainer and contributor
information derived from the &lt;code>package.json&lt;/code> file of the &lt;code>npm&lt;/code> packages. These
weak links were:&lt;/p>
&lt;ul>
&lt;li>Expired Maintainer Email Domain&lt;/li>
&lt;li>Package Installation Script&lt;/li>
&lt;li>Unmaintained Package&lt;/li>
&lt;li>Too Many Maintainers&lt;/li>
&lt;li>Too Many Contributors&lt;/li>
&lt;li>Overloaded Maintainer&lt;/li>
&lt;/ul>
&lt;p>The author&amp;rsquo;s performed a case study on 14,892 packages to identify the
prevalence of these weak links. They then validated the usefulness of these weak
links by surveying 470 contributors. They found that surveyors agreed with the
first three weak links, but not the remaining three as it went against the ethos
of open source development. They also proposed eight new weak links which were
not validated by the authors:&lt;/p>
&lt;ul>
&lt;li>Ownership Transfer&lt;/li>
&lt;li>Adding New Maintainers&lt;/li>
&lt;li>Maintainer Identity&lt;/li>
&lt;li>Maintainer Two-Factor Authentication&lt;/li>
&lt;li>No Source Code Repository&lt;/li>
&lt;li>&lt;code>npm&lt;/code> Package vs Source Code Repository&lt;/li>
&lt;li>CI/CD Pipeline&lt;/li>
&lt;li>Open Pull Request&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/what-are-weak-links-in-the-npm-supply-chain/</description></item><item><title>A summary of How Developers and Managers Define and Trade Productivity for Quality by Margaret-Anne Storey et al.</title><link>https://nsynovic.dev/summaries/how-developers-and-managers-define-and-trade-productivity-for-quality/</link><pubDate>Thu, 27 Oct 2022 16:03:42 -0500</pubDate><guid>https://nsynovic.dev/summaries/how-developers-and-managers-define-and-trade-productivity-for-quality/</guid><description>&lt;h1 id="a-summary-of-how-developers-and-managers-define-and-trade-productivity-for-quality">A summary of &lt;em>How Developers and Managers Define and Trade Productivity for Quality&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Margaret-Anne Storey et al.; &lt;a href="https://doi.org/10.1145/3528579.3529177">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-how-developers-and-managers-define-and-trade-productivity-for-quality">A summary of &lt;em>How Developers and Managers Define and Trade Productivity for Quality&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>This paper aims to understand the issue of developers and managers having
differing views of productivity, and when to trade the quality of the product
for more productivity. Additionally, this calls into question what is quality,
as well as how does one measure both of these attributes.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this paper as it provides a case study conducted with
Microsoft developers and managers about how they measure and value productivity.
Thereby allowing establishing what a sample of developers define productivity
as, and what a sample of managers define it as well. Additionally, the authors
propose utilize their existing framework SPACE to codify developer and manager
responses. They also propose a new framework, TRUCE, designed to help developers
and managers make decisions about software quality vs productivity trade-offs.
These frameworks are related but provide different lenses into software
development.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is a survey paper of practitioners in industry.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is closely related to industry metric usage survey papers,
productivity and quality papers, and - more broadly - papers that discuss the
usage of and of software metrics in teams.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their contributions is a survey of what developers and managers at Microsoft
consider to be productivity and quality, as well as the TRUCE framework for
identifying when productivity should be comprimised for quality.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done into understanding what software quality is, how to measure
productivity, and what is productivity. Also, the authors have previously
described a framework called SPACE (Satisfaction, Performance, Activity,
Collaboration, and Efficiency) which was used to codify the responses from the
survey participants.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the figures are labeled properly, easy to understand, and have clear
captions.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is well written. However, I&amp;rsquo;m not a fan with how the abstract was
structured. I found the topic-description approach didn&amp;rsquo;t engage me as a reader.
But that is more of a personal opinion than an objective fact.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://doi.org/10.1145/3454122.3454124">The SPACE of Developer Productivity: Theres More to It than You Think&lt;/a>
[2]&lt;/li>
&lt;li>&lt;a href="https://doi.org/10.1109/TSE.2018.2842201">Motivation and satisfaction of software engineers&lt;/a>
[3]&lt;/li>
&lt;li>&lt;a href="https://doi.org/10.1007/978-1-4842-4221-6_3">Why We Should Not Measure Productivity&lt;/a>
[4]&lt;/li>
&lt;li>&lt;a href="https://www.microsoft.com/en-us/research/publication/appendix-to-productivity-quality-alignment">Appendix to How Developers and Managers Define and Trade Off Productivity and Quality&lt;/a>
[5]&lt;/li>
&lt;li>&lt;a href="https://doi.org/10.1109/TSE.2019.2944354">Towards a Theory of Software Developer Job Satisfaction and Perceived Productivity&lt;/a>
[6]&lt;/li>
&lt;/ul>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors conducted a survey of Microsoft developers and managers on how they
define productivity and quality, as well as the trade offs between productivity
and quality. 167 responses were collected, with 131 responses being from
developers and 34 from managers. Responses were codified using the SPACE
methodology proposed by the authors in a previous work.&lt;/p>
&lt;p>Comparisons were made between how:&lt;/p>
&lt;ul>
&lt;li>Developers define productivity&lt;/li>
&lt;li>Managers define team productivity&lt;/li>
&lt;li>Developers define quality&lt;/li>
&lt;li>Managers define team quality&lt;/li>
&lt;li>How developers &lt;em>think&lt;/em> managers define team productivity&lt;/li>
&lt;li>How managers &lt;em>think&lt;/em> developers define productivity&lt;/li>
&lt;li>Do developers and managers trade quality for productivity?&lt;/li>
&lt;/ul>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>They propose the TRUCE framework for software quality (Timeliness, Robustness,
User Needs, Collaboration Needs, and Evolvable) based on this study alone.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>I don&amp;rsquo;t think that TRUCE can stand on its own just on this work. Additional
surveys and work need to be done to validate the usefulness and how applicable
this framework is outside of the subset of developers and managers at Microsoft
that responded to the survey. The authors do address this in their paper.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d love to perform a literary review of related works and surveys to identify
if the TRUCE framework is applicable. Additionally, I&amp;rsquo;d like to perform a follow
up study of answering and analyzing the question as to what developers &lt;em>think&lt;/em>
managers consider to be quality work and vice versa.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>What prevented the authors from asking about what developers and managers
&lt;em>think&lt;/em> the other considers quality work?&lt;/p>
&lt;p>Can any of the SPACE or TRUCE definitions be quantified automatically by
analyzing feature requests?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>I appreciate the clear figures, charts, and tables. I don&amp;rsquo;t like the style of
the abstract as it doesn&amp;rsquo;t engage me as a reader. But the finding boxes that
summarize the survey results per subsection were a nice touch and are
appreciated.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>How Developers and Managers Define and Trade Productivity for
Quality&lt;/em> by Margaret-Anne Storey et al. [1] presents the results of a survey
conducted on 131 Microsoft developers and 34 managers about what they consider
to be productive work, quality work, what the other cohort defines each topic,
and if they have ever made explicit trade offs between productivity and quality.
The authors took the responses and codified them using the SPACE framework that
they proposed in an earlier paper [2].&lt;/p>
&lt;p>They found that developers tend to define productive work as activities (number
of tasks completed or iterations; 50%), efficiency and flow (entering a flow
state; 38%), and productivity (delivering on projects; 35%). Managers tend to
define productive work across the team in productivity (67%), efficiency and
flow (45%), and collaboration (working with others to brainstorm ideas/
providing feedback; 33%).&lt;/p>
&lt;p>However, developers &lt;em>think&lt;/em> managers define team productivity in activity (53%),
productivity (37%), and collaboration (19%). Whereas managers &lt;em>think&lt;/em> developers
define productivity in activity (52%), efficiency and flow (42%), and
productivity (24%). Here, both managers and developers &lt;em>think&lt;/em> the other defines
productivity than what is actually true.&lt;/p>
&lt;p>The take away with productivity, is that all participants defined productivity
under the SPACE framework.&lt;/p>
&lt;p>Quality was defined under the proposed TRUCE (Timeliness, Robustness, User
Needs, Collaboration, Evolution) framework. Developers and managers both define
quality similarly as robustness (71%, 88%), evolution (44%, 33%), and user need
(38%, 39%). Managers rank user need higher than evolution, where as developers
disagree with them.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/how-developers-and-managers-define-and-trade-productivity-for-quality/</description></item><item><title>A summary of Robust Real Time-Face Detection by P. Viola and M.J. Jones</title><link>https://nsynovic.dev/summaries/robust-real-time-face-detection/</link><pubDate>Mon, 24 Oct 2022 19:29:57 -0500</pubDate><guid>https://nsynovic.dev/summaries/robust-real-time-face-detection/</guid><description>&lt;h1 id="a-summary-of-robust-real-time-face-detection">A summary of &lt;em>Robust Real-Time Face Detection&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>P. Viola and M.J. Jones;
&lt;a href="https://doi.org/10.1023/B:VISI.0000013087.49260.fb">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-robust-real-time-face-detection">A summary of &lt;em>Robust Real-Time Face Detection&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Robust Real-Time Face Detection&lt;/em> by P. Viola and M.J. Jones [1]
presents a new methodology for efficiently performing face detection. They due
this through the usage of an integral image which is able to reduce the
computational complexity to constant time (O(1)) of analyzing an image as it
doesn&amp;rsquo;t rely on scale invariance and thus an image pyramid. Additional, the
classifier that they build is &amp;ldquo;simple and efficient&amp;rdquo; and allows for the engineer
to specify a large number of features to be analyzed without compromising on
performance as it relies upon the Ada Boost algorithm to select important
features. Furthermore, the authors propose a method for building a cascade of
classifiers which further reduces computation time as each classifier specifies
. Finally, they propose experiments that can be ran on face detection data sets
to conduct supervised learning.&lt;/p>
&lt;p>While this paper does propose many new and innovative ideas, the paper
originates from 2003.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is a research paper focusing on improving the Computer Vision task of face
detection without the reliance of CNNs.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is most closely related to non-CNN face detection papers.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>They create an integral image which is able to reduce the computational
complexity to constant time (O(1)) of analyzing an image as it doesn&amp;rsquo;t rely on
scale invariance and thus an image pyramid. Additionally, the classifier that
they build is &amp;ldquo;simple and efficient&amp;rdquo; and allows for the engineer to specify a
large number of features to be analyzed without compromising on performance as
it relies upon the Ada Boost algorithm to select important features.
Furthermore, the authors propose a method for building a cascade of classifiers
which further reduces computation time as each classifier specifies . Finally,
they propose experiments that can be ran on face detection data sets to conduct
supervised learning.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Prior work has been done in creating face detection systems. Prior work has been
done in creating the Ada Boost algorithm that is used to create a cascade of
classifiers. Prior work has been done in identifying methodologies to create
image features.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this paper as it presents a non-CNN methodology for
reliably identifying faces in images. Additionally, the authors also present a
methodology for doing this task efficiently on low end hardware.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The figures, diagrams, and graphs are well explained and designed.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>Yes, however a bit lengthy. Optimizations could have been made with respect to
reducing the amount of content describing the background to the Ada Boost
algorithm.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>A Decision-Theoretic Generalization of On-Line Learning and an Application to
Boosting [2]&lt;/li>
&lt;/ul>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d like to implement their work on a low powered device and compare it to a
newer CNN model on ML metrics.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Robust Real-Time Face Detection&lt;/em> by P. Viola and M.J. Jones [1]
presents a new methodology for efficiently performing face detection. They due
this through the usage of an integral image which is able to reduce the
computational complexity to constant time (O(1)) of analyzing an image as it
doesn&amp;rsquo;t rely on scale invariance and thus an image pyramid. Additionally, the
classifier that they build is &amp;ldquo;simple and efficient&amp;rdquo; and allows for the engineer
to specify a large number of features to be analyzed without compromising on
performance as it relies upon the Ada Boost algorithm to select important
features. Furthermore, the authors propose a method for building a cascade of
classifiers which further reduces computation time as each classifier specifies
. Finally, they propose experiments that can be ran on face detection data sets
to conduct supervised learning.&lt;/p>
&lt;p>The main &amp;ldquo;wow&amp;rdquo; factor of this work is that it was built on a low powered system.
This same application could be more performant on modern smartphones in
comparison to the system that it was originally tested on.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/robust-real-time-face-detection/</description></item><item><title>A summary of Learning Deep Features for Discriminative Localization by Bolei Zhou et al.</title><link>https://nsynovic.dev/summaries/learning-deep-features-for-discriminative-localization/</link><pubDate>Mon, 24 Oct 2022 14:44:26 -0500</pubDate><guid>https://nsynovic.dev/summaries/learning-deep-features-for-discriminative-localization/</guid><description>&lt;h1 id="a-summary-of-learning-deep-features-for-discriminative-localization">A summary of &lt;em>Learning Deep Features for Discriminative Localization&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Bolei Zhou et al.; &lt;a href="http://arxiv.org/abs/1512.04150">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-learning-deep-features-for-discriminative-localization">A summary of &lt;em>Learning Deep Features for Discriminative Localization&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Learning Deep Features for Discriminative Localization&lt;/em> by Bolei Zhou
et al. [1] describes using the global average pooling layer of CNNs to not
only regularize data, but also to localize objects in an image &lt;strong>even if the
network wasn&amp;rsquo;t trained for object detection&lt;/strong>. The authors propose a method for
object localization that involves a simple modification to the layer to generate
what they call &amp;ldquo;class activation maps&amp;rdquo; (CAMs), which are heat maps of where the
CNN is &amp;ldquo;looking&amp;rdquo; at an image for labeling. The hotter the heat map, the more
focus the CNN is putting on that specific image region.&lt;/p>
&lt;p>The authors go into detail as to how one would accomplish this with a
weakly-supervised object localization method, and its applications towards deep
features for generic localization, fine-grained recognition, and pattern
discovery. They conclude with visualizing class specific units.&lt;/p>
&lt;p>Their technique accomplishes object localization in a single forward pass on
existing CNN models that utilize a global average pooling layer.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is a CNN understanding and technique paper. It discusses a method for
understanding what a CNN is looking at as well as expanding the usage of image
classifiers for object localization.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is related to works involving object localization, image
classification, CNNs, and Deep Learning papers.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The author&amp;rsquo;s main contribution is a method for modifying the global average
pooling layer in CNNs to perform object localization in a single forward pass.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>There has been work done in utilizing weakly-supervised learning to perform
object localization. However, these works either don&amp;rsquo;t evaluate the object
localization task, or utilize multiple passes to perform the task.&lt;/p>
&lt;p>There has been numerous work that has gone into visualizing what occurs within a
CNN. Additionally, there has been work that has looked at the global &lt;em>max&lt;/em>
pooling layer, however, this work is the first to utilize the global &lt;em>average&lt;/em>
layer.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this paper as it provides a methodology of utilizing
existing CNNs trained on image classification to perform object localization
tasks &amp;ldquo;for free&amp;rdquo;. In other words, this paper presents a methodology for object
localization by reusing existing SOTA CNNs.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the figures and tables are labeled clearly, have detailed captions, and
make sense with respect to the paper.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is well written.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>Self-taught object localization with deep networks [2]&lt;/li>
&lt;li>Weakly supervised object localization with multi-fold multiple instance
learning [3]&lt;/li>
&lt;li>Learning and transferring mid-level image representations using convolutional
neural networks [4]&lt;/li>
&lt;li>Is object localization for free? weakly-supervised learning with convolutional
neural networks [5]&lt;/li>
&lt;li>Visualizing and understanding convolutional networks [6]&lt;/li>
&lt;li>Object detectors emerge in deep scene CNNs [7]&lt;/li>
&lt;li>Network in network [8]&lt;/li>
&lt;li>Going deeper with convolutions [9]&lt;/li>
&lt;/ul>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I would love to take this work and apply it to my current research in low
powered computer vision. By utilizing larger networks to localize where in a
static scene the object of interest is most likely to be in (for example, a
static video of a bird sitting on a wire), I can pass in this mapping into a CNN
to specifically be interested in that region of the video/ image. Additionally,
by figuring out where a larger CNN is localizing data, I can then mask out any
cold area of the image prior to analysis by a smaller CNN.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Learning Deep Features for Discriminative Localization&lt;/em> by Bolei Zhou
et al. [1] discusses a weakly supervised method of performing object
localization on existing CNN models. Their method involves replacing the fully
connected layer at the end of a CNN performing image classification, with a
global average pooling layer into a Softmax layer. This is so that the models
original functionality is not cut from the new model. However, the global
average pooling layer is modified so that a heat map can be extracted focusing
on what the CNN is focusing on prior to labeling the image.&lt;/p>
&lt;p>Previous work involved the usage of weakly supervised CNNs, but relied on global
max pooling. Additional work utilized deconvolutional layers to perform a
similar task.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/learning-deep-features-for-discriminative-localization/</description></item><item><title>A summary of How to implement SVMs by John Platt</title><link>https://nsynovic.dev/summaries/how-to-implement-svms/</link><pubDate>Mon, 24 Oct 2022 13:46:36 -0500</pubDate><guid>https://nsynovic.dev/summaries/how-to-implement-svms/</guid><description>&lt;h1 id="a-summary-of-how-to-implement-svms">A summary of &lt;em>How to implement SVMs&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>John Platt; &lt;a href="https://doi.ieeecomputersociety.org/10.1109/5254.708428">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-how-to-implement-svms">A summary of &lt;em>How to implement SVMs&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The essay &lt;em>How to implement SVMs&lt;/em> by John Platt (as part of the larger &lt;em>Support
vector machine&lt;/em> collection of essays in the July/ August edition of the 1998
IEEE Intelligent Systems magazine) [1] discusses how to implement a Support
Vector Machine (SVM). This essay goes into great detail on implementation
strategies for handling larger data sets, as well as methods for training SVMs.
Topics include understanding the Quadratic Problem (what SVMs aim to solve),
sequential minimal optimization (reaching a global minimal value), and where to
find SVM implementations.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This essay seems to be a tutorial/ workshop paper about SVMs.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>I would expect papers that are about implementing SVMs from scratch would be
related to this essay.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contributions are an understanding of how SVMs work as well as how to
implement them efficiently.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has already been done on experimenting optimal SVM algorithms and
minimization functions.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this paper as it provides an understanding of what a SVM is
and how they function.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The figure and charts have proper labels and captions that explain what they are
representing.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>For the most part, yes. However, the essay expects the reader to be
knowledgeable about SVMs prior to reading the essay. This is shown mostly
through the usage of mathematical notation specific to the problem domain, and
linking to other work to explain it. While this is a short essay for a magazine,
a brief sentence or two about the notation would have been appreciated.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>A Tutorial on Support Vector Machines for Pattern Recognition [2]&lt;/li>
&lt;/ul>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The author assumes that the reader, should they implement their own SVM
algorithm, will be using a commercial numerical analysis package.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>Without understanding the nature of the numerical analysis packages of 1998, I
would assume that this assumption is correct. I base this on that the author
mentions that free numerical analysis packages (not if they were open sourced or
not) run slower than commercial packages and may have errors due to precision
mistakes.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;m not interested in creating my own SVM algorithm. However, having a better
understanding of how SVMs work as well as the different minimization functions
that they implement, would be nice to know.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The essay &lt;em>How to implement SVMs&lt;/em> by John Platt (as part of the larger &lt;em>Support
vector machine&lt;/em> collection of essays in the July/ August edition of the 1998
IEEE Intelligent Systems magazine) [1] discusses how to implement a Support
Vector Machine (SVM). The author goes into detail about what an SVM is trying to
accomplish (minimize a quadratic problem on a high dimensional matrix), what
techniques exist to solve this problem, as well as available programs to allow
for researchers to utilize SVMs in their work.&lt;/p>
&lt;p>Overall, the essay does a good job of explaining the problem space as well as
implementation details, however, the essay is very much a product of its time.
There is less of a need to develop new SVM algorithms as there are many that are
provided off of the shelf in free and open source numerical analysis packages
[3] [4]. Additionally, the suggestion that readers should purchase a
numerical analysis package to create their own SVM is dated in my opinion, as
again, there are many free options available [5].&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/how-to-implement-svms/</description></item><item><title>A summary of Applying SVMs to Face Detection by Edgar Osuna</title><link>https://nsynovic.dev/summaries/applying-svms-to-face-detection/</link><pubDate>Mon, 24 Oct 2022 09:20:40 -0500</pubDate><guid>https://nsynovic.dev/summaries/applying-svms-to-face-detection/</guid><description>&lt;h1 id="a-summary-of-applying-svms-to-face-detection">A summary of &lt;em>Applying SVMs to Face Detection&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Edgar Isuna; &lt;a href="https://doi.ieeecomputersociety.org/10.1109/5254.708428">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-applying-svms-to-face-detection">A summary of &lt;em>Applying SVMs to Face Detection&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The essay &lt;em>Applying SVMs to Face Detection&lt;/em> by Edgar Osuna (as part of the
larger &lt;em>Support vector machine&lt;/em> collection of essays in the July/ August edition
of the 1998 IEEE Intelligent Systems magazine) [1] describes the usage of
Support Vector Machines (SVMs) to identify faces in static images and real time
systems. The work goes into detail about previous systems that attempted this
task, as well as a real time system that can classify images at 4 to 5 frames
per second.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is both a small systems essay, as well as a CV task analysis of the
state of the art when using this particular technique.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This essay is most similar to papers that discuss systems that implement face
detection.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The author&amp;rsquo;s main contributions are a system that utilizes SVMs for real time
facial detection. Additionally, their contributions include a discuss of
previous systems that attempted this task.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Quote from the &lt;em>Previous systems&lt;/em> section of the paper:&lt;/p>
&lt;p>&amp;ldquo;Researchers have approached the face-detection problem with different
techniques in the last few years, including neural networks [2] [3],
detection of face features and use of geometrical constraints [4], density
estimation of the training data [5], labeled graphs [6], and clustering and
distribution-based modeling [7] [8].&amp;rdquo;&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this essay as it proposes an SVM based solution for both
static image and real time face detection.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The figures are clear and explained well through their captions. However, Table
2 uses a metric called &amp;ldquo;False Alarms&amp;rdquo; to measure the number of times the system
reported a &amp;ldquo;face&amp;rdquo; that wasn&amp;rsquo;t a face. A more appropriate metric, such as recall,
would have been appropriate in this case.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is well written, however, it can be improved upon. The biggest
complaint that I have is the usage of bullet points to describe tasks/ steps
that were taken to complete a task. Additionally, many bullet points contained
more than one sentence. I find it to be more appropriate for papers to utilize
bullet points for short, unordered lists. Most appropriately used when listing
off different techniques or definitions, which this essay does utilize. Aside
from that, the individual steps are written well and clearly, and seem to be
fairly reproducible.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>Detection and localization of faces on digital images [2]&lt;/li>
&lt;li>Human Face Detection in Visual Scenes [3]&lt;/li>
&lt;li>Human face detection in a complex background [4]&lt;/li>
&lt;li>Probabilistic visual learning for object detection [5]&lt;/li>
&lt;li>Determination of face position and pose with a learned representation based on
labeled graphs [6]&lt;/li>
&lt;li>Learning and Example Selection for Object and Pattern Detection [7]&lt;/li>
&lt;li>Example-based learning for view-based human face detection [8]&lt;/li>
&lt;/ul>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors trained their system to identify vertically oriented, gray-scale
images of faces for their static image face detector. They make no mention as to
whether this detector is capable of identifying faces in off axis positions, or
if their system is capable enough to orient faces properly.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>Without understanding the availability of data sets at the time, this seems like
a valid assumption to make. However, simple data augmentation (such as rotating
the image) could&amp;rsquo;ve been done to increase the number of training examples of
faces not in the vertical orientation.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>A re-implementation of their work, both on static images and real time image
capture, would be interesting to perform on devices such as cameras, Raspberry
Pis, or other low powered systems. Additionally, comparing the power draw
between an SVM based solution and one that is powered by DL would be interesting
as well.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The essay &lt;em>Applying SVMS to Face Detection&lt;/em> by Edgar Osuna (as part of the
larger &lt;em>Support vector machine&lt;/em> collection of essays in the July/ August edition
of the 1998 IEEE Intelligent Systems magazine) [1] describes the usage of
Support Vector Machines (SVMs) to identify faces in static images and real time
systems. The author goes into detail about existing systems that were powered by
non-SVM techniques, as well as presenting their own system (for both static
image and real time image capture) for face detection.&lt;/p>
&lt;p>Their static image system only works on gray scale images of vertically aligned
faces. Additionally, they used a small data set to train the SVM. In doing so,
they limit the usage of the static image system to that specific domain, as well
as potentially creating a system that is unable to detect a face in all
potential cases (such as different ethnicity, lighting conditions, face
orientations, etc.).&lt;/p>
&lt;p>Their real time image capture system works on full color images of vertically
aligned faces by using a combination of a skin detector and a &amp;ldquo;primitive&amp;rdquo; motion
detector. This system was capable of recognizing faces at 4 to 5 frames per
second.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/applying-svms-to-face-detection/</description></item><item><title>A summary of Using SVMs For Text Categorization by Susan Dumais et al</title><link>https://nsynovic.dev/summaries/using-svms-for-text-categorization/</link><pubDate>Sun, 23 Oct 2022 16:45:32 -0500</pubDate><guid>https://nsynovic.dev/summaries/using-svms-for-text-categorization/</guid><description>&lt;h1 id="a-summary-of-using-svms-for-text-categorization">A summary of &lt;em>Using SVMs For Text Categorization&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Susan Dumais et al.;
&lt;a href="https://doi.ieeecomputersociety.org/10.1109/5254.708428">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-using-svms-for-text-categorization">A summary of &lt;em>Using SVMs For Text Categorization&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The essay &lt;em>Using SVMs For Text Categorization&lt;/em> by Susan Dumais et al. (as part
of the larger &lt;em>Support vector machine&lt;/em> collection of essays in the July/ August
edition of the 1998 IEEE Intelligent Systems magazine) [1] provides examples
of when using a Support Vector Machine (SVM) is beneficial with respect to text
classification. They discuss text classification, text representation and
feature selection, and an example use case on the Reuters collection. They
support the position that using SVMs for text classification (or really any
algorithm so long as it isn&amp;rsquo;t run by a human) is beneficial for this task.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This essay is more argumentative and position oriented.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This essay would most likely be classified alongside similar works that
evaluated the usefulness of SVMs with respect to human tasks.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their contributions is an analysis of SVMs for text classification.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done to understand what SVMs are, as well as use cases for SVMs.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Because it provides a case study of using SVMs on the Reuters collection with
respect to text classification.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the graphs and charts are clear to understand and have properly labeled
axis.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>This essay is clearly written.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>Introduction to Modern Information Retrieval [2]&lt;/li>
&lt;li>Fast Training of SVMs Using Sequential Minimal Optimization [3]&lt;/li>
&lt;/ul>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I would like to implement their study using the five different learning
algorithms they utilized to validate their results. The algorithms in question
are: Findsim, Naive Bayes, BayesNets, Trees, and LinearSVM.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The essay &lt;em>Using SVMs For Text Categorization&lt;/em> by Susan Dumais et al. (as part
of the larger &lt;em>Support vector machine&lt;/em> collection of essays in the July/ August
edition of the 1998 IEEE Intelligent Systems magazine) [1] presents the usage
of SVMs for text categorization on the Reuters collection in comparison to other
classification algorithms. They found that SVMs perform best on this
classification task.&lt;/p>
&lt;p>The greater reason for this essay is to encourage engineers to use learning
algorithms for human intensive tasks - such as text classification.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/using-svms-for-text-categorization/</description></item><item><title>A summary of SVMs - A Practical Consequence of Learning Theory by Bernhard Scholkopf</title><link>https://nsynovic.dev/summaries/svms-a-practical-consequence-of-learning-theory/</link><pubDate>Sun, 23 Oct 2022 10:02:41 -0500</pubDate><guid>https://nsynovic.dev/summaries/svms-a-practical-consequence-of-learning-theory/</guid><description>&lt;h1 id="a-summary-of-svms-a-practical-consequence-of-learning-theory">A summary of &lt;em>SVMs: A Practical Consequence of Learning Theory&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Bernhard Scholkopf;
&lt;a href="https://doi.ieeecomputersociety.org/10.1109/5254.708428">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-svms-a-practical-consequence-of-learning-theory">A summary of &lt;em>SVMs: A Practical Consequence of Learning Theory&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The essay &lt;em>SVMs - a practical consequence of learning theory&lt;/em> by Bernhard
Scholkopf (as part of the larger &lt;em>Support vector machine&lt;/em> collection of essays
in the July/ August edition of the 1998 IEEE Intelligent Systems magazine) [1]
discusses the underlying theory that powers Support Vector Machine (SVM)
algorithms and argues that these algorithms are useful and performant. His essay
contains sections on &lt;em>Learning pattern recognition from examples&lt;/em>,
&lt;em>Hyperplanes&lt;/em>, &lt;em>Feature spaces and kernels&lt;/em>, &lt;em>SVMs&lt;/em>, and &lt;em>Current developments
and open issues&lt;/em> which indicates an essay that will holistically look at SVMs,
rather than a particular facet of them.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is best classified as an informative essay on the benefits of SVMs
from a theoretical and practical view.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is most related to other papers within the magazine&amp;rsquo;s collection, as
well as work that goes into the theory behind SVMs.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>A brief description of the theory that powers SVMs, as well as identifying where
SVMs are practical.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done to develop and implement the SVM algorithm.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Because it provides a concise description of the theory that powers SVMs, and
practical usages of SVMs.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The paper doesn&amp;rsquo;t provide and graphs or charts. However, the figures and
diagrams that are presented are clearly explained in the descriptions, are well
made, and are easy to comprehend.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>Sort of? The theory components of the essay are written distinctly differently
than the introduction and concluding sections of the paper. This could be due to
the discussion of mathematical prose; but due to this, the essay has two
different voices.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>The Nature of Statistical Learning Theory [2]&lt;/li>
&lt;/ul>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d like to explore the usage of SVMs for face or object detection and compare
it against the usage of DL techniques on both traditional and low-powered
metrics.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The essay &lt;em>SVMs: A Practical Consequence of Learning Theory&lt;/em> by Bernhard
Scholkopf (as part of the larger &lt;em>Support vector machine&lt;/em> collection of essays
in the July/ August edition of the 1998 IEEE Intelligent Systems magazine) [1]
discuss both the mathematical theory and current practice of using SVMs. SVMs
are useful in a research aspect as their functionality can be mathematically
explained. SVMs are a linear classifier that operate in multi-dimensional space
through the usage of a hyper plane. Hyper planes are chosen by finding support
vectors, which are instances of a class that are closest to one another. The
hyper plane then splits these two instances into two separable sides. To assist
in this calculation, a kernel algorithm is applied to map one multi-dimensional
space to another for easier computation.&lt;/p>
&lt;p>Overall, this paper provides a good understanding of the theory behind SVMs. It
also alludes to additional usages of SVMs and their current problems, but it is
not focused on discussing or resolving them.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/svms-a-practical-consequence-of-learning-theory/</description></item><item><title>A summary of Runnemede: An Architecture for Ubiquitous High-Performance Computing by Nicholas P. Carter et al.</title><link>https://nsynovic.dev/summaries/runnemede-an-architecture-for-ubiquitous-high-performance-computing/</link><pubDate>Fri, 30 Sep 2022 09:07:41 -0500</pubDate><guid>https://nsynovic.dev/summaries/runnemede-an-architecture-for-ubiquitous-high-performance-computing/</guid><description>&lt;h1 id="a-summary-of-runnemede-an-architecture-for-ubiquitous-high-performance-computing">A summary of &lt;em>Runnemede: An Architecture for Ubiquitous High-Performance Computing&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Nicholas P. Carter et al;
&lt;a href="https://doi.org/10.1109/HPCA.2013.6522319">https://doi.org/10.1109/HPCA.2013.6522319&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-runnemede-an-architecture-for-ubiquitous-high-performance-computing">A summary of &lt;em>Runnemede: An Architecture for Ubiquitous High-Performance Computing&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Runnemede: An Architecture for Ubiquitous High-Performance Computing&lt;/em>
by Nicholas P. Carter et al. [1] describes the Runnemede high performance
computing architecture targeting extreme-scale systems. This architecture was
developed for the DARPA&amp;rsquo;s Ubiquitous High-Performance Computing program. The
authors describe multiple facets of the architecture including the networking,
hardware and software design, the energy efficiencies of the architecture. They
also evaluate the performance of the architecture as well. Their many
contributions are a theoretical architecture that is well optimized for energy
efficiency on extra-scale computers.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is a theoretical paper describing an architecture for HPC systems.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>Similar works would involve HPC architecture descriptions as well as low powered
computing architectures as well.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their contributions are a theoretical design and analysis of a HPC architecture
focused on energy efficiency.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Runnemede is a one of four architectures under the DARPA UHPC program.
Additionally, work has been done before to build both low powered cluster
computers, and HPC.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>The justification for this work is that there exists a theory that larger and
larger HPC computers will require more and more power, without fully utilizing
the entire device array. Additionally, a test chip was designed, but never
produced, called &amp;ldquo;Sunshine&amp;rdquo;. By designing this chip, the authors were able to
theoretically test the ideas presented in the paper as well as develop new ones
for the architecture.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the figures and tables are clear and easy to understand.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is well written and clear to understand.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>[2]&lt;/li>
&lt;/ul>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The author&amp;rsquo;s assumed that, &amp;ldquo;&amp;hellip; The power consumed by logic is expected to scale
well as feature sizes shrink, but not as well as transistor density, leading to
the design of &lt;em>over provisioned, energy-limited&lt;/em> systems that contain more
hardware than they can operate simultaneously&amp;rdquo;. In other words, systems will
have more and more &lt;em>power hungry&lt;/em> hardware that cannot be utilized in its
entirety. Additionally, they assume that the current trend with DRAM will cause
power consumption to decrement over time, but not fast enough.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>As the year is 2022, the current next generation hardware from NVIDIA, Intel,
and AMD has been announced, all of which require immense power draw to operate.
Additionally, DDR5 DRAM exists and consumes less power than the previous DDR4
DRAM. Therefore, I agree with the assumptions of the authors.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Runnemede: An Architecture for Ubiquitous High-Performance Computing&lt;/em>
by Nicholas P. Carter et al. [1] describes the Runnemede high performance
computing architecture targeting extreme-scale systems. This architecture was
developed for the DARPA&amp;rsquo;s Ubiquitous High-Performance Computing program to
address over provisioned, energy limited HPC architecture designs. The authors
proposed a theoretical architecture design, and justify it via bench marking
that they performed with simulations. Their work assumes (correctly in my
opinion) that systems will continue to require more power to operate in order to
achieve better performance.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/runnemede-an-architecture-for-ubiquitous-high-performance-computing/</description></item><item><title>A summary of ImageNet Classification with Deep Convolutional Neural Networks by Krizhevsky et al.</title><link>https://nsynovic.dev/summaries/imagenet-classification-with-deep-convolutional-neural-networks/</link><pubDate>Thu, 29 Sep 2022 14:33:01 -0500</pubDate><guid>https://nsynovic.dev/summaries/imagenet-classification-with-deep-convolutional-neural-networks/</guid><description>&lt;h1 id="a-summary-of-imagenet-classification-with-deep-convolutional-neural-networks">A summary of &lt;em>ImageNet Classification with Deep Convolutional Neural Networks&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Krizhevsky et al.;
&lt;a href="https://doi.org/10.1145/3065386">https://doi.org/10.1145/3065386&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-imagenet-classification-with-deep-convolutional-neural-networks">A summary of &lt;em>ImageNet Classification with Deep Convolutional Neural Networks&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#discussion-of-the-proofs">Discussion of the Proofs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>ImageNet Classification with Deep Convolutional Neural Networks&lt;/em> by
Krizhevsky et al. discusses the AlexNet model and its architecture as well as
its SOTA achievements in the 2012 ImageNet Challenge. The difference between
AlexNet and other contestants was that the model relies on GPU training to train
the convolutional neural network model. By utilizing the GPU, training time can
be accelerated significantly more than what was previously possible. Their major
contributions is that a large, deep convolutional neural network is capable of
achieving record-breaking results via supervised learning. They did not utilize
unsupervised pre-training, but the authors suspect that it would improve the
accuracy of the model.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is a computer vision model evaluation and architecture paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is similar to others that have published about SOTA results from the
ImageNet Challenge.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contributions were that training on GPUs allows for accelerated
training, that large and deep convolutional neural networks are effective at
classifying images, and that removing layers does decrease the performance of
models. Therefore, a larger, deeper model is applicable. It should be noted that
AlexNet was the largest model ever at the time of publication.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Previous work on designing convolutional neural networks and architectures.
However, they were bounded by not being particularly deep.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Because it is one of the key papers that demonstrates that large, deep,
convolutional neural networks are effective for image classification. As well as
providing evidence that training on GPUs is not only effective but recommended
for optimal performance. Additionally it provides empirical evidence that
removing a layer from a convolutional neural network is detrimental to the
performance of the model. In other words, the more layers you add, the more
potential there is for improvement.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>Nearly all of the figures are designed well, with the exception of Figure 2.
Figure 2 is the model architecture of AlexNet. This figure suffers from
information density and a three dimensional design which makes it hard to
determine what is going on and in what dimension are images being manipulated.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>They assume that it is because of the larger compute devices and data sets that
make these deep convolutional neural networks possible.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>While true, &lt;a href="going-deeper-with-convolutions.md">Szegedy et al.&lt;/a> designed their
own architecture using unique algorithms not prevalent in existing convolutional
neural networks.&lt;/p>
&lt;h3 id="discussion-of-the-proofs">Discussion of the Proofs&lt;/h3>
&lt;p>Their training involved both dropout and data augmentation.&lt;/p>
&lt;p>Dropout involves not using the outputs of neurons whose activation is less than
0.5.&lt;/p>
&lt;p>Data augmentation involves manipulating the input images such that 5 244 x 244
images are derived from one 256 x 256 image (e.g., the four corners and one
centered). Additionally, PCA was done on the RGB channels of all of the images
in the ImageNet 2010 and 2012 data sets. These eigenvectors were then added to
each of the images respective color channels.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>A reimplementation of the work would be interesting, with particular respect to
bench marking training time, as the authors were limited by their GPU compute
units&amp;rsquo; performance.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>&lt;em>Taken from &lt;a href="#first-pass">First Pass&lt;/a>&lt;/em>&lt;/p>
&lt;p>The paper &lt;em>ImageNet Classification with Deep Convolutional Neural Networks&lt;/em> by
Krizhevsky et al. [1] discusses the AlexNet model and its architecture as well
as its SOTA achievements in the 2012 ImageNet Challenge. The difference between
AlexNet and other contestants was that the model relies on GPU training to train
the convolutional neural network model as well as being a deep convolutional
neural network.&lt;/p>
&lt;p>By utilizing the GPU, training time can be accelerated significantly more than
what was previously possible. The benefits of being a deep convolutional neural
network is that the classification of images builds off of the features found in
the previous images. The result of this is that their top 1% and top 5% error
were the lowest ever in the competition.&lt;/p>
&lt;p>They trained their model by utilizing both dropout, where neurons that activated
with a value less than 0.5 are not inputted into the next layer, and by
augmenting the Imagenet 2010 and 2012 data sets to increase the amount of data
that they can throw at the model.&lt;/p>
&lt;p>Their work is important as it kicked off the usage of both deep convolutional
neural networks and the usage of GPUs to reduce training time.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/imagenet-classification-with-deep-convolutional-neural-networks/</description></item><item><title>A summary of Very Deep Convolutional Networks for Large-Scale Image Recognition by Karen Simonyan and Andrew Zisserman</title><link>https://nsynovic.dev/summaries/very-deep-convolutional-networks-for-large-scale-image-recognition/</link><pubDate>Wed, 28 Sep 2022 22:40:46 -0500</pubDate><guid>https://nsynovic.dev/summaries/very-deep-convolutional-networks-for-large-scale-image-recognition/</guid><description>&lt;h1 id="a-summary-of-very-deep-convolutional-networks-for-large-scale-image-recognition-by-karen-simonyan-and-andrew-zisserman">A summary of &lt;em>Very Deep Convolutional Networks for Large-Scale Image Recognition&lt;/em> by Karen Simonyan and Andrew Zisserman&lt;/h1>
&lt;blockquote>
&lt;p>Karen Simonyan and Andrew Zisserman;
&lt;a href="https://doi.org/10.48550/arXiv.1409.1556">https://doi.org/10.48550/arXiv.1409.1556&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-very-deep-convolutional-networks-for-large-scale-image-recognition-by-karen-simonyan-and-andrew-zisserman">A summary of &lt;em>Very Deep Convolutional Networks for Large-Scale Image Recognition&lt;/em> by Karen Simonyan and Andrew Zisserman&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Very Deep Convolutional Networks for Large Scale Image Recognition&lt;/em>
by Karen Simonyan and Andrew Zissernman discusses the SOTA performance of their
model in the 2014 ImageNet Challenge on localization and classification tasks.
They discuss that be extending the depth of convolutional neural networks to 16
up to 19 layers, with a 3x3 filter size, SOTA performance is possible without
redeveloping the architecture of existing convolutional neural networks. This is
in contrast to &lt;a href="going-deeper-with-convolutions.md">Szegedy&amp;rsquo;s work&lt;/a> who proposes
the Inception architecture for classification and object detection; with which
the reference implementation also came first in the 2014 ImageNet Challenge in
its respective tasks. Simoyan et al. discuss the architecture and training that
went into their model (VGG) and how to architect future models to perform as
well or better.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is both a computer vision model evaluation and architecture paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is most closely related to others who publish work regarding SOTA
performance on CV architecture and models.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contributions is an exploration of depth in traditional convolutional
neural networks to achieve SOTA performance.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Prior work has gone into optimizing the width and initial convolutions of
convolutional neural networks.&lt;/p>
&lt;p>&lt;a href="going-deeper-with-convolutions.md">Szegedy et al.&lt;/a> proposed a new architecture
(Inception) that achieved SOTA performance in the 2014 ImageNet Challenge. Else,
Krizhevsky et al. [2] and others have proposed improvements to the
convolutional neural network architecture.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about the authors work as increasing the depth of a neural
network by their proposed architecture allows for easy expansion of existing
convolutional neural networks without redesigning the libraries used to create
them.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The tables that are presented are easy to read, but can be improved upon. Often,
multiple rows will correspond with a single model configuration. This is fine,
however, it is difficult to make out what configuration each row corresponds to.
Additionally, the tables make comparing error percentages easy across model
configurations.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is well written and can be understood.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>Classical convolutional neural network architecture - [3]&lt;/li>
&lt;li>GoogLeNet - [2]&lt;/li>
&lt;li>Clarifai&lt;/li>
&lt;li>ImageNet classification with deep convolutional neural net- works [4]&lt;/li>
&lt;li>Isotropically-rescaled training image&lt;/li>
&lt;li>ImageNet 2013 submissions - [5], [6] Localization and Detection using
Convolutional Networks&lt;/li>
&lt;/ul>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors assume that the performance improvements that convolutional neural
networks are achieving are based off of larger data sets and better compute
optimization.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>I agree with their assumption. However, [2] created a SOTA model utilizing a
new architecture, rather than improving upon an existing one.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I would love to try and optimize the input layer of convolutional neural
networks by having a computation that not only looks at the color space, but
also the opacity of an image. This would allow for images to have their
background removed for the purposes of classification by making the background
less opaque than the foreground.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Very Deep Convolutional Networks for Large Scale Image Recognition&lt;/em>
by Karen Simonyan and Andrew Zissernman discusses the SOTA performance of their
model in the 2014 ImageNet Challenge on localization and classification tasks.
They discuss that be extending the depth of convolutional neural networks to 16
up to 19 layers, with a 3x3 filter size, SOTA performance is possible without
redeveloping the architecture of existing convolutional neural networks. Their
work builds of previous efforts of improving convolutional neural network
performance by optimizing the filter size and initial layer, but contrasts
contemporaries [2] by not developing a new architecture. Their work has
importance as it shows that the existing convolutional neural network
architecture is capable of SOTA performance by increasing the depth of the
model. They justify this by trying six different model configurations, and
finding that models with 16 to 19 layers performed best on the 2014 ImageNet
Challenge classification and localization challenges.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/very-deep-convolutional-networks-for-large-scale-image-recognition/</description></item><item><title>A summary of Going deeper with convolutions by Christian Szegedy et al.</title><link>https://nsynovic.dev/summaries/going-deeper-with-convolutions/</link><pubDate>Wed, 28 Sep 2022 20:07:40 -0500</pubDate><guid>https://nsynovic.dev/summaries/going-deeper-with-convolutions/</guid><description>&lt;h1 id="a-summary-of-going-deeper-with-convolutions">A summary of &lt;em>Going deeper with convolutions&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Christian Szegedy et al.;
&lt;a href="https://doi.org/10.48550/arXiv.1409.4842">https://doi.org/10.48550/arXiv.1409.4842&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-going-deeper-with-convolutions">A summary of &lt;em>Going deeper with convolutions&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Going deeper with convolutions&lt;/em> by Christian Szegedy et al. [1]
describes a 2014 state of the art computer vision model (on the ImageNet
Large-Scale Visual Recognition Challenge) called GoogLeNet architect ed based on
Hebbian principles (i.e., neurons that fire together, are wired together)and a
constant computational budget. Their approach relies on creative algorithms and
neuroscience principles and aims to be a more power efficient model for mobile
devices by limiting the computations during inference. Additionally, their model
is deep but not wide and is considered &amp;ldquo;sparse&amp;rdquo; by the authors. In other words,
there are as few nodes as possible within the neural network.&lt;/p>
&lt;p>Szegedy et al.&amp;rsquo;s contributions are a state of the art computer vision model that
provides experimental evidence that, &amp;ldquo;&amp;hellip; Approximating the expected optimal
sparse structure by readily available dense building blocks is a viable method
for improving neural networks for computer vision&amp;rdquo;. This means that this model
proves that dense neural networks for computer vision are not necessary in the
author&amp;rsquo;s viewpoint.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is a computer vision paper describing both a machine learning architecture
and reference model.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is related to other computer vision papers that achieve state of the
art performance values based on the ImageNet Large-Scale Visual Recognition
Challenge.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Szegedy et al.&amp;rsquo;s contributions are:&lt;/p>
&lt;ul>
&lt;li>A computer vision model architecture (Inception) that is both sparse and aims
to be computationally efficient on mobile (non-server) devices,&lt;/li>
&lt;li>A reference model of the aforementioned computer vision model architecture&lt;/li>
&lt;li>A comparison of previous state of the art work to justify their claims that
sparser networks are the future of computer vision models.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the tables have proper column labels. However, Table 1 does not provide
default values for blank cells. This is most likely due to the layer type not
performing a specific operation (as described in the column label). Regardless,
the remaining tables look good.&lt;/p>
&lt;p>Additionally, Figure 3 is very clear to read, if a little dense. However, as it
describes all of the layers of GoogLeNet and how they are connected, I find the
size to be appropriate.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is fairly well written. The only complaints that I have are minor
grammatical mistakes that the author&amp;rsquo;s left in (by accident I assume).
Additionally, that the authors didn&amp;rsquo;t optimize their tables and figures to
better fit on the pages. As tables and figures are stacked on top of one
another, it would be possible to reclaim paper space by rearranging multiple
tables and figures to be next to one another, with the exception of Figure 3 due
to the sheer size of it.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>Contrast Normalization&lt;/li>
&lt;li>Max Pooling&lt;/li>
&lt;li>Average Pooling&lt;/li>
&lt;li>Softmax Activation&lt;/li>
&lt;li>Dropout - [2]&lt;/li>
&lt;li>Localization Task - [3]&lt;/li>
&lt;li>Gabor Filters - [4]&lt;/li>
&lt;li>Network in Network - [5]&lt;/li>
&lt;li>Rectified Linear Activation - [6]&lt;/li>
&lt;li>Regions with Convolutional Neural Networks - [7]&lt;/li>
&lt;li>Multi-Box Prediction - [8]&lt;/li>
&lt;li>Arora proof - [9]&lt;/li>
&lt;li>LeNet 5 - [10]&lt;/li>
&lt;li>Fisher vectors&lt;/li>
&lt;li>Polyak Averaging - [11]&lt;/li>
&lt;li>Jaccard index&lt;/li>
&lt;li>Selective Search - [12]&lt;/li>
&lt;li>Photometric Distortions - [13]&lt;/li>
&lt;/ul>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>One assumption that the authors make is that over fitting is more prone to occur
in large models. Additionally, over fitting can occur when there is not enough
labeled examples in a data set when a large model is training. Furthermore,
increasing the size of a model increases the number of computations that must be
done between layers (e.g., chaining two convolutional layers results in
computation cost quadratic ally increasing) Their solutions relies on moving
from fully connected to sparsely connected architectures including within
convolutional layers. Also, their model architecture is based on the idea that
computers are inefficient when, &amp;ldquo;&amp;hellip; Computing numerical calculations on
non-uniform sparse data structures&amp;rdquo;.&lt;/p>
&lt;p>They assume that 1x1, 3x3, and 5x5 filters are the proper filters to use, but
did not test other size of filters. They also assume that using, &amp;ldquo;Inception
modules&amp;rdquo; is only useful at higher levels, whereas the initial levels are
standard convolutional levels. However, this was not tested either and was due
to, &amp;ldquo;infrastructural inefficiencies&amp;rdquo; in the implementation.&lt;/p>
&lt;p>Finally, that the model that achieved state of the art performance was the best
model. The authors had been training and testing other models for months prior,
however, it is unclear what the testing methodology was and why a particular
model was chosen to compete in the ImageNet competition.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>The first paragraph of assumptions seems reasonable and correct. However, the
remaining two paragraphs seem unreasonable. This is due to the lack of testing
that the author&amp;rsquo;s put in when optimizing their model with respect to the filter
sizes and choosing models. Furthermore, if testing did occur to address these
issues, it is not addressed in this paper, thus leaving the reader to wonder why
testing wasn&amp;rsquo;t performed.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>Based off of Section 3 (Motivation and High Level Considerations), one promising
area of study would be to perform a network architecture search utilizing the
principles and reasoning of their approach to other machine learning and
computer vision domains.&lt;/p>
&lt;p>An enhancement to their work is possible by analyzing what filter sizes most
optimal improve performance. Currently the author&amp;rsquo;s are restricting GoogLeNet to
1x1, 3x3, and 5x5 filter sizes, but this was due to convenience and no data was
given to support this.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Going deeper with convolutions&lt;/em> by Szegedy et al. [1] introduces a
computer vision model architecture called Inception and a reference model called
GoogLeNet.&lt;/p>
&lt;p>Inception is a model architecture that is both sparse and (attempts to be)
computationally efficient during inferencing with only 1.5 billion multiply-add
operations allowed. Inception models are composed of multiple Inception modules
that are stacked on top of each other. Each Inception module takes in data from
the previous layer and passes it into small convolutional filters (i.e., 1x1
typically). There are three of these small filters that are wired to the input
of the Inception module, with one of them connected directly to the output. The
outputs of two of these filters are then passed into larger filters (i.e., 5x5)
to which it is then passed into a DepthConcat function. Additionally, a 3x3
filter is wired to the input of the module and the output of which goes into a
1x1 filter to be passed into the DepthConcat function as well. From there, it is
passed into another Inception module and the process repeats.&lt;/p>
&lt;p>&lt;strong>Note:&lt;/strong> Depth when referring to two dimensional images refers to the color
channel of the image. As images typically have three color channels (i.e., red,
green, blue), an image would have a depth of 3.&lt;/p>
&lt;p>&lt;em>Example:&lt;/em> A 200 pixel by 200 pixel full color spectrum image would be
represented as 200x200x3.&lt;/p>
&lt;p>It is possible for Inception modules to have an additional connection to the
input of the module to perform average pooling for Softmax activation.&lt;/p>
&lt;p>GoogLeNet achieved SOTA performance in the ImageNet Large-Scale Visual
Recognition Challenge image classification task by having a top-5 error of 6.67%
on both the validation and testing data. This is an improvement of 56.5% in
comparison to 2012&amp;rsquo;s SOTA performer (SuperVision) and 2013&amp;rsquo;s SOTA performer
(Clarifai). Additionally, they achieved SOTA performance for the ImageNet
Large-Scale Visual Recognition Challenge detection task with a mean average
precision of 43.9% utilizing an ensemble inference approach. This model was
architected using the Inception architecture with 22 layers. However, not every
layer was an Inception module; the first few layers were standard convolutional
layers.&lt;/p>
&lt;p>The author&amp;rsquo;s contributions were as follows;&lt;/p>
&lt;ol>
&lt;li>The Inception computer vision architecture,&lt;/li>
&lt;li>The GoogLeNet SOTA computer vision model for classification and object
detection.&lt;/li>
&lt;/ol>
&lt;p>My opinion on this paper is that while it is well written, the author&amp;rsquo;s make
numerous assumptions about the optimal performance of their model&amp;rsquo;s
architecture. They don&amp;rsquo;t test optimal sizes for filters as well as resolving
bugs such as the usage of standard convolutional layers early in the model. Both
of which can be solved by performing a neural architecture search.&lt;/p>
&lt;p>Future work for this paper would involve optimizing the model architecture via a
neural architecture search. As well as evaluating the performance of the model
by both increasing and decreasing the depth of the model.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/going-deeper-with-convolutions/</description></item><item><title>A summary of How to Read a Paper by S. Keshav</title><link>https://nsynovic.dev/summaries/how-to-read-a-paper/</link><pubDate>Wed, 28 Sep 2022 15:11:09 -0500</pubDate><guid>https://nsynovic.dev/summaries/how-to-read-a-paper/</guid><description>&lt;h1 id="a-summary-of-how-to-read-a-paper">A summary of &lt;em>How to Read a Paper&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>S. Keshav;
&lt;a href="https://doi.org/10.1145/1273445.1273458">https://doi.org/10.1145/1273445.1273458&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-how-to-read-a-paper">A summary of &lt;em>How to Read a Paper&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#discussion-of-the-proofs">Discussion of the Proofs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#how-would-i-present-the-ideas">How Would I Present the Idea(s)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The paper, &lt;em>How to Read a Paper&lt;/em> by S. Keshav is a tutorial for graduate
students on how to read an academic paper. They propose a &amp;ldquo;three-pass&amp;rdquo; approach
that aims to reduce the frustration that graduate students face when reading
papers. Additionally, they discuss how to perform a literature survey of a new
field, their experience with this methodology, and write that this document is
meant to exist as a living work, with adjustments to be made as seen fit by the
author.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is definitely a more causal piece of academic work that aims at
easing students into the reading papers. I would classify this paper as &amp;ldquo;meta&amp;rdquo;,
educational, or as a formal letter to students. The later classification is due
to the lack of surveys or qualitative/ quantitative data from others that have
applied this or similar methods to reading papers.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is most closely related to papers that discuss the writing of
academic works and the review process of academic works.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>The assumptions in the abstract and introduction seem reasonable. However,
assuming that only graduate students are the only ones that struggle with
reading academic works is unrepresentative of &lt;em>my particular experience&lt;/em>.
Undergraduate students as well as professionals in industry also struggle with
reading these works as well.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>S. Keshav&amp;rsquo;s contributions is a three-stage process for reading papers and a
framework for performing literature reviews of a new field.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is well written and is easy to comprehend. I would strongly recommend
this paper to be read by everyone regardless of academic status.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>There are no illustrations to discuss in this paper.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>[1], [2], [3], and [4]&lt;/li>
&lt;/ul>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;m nit-picking here, but the S. Keshav focuses solely on graduate students as
the demographic that has trouble reading academic papers. Now while graduate
students do typically read more papers than undergraduates, it is not unheard of
for academic readings to be given to undergraduate students as homework
assignments or for them to read them on their own. Additionally, professionals
in industry also struggle with this task as well. A more inclusive audience
would have been appreciated, but would not have improved the content or quality
of this paper.&lt;/p>
&lt;h3 id="discussion-of-the-proofs">Discussion of the Proofs&lt;/h3>
&lt;p>The only proof of the &amp;ldquo;three-pass&amp;rdquo; method that was discussed was the experience
of the author. An awfully biased proof, however, I do appreciate at least some
quantifiable data for this method.&lt;/p>
&lt;h3 id="how-would-i-present-the-ideas">How Would I Present the Idea(s)&lt;/h3>
&lt;p>I think the author presented these ideas exceptionally well and clearly, and
cannot think of any additional presentation method aside from the critiques of
the assumptions mentioned in &lt;a href="#author-assumptions">Author Assumptions&lt;/a>.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>A survey of graduate students on their methodology for reading papers&lt;/li>
&lt;li>A survey of industry professionals on their methodology for reading papers&lt;/li>
&lt;li>An artifact that allows for a user to step through a set series of steps to
properly understand a document.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>How to Read a Paper&lt;/em> by S. Keshav is a &amp;ldquo;meta&amp;rdquo; or educational paper
about how to read an academic work. Their main contributions are a three step
process on how to read a paper, as well as a framework for performing a
literature review in a new field. This three step process involves a:&lt;/p>
&lt;ol>
&lt;li>Bird&amp;rsquo;s Eye View of the paper where only the Title, Abstract, Introduction,
Conclusion, and section and sub-section headings are read first,&lt;/li>
&lt;li>A deeper analysis of figures and content of the paper which involves finding
new, unread references to the reader and evaluating the quality of
illustrations to determine the quality of the paper,&lt;/li>
&lt;li>A virtual reimplementation of the paper where every claim of the paper is
analyzed and critiqued; typically this done by reviewers or those that are
doing a deeper analysis of the work.&lt;/li>
&lt;/ol>
&lt;p>I can see this process being useful for researchers as implementers of other&amp;rsquo;s
research must accomplish all three steps to properly appreciate and understand
what they need to do to perform their task. As for the literature review
framework, it involves utilizing academic search engines (e.g.,
&lt;a href="https://https://scholar.google.com/">Google Scholar&lt;/a>) to find work within a
particular field, finding shared citations or authors within that field, then
evaluating top conferences within that field to see who the top researchers and
research topics are within that field. For exploratory research, this is both an
extremely simple and effective framework to follow and adapt to different
domains.&lt;/p>
&lt;p>However, S. Keshav does limit the reach of this paper by making it focus solely
on the woes of graduate students. This is inaccurate of the wider academic
readership, as more and more frequently undergraduate and industry professionals
are reading academic papers both for pleasure and for utilization in
assignments. This paper can easily become more inclusive of wider audiences
without changing the content in an updated version of this document. This would
make sense as the author has requested that this paper be treated as a living
document that can be subject to change as the author adapts his process and
framework for academic review.&lt;/p>
&lt;p>I would personally like to see this work be quantified in surveys and
implemented as artifacts that ensure that readers are properly following the
review method that the author has laid out.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/how-to-read-a-paper/</description></item></channel></rss>