<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>magazine on Nicholas M. Synovic</title><link>https://nsynovic.dev/categories/magazine/</link><description>Recent content in magazine on Nicholas M. Synovic</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Nicholas M. Synovic</copyright><lastBuildDate>Sun, 19 Feb 2023 20:40:07 -0600</lastBuildDate><atom:link href="https://nsynovic.dev/categories/magazine/feed.xml" rel="self" type="application/rss+xml"/><item><title>A summary of Interactive Supercomputing With Jupyter by Rollin Thomas et al.</title><link>https://nsynovic.dev/summaries/interactive-supercomputing-with-jupyter/</link><pubDate>Sun, 19 Feb 2023 20:40:07 -0600</pubDate><guid>https://nsynovic.dev/summaries/interactive-supercomputing-with-jupyter/</guid><description>&lt;h1 id="a-summary-of-interactive-supercomputing-with-jupyter">A summary of &lt;em>Interactive Supercomputing With Jupyter&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Rollin Thomas et al.; IEEE Computing Edge, October 2022 DOI [0]&lt;/p>
&lt;/blockquote>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-interactive-supercomputing-with-jupyter">A summary of &lt;em>Interactive Supercomputing With Jupyter&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#how-it-started-and-how-its-going">How It Started And How It&amp;rsquo;s Going&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#phase-1-jupyterhub-as-science-gateway">Phase 1: JupyterHub as Science Gateway&lt;/a>&lt;/li>
&lt;li>&lt;a href="#phase-2-jupyter-on-a-cori-login-node">Phase 2: Jupyter on a Cori Login Node&lt;/a>&lt;/li>
&lt;li>&lt;a href="#phase-3-jupyter-as-interface-to-an-hpc-center">Phase 3: Jupyter as Interface to an HPC Center&lt;/a>&lt;/li>
&lt;li>&lt;a href="#phase-4-jupyterlab-as-innovation-platform">Phase 4: JupyterLab as Innovation Platform&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#jupyter--hpc--science">Jupyter + HPC = Science!&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>The Jupyter project has the potential to allow other scientific domains to
utilize supercomputing resources in an accessible manner. The National Energy
Research Scientific Computing Center (NERSC) has implemented a Jupyter interface
into their Cori supercomputer. This interface now captures 20% - 25% of user
traffic when working on Cori.&lt;/p>
&lt;h3 id="how-it-started-and-how-its-going">How It Started And How It&amp;rsquo;s Going&lt;/h3>
&lt;p>In 2015, NERSC recognized that a growing number of users were using SSH to run
their Jupyter notebooks on the previous generation Edison supercomputer. As
such, NERSC began looking into how to incorporate Jupyter notebooks as a
standard interface into Cori.&lt;/p>
&lt;h4 id="phase-1-jupyterhub-as-science-gateway">Phase 1: JupyterHub as Science Gateway&lt;/h4>
&lt;p>NERSC implemented a separate hardware solution to host JupyterHub. This
JupyterHub instance allowed users to store their notebooks on the NERSC Global
Filesystem (NGF), which allowed teams and individuals to collaborate and run
shared notebooks.&lt;/p>
&lt;h4 id="phase-2-jupyter-on-a-cori-login-node">Phase 2: Jupyter on a Cori Login Node&lt;/h4>
&lt;p>Jupyter was than ran on login nodes with outputs piped to computation nodes. The
architecture and hosting of both the Jupyter and JupyterHub instances kept
changing hardware and which confused end users.&lt;/p>
&lt;h4 id="phase-3-jupyter-as-interface-to-an-hpc-center">Phase 3: Jupyter as Interface to an HPC Center&lt;/h4>
&lt;p>JupyterHub was moved to a Docker container and hosted on Cori. It now acts as
the single point of access for running Jupyter notebooks on Cori, specialty
servers, and staff only test servers.&lt;/p>
&lt;h4 id="phase-4-jupyterlab-as-innovation-platform">Phase 4: JupyterLab as Innovation Platform&lt;/h4>
&lt;p>JupyterLab is a product from the Jupyter project aimed at collaboration and
provides many improvements on top of the standard Jupyter project. Such
improvements include better file system navigation and reusability and
reproducibility of notebook experiments. NERSC has created a number of
extensions to support JupyterLab on Cori, including file system navigation
extensions, and &lt;code>jupyterlab-slurm&lt;/code> for adding &lt;code>SLURM&lt;/code> job scheduling directly
within JupyterLab.&lt;/p>
&lt;h3 id="jupyter--hpc--science">Jupyter + HPC = Science!&lt;/h3>
&lt;p>A number of disciplines now rely on Jupyter to perform calculations on the Cori
supercomputer. These include:&lt;/p>
&lt;ul>
&lt;li>Geophysical Subsurface Imaging&lt;/li>
&lt;li>Electron Microscope Image Analysis&lt;/li>
&lt;li>Advanced Light Source Tomography&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/interactive-supercomputing-with-jupyter/</description></item><item><title>A summary of Automated Payment Terminal Testing: How to Achieve Continuous Integration for Systems that are Almost Impossible to Virtualize by Martin Gloor et al.</title><link>https://nsynovic.dev/summaries/automated-payment-terminal-testing-how-to-achieve-continuous-integration-for-systems-that-are-almost-impossible-to-virtualize/</link><pubDate>Sun, 19 Feb 2023 15:42:36 -0600</pubDate><guid>https://nsynovic.dev/summaries/automated-payment-terminal-testing-how-to-achieve-continuous-integration-for-systems-that-are-almost-impossible-to-virtualize/</guid><description>&lt;h1 id="a-summary-of-automated-payment-terminal-testing-how-to-achieve-continuous-integration-for-systems-that-are-almost-impossible-to-virtualize">A summary of &lt;em>Automated Payment Terminal Testing: How to Achieve Continuous Integration for Systems that are Almost Impossible to Virtualize&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Martin Gloor et al. IEEE Computing Edge, December 2022 DOI [0]&lt;/p>
&lt;/blockquote>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-automated-payment-terminal-testing-how-to-achieve-continuous-integration-for-systems-that-are-almost-impossible-to-virtualize">A summary of &lt;em>Automated Payment Terminal Testing: How to Achieve Continuous Integration for Systems that are Almost Impossible to Virtualize&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#shift-left-of-die---or-why-are-we-doing-this">Shift Left of Die - Or, Why are We Doing This?&lt;/a>&lt;/li>
&lt;li>&lt;a href="#to-virtualize-or-not-to-virtualize">To Virtualize or Not To Virtualize&lt;/a>&lt;/li>
&lt;li>&lt;a href="#a-journey-of-trial-and-error">A Journey of Trial and Error&lt;/a>&lt;/li>
&lt;li>&lt;a href="#scaling-with-cards">Scaling With Cards&lt;/a>&lt;/li>
&lt;li>&lt;a href="#coping-with-calibration">Coping With Calibration&lt;/a>&lt;/li>
&lt;li>&lt;a href="#apis-and-abstraction-layers">APIs and Abstraction Layers&lt;/a>&lt;/li>
&lt;li>&lt;a href="#generic-but-not">Generic, But Not!&lt;/a>&lt;/li>
&lt;li>&lt;a href="#habemus-automation">Habemus Automation!&lt;/a>&lt;/li>
&lt;li>&lt;a href="#a-continuous-journey">A Continuous Journey&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Pure software solutions benefit from continuous integration platforms. However,
solutions that involve both hardware and software - such as payment terminals -
need to develop custom continuous integration platforms that test both
components in order to meet modern Agile development practices.&lt;/p>
&lt;h3 id="shift-left-of-die---or-why-are-we-doing-this">Shift Left of Die - Or, Why are We Doing This?&lt;/h3>
&lt;p>Since Abrantix (the company the authors work at) design and development payment
terminal systems, they typically test end-to-end solutions with human testers.
However, as tests involve many repetitive tasks, human error and fatigue can
invalidate many tests. Therefore, they needed to develop an automated platform
to not only handle software testing and continuous integration, but also
hardware testing and integration. In doing so, they aim to catch bugs earlier in
the development cycle, reduce human error, and run regression testing all
autonomously.&lt;/p>
&lt;h3 id="to-virtualize-or-not-to-virtualize">To Virtualize or Not To Virtualize&lt;/h3>
&lt;p>While it is possible to abstract hardware interfaces virtually, there is
difficulty in virtualizing some actions that need to be tested. For example:&lt;/p>
&lt;ul>
&lt;li>Card readers are hard to mock as they need to test magstripe, contactless and
contact chip payments, and the Eurocard, MasterCard, Visa (EMV) standard would
need to be virtualized as well.&lt;/li>
&lt;li>Secure modules are hard to mock as the required cryptographic functions are
often proprietary, can involve both symmetric and asymmetric keys, and have to
comply with rigorous standards.&lt;/li>
&lt;li>No real end-to-end testing of hardware and software is tested.&lt;/li>
&lt;/ul>
&lt;h3 id="a-journey-of-trial-and-error">A Journey of Trial and Error&lt;/h3>
&lt;p>Abrantix decided to go with a robot to solve the end-to-end hardware and
software testing. They started with a cheap robotic arm and found that it was
capable of handling both contactless and contact based chip solutions, but was
unable to enter PIN codes. They therefore pivoted to a 3D printed solution to
enter PIN codes and to drop the robotic arm solution.&lt;/p>
&lt;h3 id="scaling-with-cards">Scaling With Cards&lt;/h3>
&lt;p>To scale with multiple cards, a multiplexer was built to handle inputs from
multiple different cards into a single output.&lt;/p>
&lt;h3 id="coping-with-calibration">Coping With Calibration&lt;/h3>
&lt;p>In order to calibrate the robot for each terminal, a layout of each terminal was
made and a user could select what terminal to test. It was therefore on the user
to swap in and out the terminals for each test. This way a single test could be
written and abstracted away from the terminal interface.&lt;/p>
&lt;h3 id="apis-and-abstraction-layers">APIs and Abstraction Layers&lt;/h3>
&lt;p>At this point the base end-to-end testing solution was built. However, as the
solution scaled to meet the demands of the company and partners, it became
apparent that the software architecture couldn&amp;rsquo;t handle all of the requested
features. Therefore, abstraction layers were written and a REST HTTP API
interface was created to simplify both the development of the software solution
as well as to allow third party orchestration tools to hook into the state of
the testing solution.&lt;/p>
&lt;h3 id="generic-but-not">Generic, But Not!&lt;/h3>
&lt;p>To allow for control of the robotic testing platform as well as to provide
extensibility to the REST API, a schema based on JSON was developed that can be
used to control the robot.&lt;/p>
&lt;h3 id="habemus-automation">Habemus Automation!&lt;/h3>
&lt;p>Now that the testing solution had been developed, a stress test of running 5,000
transactions on individual terminals was ran. 70 bugs were found on average per
terminal model. These bugs couldn&amp;rsquo;t have been found in such a short time without
the development of this tool.&lt;/p>
&lt;h3 id="a-continuous-journey">A Continuous Journey&lt;/h3>
&lt;p>There a number of security concerns and procedures not currently implemented in
the testing solution. However, Abrantix is intending to continue the development
of this solution and iterate upon, just as any developer would working with
continuous integration.&lt;/p>
&lt;hr>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/automated-payment-terminal-testing-how-to-achieve-continuous-integration-for-systems-that-are-almost-impossible-to-virtualize/</description></item><item><title>A summary of Toward Fail Safety for Security Decisions by Trent Jaeger</title><link>https://nsynovic.dev/summaries/toward-fail-safety-for-security-decisions/</link><pubDate>Sun, 19 Feb 2023 13:39:57 -0600</pubDate><guid>https://nsynovic.dev/summaries/toward-fail-safety-for-security-decisions/</guid><description>&lt;h1 id="a-summary-of-toward-fail-safety-for-security-decisions">A summary of &lt;em>Toward Fail Safety for Security Decisions&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Trent Jaeger IEEE Computing Edge, December 2022 DOI [0]&lt;/p>
&lt;/blockquote>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-toward-fail-safety-for-security-decisions">A summary of &lt;em>Toward Fail Safety for Security Decisions&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#examples-of-how-user-decisions-impact-security">Examples of How User Decisions Impact Security&lt;/a>&lt;/li>
&lt;li>&lt;a href="#a-design-goal-fail-safe-decisions">A Design Goal: Fail-Safe Decisions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#using-fail-safety-effectively">Using Fail Safety Effectively&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Developers of technologies aim to automate security decisions on behalf of the
consumer. However, security decisions made by end users are often ad-hoc and
unguided. As researchers, we should figure out how to create systems that are
reduce vulnerabilities when poor manual choices are made.&lt;/p>
&lt;h3 id="examples-of-how-user-decisions-impact-security">Examples of How User Decisions Impact Security&lt;/h3>
&lt;p>Mobile applications often request access to sensors on the device to operate.
However, it is often unclear as to why these applications want to utilize these
sensors. Additionally, by granting security access to malicious apps, it is
possible for the apps to modify core system functionality or compromise core
processes for their own gain.&lt;/p>
&lt;p>Within the IoT space, often developers let the end users handle the security
permissions on their end. However, it is both the developers and the end-users
job to ensure that the devices are operating within in a secure environment.&lt;/p>
&lt;h3 id="a-design-goal-fail-safe-decisions">A Design Goal: Fail-Safe Decisions&lt;/h3>
&lt;p>We should have fail-safe defaults that expresses the permissions that a device
or application has rather than what it doesn&amp;rsquo;t. In other words, return positive
feedback as to the what an application or device can do, rather than what it
can&amp;rsquo;t.&lt;/p>
&lt;p>To support this, applications need to help users make better decisions. This
could be done through supporting safe choices through the path of least
resistance. In other words, make it easy to secure an application and difficult
to disable security. However, it is difficult for programmers to decide what is
and isn&amp;rsquo;t a safe decision.&lt;/p>
&lt;h3 id="using-fail-safety-effectively">Using Fail Safety Effectively&lt;/h3>
&lt;p>If fail safety is implemented on a per device level, it won&amp;rsquo;t be able to scale
to entire technology sectors. Therefore, best practices need to be developed in
order to allow fail-safety to grow.&lt;/p>
&lt;p>However, as legacy applications did not focus on information-flow integrity, it
might be impossible to back port fail safety to legacy applications.&lt;/p>
&lt;p>Furthermore, research needs to be done on how to identify when manual security
changes expose applications to security vulnerabilities. This way programmers
and researchers can devise new methods of protecting the end user when they
disable security features.&lt;/p>
&lt;hr>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/toward-fail-safety-for-security-decisions/</description></item><item><title>A summary of Pervasive Security and Privacy by - A Brief Reflection on Challenges and Opportunities by Florian Alt</title><link>https://nsynovic.dev/summaries/pervasive-security-and-privacy-a-brief-reflection-on-challenges-and-opportunities/</link><pubDate>Sun, 19 Feb 2023 10:56:20 -0600</pubDate><guid>https://nsynovic.dev/summaries/pervasive-security-and-privacy-a-brief-reflection-on-challenges-and-opportunities/</guid><description>&lt;h1 id="a-summary-of-pervasive-security-and-privacy-by---a-brief-reflection-on-challenges-and-opportunities">A summary of &lt;em>Pervasive Security and Privacy by - A Brief Reflection on Challenges and Opportunities&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Florian Alt IEEE Computing Edge, December 2022 DOI [0]&lt;/p>
&lt;/blockquote>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-pervasive-security-and-privacy-by---a-brief-reflection-on-challenges-and-opportunities">A summary of &lt;em>Pervasive Security and Privacy by - A Brief Reflection on Challenges and Opportunities&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#the-state-of-security-and-privacy-in-pervasive-computing">The State of Security and Privacy in Pervasive Computing&lt;/a>&lt;/li>
&lt;li>&lt;a href="#implications">Implications&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#security-and-privacy-decision-overload">Security and Privacy Decision Overload&lt;/a>&lt;/li>
&lt;li>&lt;a href="#unawareness-of-data-sensitivity">Unawareness of Data Sensitivity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#sensing-close-to-the-body">Sensing Close to the Body&lt;/a>&lt;/li>
&lt;li>&lt;a href="#unclear-flow-of-data">Unclear Flow of Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#multidevice-environments">Multidevice Environments&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#challenges-and-opportunities">Challenges and Opportunities&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#designing-appropriate-mechanisms">Designing Appropriate Mechanisms&lt;/a>&lt;/li>
&lt;li>&lt;a href="#involvement-of-different-stakeholders">Involvement of Different Stakeholders&lt;/a>&lt;/li>
&lt;li>&lt;a href="#out-of-the-box-security-and-privacy">Out-of-the-Box Security and Privacy&lt;/a>&lt;/li>
&lt;li>&lt;a href="#adaptive-security-and-privacy-mechanisms">Adaptive security and Privacy Mechanisms&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Pervasive computing developments open up the opportunity for different human
experiences and research. However, security and privacy methods must take into
account what these new developments bring to the table; including both the
positives and the negatives of such computing. With this technology, user states
and contexts can be more easily inferred as well as their emotional and
cognitive states.&lt;/p>
&lt;p>This article is meant to give an overview of the challenges and opportunities
that arise within the security and privacy domain of pervasive computing.&lt;/p>
&lt;h3 id="the-state-of-security-and-privacy-in-pervasive-computing">The State of Security and Privacy in Pervasive Computing&lt;/h3>
&lt;p>The &lt;em>mainframe era&lt;/em> involved securing the intellectual rights of technologies
stored on big iron. The &lt;em>personal computing era&lt;/em> involved securing protecting
the privacy and data of everyday users. The &lt;em>pervasive computing era&lt;/em> will
involve protecting all of the data generating and capturing devices that a user
not only owns but interacts with.&lt;/p>
&lt;p>Pervasive computing includes both traditional computing devices, but also edge
devices, smart devices and appliances, and internet software. Pervasive
computing allows for sensitive data to be accessed both locally and remotely,
and therefore presents new challenges w.r.t security and privacy.&lt;/p>
&lt;p>Boundaries between domains are changing. For example, their used to be a barrier
between work and home, but with the COVID-19 pandemic, work and home became one.
This opened the doors to new attack vectors as it became more common for people
to work from home.&lt;/p>
&lt;h3 id="implications">Implications&lt;/h3>
&lt;h4 id="security-and-privacy-decision-overload">Security and Privacy Decision Overload&lt;/h4>
&lt;p>As we interface with more and more computers, we (as users) become overloaded
with different authentication schemes and practices. Additionally, all of the
devices that we interact with have many different privacy permissions and
options that the user might not be aware of and therefore enable or disable.&lt;/p>
&lt;h4 id="unawareness-of-data-sensitivity">Unawareness of Data Sensitivity&lt;/h4>
&lt;p>It is possible to generate many data points about an individual from a single
sensor. Therefore, it is imperative that users not only know about these
different data points, but also the implications for each data point. However,
it is currently very difficult to inform users of the importance of each data
point.&lt;/p>
&lt;h4 id="sensing-close-to-the-body">Sensing Close to the Body&lt;/h4>
&lt;p>Many of the &lt;a href="#unawareness-of-data-sensitivity">data sensitivity problems&lt;/a> arises
from users wearing sensors close to the body. These sensors can pick up on
health related information about an individual. Current sensor providers do very
little to protect this information. Therefore figuring out methods of obscuring
or reducing the collection of such information is important.&lt;/p>
&lt;h4 id="unclear-flow-of-data">Unclear Flow of Data&lt;/h4>
&lt;p>It is very difficult to understand where all of the data from internet of things
(IoT) devices is being stored. Thus the flow of data from a sensor to the
end-user is unclear. What data goes to the cloud? What stays locally? How is
data accessed? Who can access that data? How is it processed? Which data is
being collected? Novel solutions (i.e., privacy labels/badges) must be developed
to answer these questions to protect consumers.&lt;/p>
&lt;h4 id="multidevice-environments">Multidevice Environments&lt;/h4>
&lt;p>There is a push by hardware and software developers to integrate experiences
tightly together via multidevice communication. For example, logging into
Netflix on a smart TV through your cell phone. However, these multidevice
experiences raise security and privacy concerns as there are more points of
failure and attack vectors as the number of devices involved scales.&lt;/p>
&lt;h3 id="challenges-and-opportunities">Challenges and Opportunities&lt;/h3>
&lt;h4 id="designing-appropriate-mechanisms">Designing Appropriate Mechanisms&lt;/h4>
&lt;p>Better security interfaces need to be designed to promote users to protect their
data. Furthermore, good enough security practices (i.e., password
authentication) need to be revisited to see if and where areas of improvement
need to occur to better protect end users.&lt;/p>
&lt;h4 id="involvement-of-different-stakeholders">Involvement of Different Stakeholders&lt;/h4>
&lt;p>The end user is not the enemy with respect to security. Therefore, pervasive
computing technologies and experiences need to take into account the security
practices and limitations that end users experience and accommodate that. For
example, replacing password authentication with bio-metrics.&lt;/p>
&lt;h4 id="out-of-the-box-security-and-privacy">Out-of-the-Box Security and Privacy&lt;/h4>
&lt;p>Pervasive computing technologies need to be secure out-of-the-box and involve
very little user interaction to enable sensible security and privacy settings.&lt;/p>
&lt;h4 id="adaptive-security-and-privacy-mechanisms">Adaptive security and Privacy Mechanisms&lt;/h4>
&lt;p>Pervasive computing can allow for authentication schemes based on the state of
the user. However, this information must simultaneously be protected and secured
in order to prevent data leaks.&lt;/p>
&lt;hr>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/pervasive-security-and-privacy-a-brief-reflection-on-challenges-and-opportunities/</description></item><item><title>A summary of Economics of Artificial Intelligence in Cybersecurity by Nir Kshetri</title><link>https://nsynovic.dev/summaries/economics-of-artificial-intelligence-in-cybersecurity/</link><pubDate>Sun, 19 Feb 2023 10:16:22 -0600</pubDate><guid>https://nsynovic.dev/summaries/economics-of-artificial-intelligence-in-cybersecurity/</guid><description>&lt;h1 id="a-summary-of-economics-of-artificial-intelligence-in-cybersecurity">A summary of &lt;em>Economics of Artificial Intelligence in Cybersecurity&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Nir Kshetri IEEE Computing Edge, December 2022 DOI [0]&lt;/p>
&lt;/blockquote>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-economics-of-artificial-intelligence-in-cybersecurity">A summary of &lt;em>Economics of Artificial Intelligence in Cybersecurity&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#current-state-of-ai-in-cybersecurity-and-key-areas-being-transformed">Current State of AI in Cybersecurity and Key Areas Being Transformed&lt;/a>&lt;/li>
&lt;li>&lt;a href="#key-factors-driving-ais-use-in-cybersecurity">Key Factors Driving AI&amp;rsquo;s Use in Cybersecurity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#some-shortcomings-limitations-and-challenges">Some Shortcomings, Limitations, and Challenges&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>AI in cybersecurity is a growing market estimated to reach $101.8 billion by
2030.&lt;/p>
&lt;p>As the internet was designed &lt;strong>without&lt;/strong> security in mind, their exists an
asymmetric where small organizations or individuals can comprise large groups,
companies, or nation states. Therefore, defense mechanisms must be in place to
stop these attacks. Since defendants have access to a large corpus of malware
examples, it is possible to train AI models to defend against attacks.&lt;/p>
&lt;h3 id="current-state-of-ai-in-cybersecurity-and-key-areas-being-transformed">Current State of AI in Cybersecurity and Key Areas Being Transformed&lt;/h3>
&lt;p>AI is often faster at detecting malware than humans. Additionally, it is also
faster at containing infected devices on a network than a human. Furthermore, AI
can lower the cost of detecting malware.&lt;/p>
&lt;p>Traditional antivirus programs rely on a database of malware examples to check
against. However, AI can learn the representations of malware and then find new
malware examples prior to them being added to a database. Thus, it is possible
for AI to stop zero-day vulnerabilities by recognizing the representation of
malware during or prior to the attack.&lt;/p>
&lt;p>However, security experts recommend taking an augmented intelligence approach
with AI security. This involves a human-AI partnership in identifying and acting
upon threats. This is because current AI techniques are not accurate or advanced
enough to entirely replace a human security professional.&lt;/p>
&lt;p>AI has already been used to stop advance attacks from foreign, state run
organizations (i.e., APT41) and more common attacks.&lt;/p>
&lt;p>AI is already in use to detect anomalies in identity and access security. It is
possible for an AI to monitor and make decisions based off of an accounts
activity. As an example, Facebook uses an AI-powered deep entity classification
(DEC) to determine if an account is fraudulent or not. If so, the account is
removed from Facebook. This tool was used to crack down on fake accounts and
accounts that utilized deep fakes.&lt;/p>
&lt;p>AI cybersecurity software is being used by academic institutions as attackers
are increasingly targeting universities and academic institutions. These tools
have been successful in stopping or preventing cyber threats on students, staff,
and networks.&lt;/p>
&lt;h3 id="key-factors-driving-ais-use-in-cybersecurity">Key Factors Driving AI&amp;rsquo;s Use in Cybersecurity&lt;/h3>
&lt;p>The cost of AI cybersecurity tools is dropping both for consumers and for
enterprises. There exists more publicly available and enterprise-only datasets
for training AI to detect malware. And there is a shortage of cybersecurity
professionals entering the workforce, so AI could be used to address this
shortage of staff.&lt;/p>
&lt;h3 id="some-shortcomings-limitations-and-challenges">Some Shortcomings, Limitations, and Challenges&lt;/h3>
&lt;p>There exists several shortcomings with using AI cybersecurity tools. For
starters, it is difficult to explain what the tool is doing. Security
professionals therefore prefer an, &amp;ldquo;Explainable First, Predictive Second&amp;rdquo; [1]
approach to AI tools. Additionally, AI tools can not make security related
decisions without human interventions.&lt;/p>
&lt;p>And as AI tools grow in popularity, bias will start to develop within these
tools. This could potentially allow an attacker to exploit this bias and write
malware that is not detected by the tool.&lt;/p>
&lt;p>Furthermore, it is uncertain how AI tools would handle volatile situations, such
as the COVID-19 pandemic. As such, cybersecurity professionals might turn &lt;em>off&lt;/em>
or raise the detection threshold on these tools during such situations,
potentially allowing attacks to slip through.&lt;/p>
&lt;p>Finally, not all of the necessary data to properly train these tools is
available due to federal regulation. Personally identifying information (PII)
cannot be made publicly available for the purposes of training. Additionally, it
is assumed that large &amp;ldquo;data lakes&amp;rdquo; of Americans exist under the control of
foreign entities. It is therefore possible for an entity to utilize one of these
data lakes to write attacks that would not be detected as the attack is coming
from an American rather than a foreign entity.&lt;/p>
&lt;hr>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/economics-of-artificial-intelligence-in-cybersecurity/</description></item><item><title>A summary of Sentiment Analysis and Topic Recognition in Video Transcripts by Lukas Stappen et al.</title><link>https://nsynovic.dev/summaries/sentiment-analysis-and-topic-recognition-in-video-transcripts/</link><pubDate>Sat, 18 Feb 2023 19:03:11 -0600</pubDate><guid>https://nsynovic.dev/summaries/sentiment-analysis-and-topic-recognition-in-video-transcripts/</guid><description>&lt;h1 id="a-summary-of-sentiment-analysis-and-topic-recognition-in-video-transcripts">A summary of &lt;em>Sentiment Analysis and Topic Recognition in Video Transcripts&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Lukas Stappen et al. Posted in IEEE Computing Edge, December 2022 DOI [0]&lt;/p>
&lt;/blockquote>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-sentiment-analysis-and-topic-recognition-in-video-transcripts">A summary of &lt;em>Sentiment Analysis and Topic Recognition in Video Transcripts&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#related-work">Related Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#dataset-the-muse-topic-subchallenge">Dataset: The MuSe-Topic Subchallenge&lt;/a>&lt;/li>
&lt;li>&lt;a href="#exploratory-analysis">Exploratory Analysis&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#speaker-topics">Speaker Topics&lt;/a>&lt;/li>
&lt;li>&lt;a href="#emotions">Emotions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#prediction-results">Prediction Results&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#speaker-topics-1">Speaker Topics&lt;/a>&lt;/li>
&lt;li>&lt;a href="#emotions-1">Emotions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>It is difficult to extract the sentiment and topic from a video transcript. With
this in mind, researchers developed SenticNet [2], a natural language
processing (NLP) model to identify the sentiment and topic of a transcript with
far less computational resources than previous attempts. The authors were able
to achieve 3% better performance than previous solutions for the MuSe
competition.&lt;/p>
&lt;p>Multi-model sentiment analysis (MSA) is taking a variety of data streams and
information types and extracting sentiment from them. MSA research aims to
understand the sentiment holder, emotional disposition, and the reference
object. MSA typically works on video data as it includes visual (e.g., facial
expressions), audio, and textual (e.g., transcripts) data modalities.
Transcripts have been found to provide the greatest impact in understanding the
topic at hand.&lt;/p>
&lt;p>The authors solution learned a continuous vector space of embeddings from the
symbolic space of words from the transcripts. To identify sentiments, their
solution adheres to the description of of sentiments defined by the &lt;em>Hourglass
of Emotions&lt;/em> [1].&lt;/p>
&lt;h3 id="related-work">Related Work&lt;/h3>
&lt;p>Human communication is a symbolic and naturally ordered within a structured
hierarchy. Current solutions to identifying sentiment from human communication
rely on synsets which are labels that indicate emotion and mood categories.
SenticNet has the largest amount of synsets with 200,000 concepts map words to a
sentiment.&lt;/p>
&lt;p>Automated sentiment and aspect extraction is of interest within the MSA field.
Current solutions involve hand crafted features. The authors applied
&amp;ldquo;commonsense knowledge&amp;rdquo; about topic extraction involving several sentences.&lt;/p>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;p>First, natural world concepts are obtained using SenticNet. Versions 5 and 6 of
SenticNet were used as both extracted different sentics. Stop words were
removed. A linear SVM was applied on the vector word embeddings to predict the
valence, arousal, and topics of the transcript. To improve the generalization
between the feature maps, embedding dropout was used as well as time-step
dropout in order to drop entire embeddings rather than features.&lt;/p>
&lt;h3 id="dataset-the-muse-topic-subchallenge">Dataset: The MuSe-Topic Subchallenge&lt;/h3>
&lt;p>The MuSe-CaR [3] dataset is a large, multi-modal dataset consisting of YouTube
videos of car reviews. The purpose of the dataset is to support MSA research.&lt;/p>
&lt;p>The authors only used the language modality of the dataset and ignored the video
and audio modality. Recent advances in speech to text technologies have resulted
in near human performance.&lt;/p>
&lt;p>For the MuSe-Topic challenge, the weighted score of the combination of the
unweighted average recall and micro F1 measures for each prediction (valence,
arousal, and topic) was reported.&lt;/p>
&lt;h3 id="exploratory-analysis">Exploratory Analysis&lt;/h3>
&lt;h4 id="speaker-topics">Speaker Topics&lt;/h4>
&lt;p>The concepts of semantics were used to identify the contextual information of
the video. These were used to understand the characteristic properties of the
video.&lt;/p>
&lt;h4 id="emotions">Emotions&lt;/h4>
&lt;p>SenticNet was used in an unsupervised fashion to identify the emotions of the
video from the contextual information.&lt;/p>
&lt;h3 id="prediction-results">Prediction Results&lt;/h3>
&lt;h4 id="speaker-topics-1">Speaker Topics&lt;/h4>
&lt;p>The best performance measured achieved a score of 66.16% on the test dataset.
This was better than the LSTM approach the authors also tried. However, Albert
(an end-2-end NLP transformer for supervised NLP tasks) [4] still outperforms
this solution.&lt;/p>
&lt;h4 id="emotions-1">Emotions&lt;/h4>
&lt;p>SenticNet version 6 outperforms version 5 when identifying emotions.&lt;/p>
&lt;hr>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/sentiment-analysis-and-topic-recognition-in-video-transcripts/</description></item><item><title>A summary of Advances in Human Activity Recognition by Gulustan Dogan</title><link>https://nsynovic.dev/summaries/advances-in-human-activity-recognition/</link><pubDate>Sat, 18 Feb 2023 18:51:36 -0600</pubDate><guid>https://nsynovic.dev/summaries/advances-in-human-activity-recognition/</guid><description>&lt;h1 id="a-summary-of-advances-in-human-activity-recognition">A summary of &lt;em>Advances in Human Activity Recognition&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Gulustan Dogan IEEE Computing Edge, December 2022 DOI [0]&lt;/p>
&lt;/blockquote>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-advances-in-human-activity-recognition">A summary of &lt;em>Advances in Human Activity Recognition&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#this-method-is-great-when-working-on-raw-data-streams-but-there-do-exist-better-algorithms-and-models-to-handle-visual-representations-of-movement">This method is great when working on raw data streams, but there do exist better algorithms and models to handle visual representations of movement.&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Human activity recognition (HAR) involves classifying sequences of accelerometer
data together to identify defined movements. Current solutions involve hand
crafting features (thereby requiring an expert of the space to assist), or by
training machine learning models using decision trees.&lt;/p>
&lt;p>Long short term memory (LSTM) models are currently the most powerful type of
recurrent neural networks (RNNs). LSTMs are great at identifying and predicting
sequential information as they take both time and sequence in to account.
However, LSTMs are computationally expensive.&lt;/p>
&lt;h2 id="the-current-state-of-the-art-devises-an-algorithm-that-takes-in-raw-signal-or-visual-data-and-can-identify-patterns-and-sequences-of-movement-1-this-method-is-great-when-working-on-raw-data-streams-but-there-do-exist-better-algorithms-and-models-to-handle-visual-representations-of-movement">The current state of the art devises an algorithm that takes in raw signal or visual data, and can identify patterns and sequences of movement [1]. This method is great when working on raw data streams, but there do exist better algorithms and models to handle visual representations of movement.&lt;/h2>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/advances-in-human-activity-recognition/</description></item><item><title>A summary of Challenges and Opportunities for Autonomous Micro-UAVs in Precision Agriculture by Xu Liu et al.</title><link>https://nsynovic.dev/summaries/challenges-and-opportunities-for-autonomous-micro-uavs-in-precision-agriculture/</link><pubDate>Sat, 18 Feb 2023 13:08:36 -0600</pubDate><guid>https://nsynovic.dev/summaries/challenges-and-opportunities-for-autonomous-micro-uavs-in-precision-agriculture/</guid><description>&lt;h1 id="a-summary-of-challenges-and-opportunities-for-autonomous-micro-uavs-in-precision-agriculture">A summary of &lt;em>Challenges and Opportunities for Autonomous Micro-UAVs in Precision Agriculture&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Xu Liu et al. IEEE Computing Edge, December 2022 DOI [0]&lt;/p>
&lt;/blockquote>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-challenges-and-opportunities-for-autonomous-micro-uavs-in-precision-agriculture">A summary of &lt;em>Challenges and Opportunities for Autonomous Micro-UAVs in Precision Agriculture&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#uav-hardware-and-autonomy">UAV Hardware and Autonomy&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#uav-platforms-and-autonomy">UAV Platforms and Autonomy&lt;/a>&lt;/li>
&lt;li>&lt;a href="#sensor-configuration">Sensor Configuration&lt;/a>&lt;/li>
&lt;li>&lt;a href="#challenges">Challenges&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#object-detection-and-segmentation">Object Detection and Segmentation&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#image-based-2-d">Image-Based (2-D)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#lidar-based-3-d">LiDAR-Based (3-D)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#challenges-1">Challenges&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#robot-localization-and-mapping">Robot Localization and Mapping&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#semantic-localization-and-mapping">Semantic Localization and Mapping&lt;/a>&lt;/li>
&lt;li>&lt;a href="#challenges-2">Challenges&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Unmanned ground and areal vehicles (UGVs and UAVs respectfully) are utilized for
precision agriculture. UGVs can carry a larger payload and can run longer, but
suffer from only being operational within 2D space. UAVs have the benefit of
being able to navigate rougher terrains in 3D space, but suffer from limited
flight times and smaller payloads. This article aims to survey the recent
advances in UAV technologies applied to precision farming and to present
opportunities for improvement.&lt;/p>
&lt;p>There are many agriculture issues that exist in our world today. Over 700
million people are malnourished, 70% of fresh water is utilized by agriculture
domains, and advancements made on micro scales do not scale to the macro
environment. Having more data from autonomous sensors and vehicles will help
improve the realizations of scientific advancements in scale.&lt;/p>
&lt;p>Current UGV technologies suffer from being able to only see points of interest
close to vehicle, they can not survey large areas quickly, and they are unable
to navigate rough agriculture environments (i.e.,, rice fields). UAVs do not
suffer from these drawbacks, however, to work for precision agriculture, UAV
technologies need to work close to crops (under canopy flight), must have
reliable relative coordinate reporting, handle a dynamic environment in PD space
(i.e.,, wind, rocks, hills), and must be able to map dense environments. There
have been some under canopy tests that have been performed to limited success.
However, these were only applied in small scale environments and haven&amp;rsquo;t been
tested in larger environments.&lt;/p>
&lt;h3 id="uav-hardware-and-autonomy">UAV Hardware and Autonomy&lt;/h3>
&lt;p>Current autonomous UAV technologies are available, but their usage is limited.
They currently are reliant upon GPS, which requires an open canopy, making under
canopy flight currently impossible. Additionally, complex tasks such as
segmentation of fruits or trees is not possible at this time.&lt;/p>
&lt;h4 id="uav-platforms-and-autonomy">UAV Platforms and Autonomy&lt;/h4>
&lt;p>Several different autonomous systems have been proposed for under-canopy
autonomous flight.&lt;/p>
&lt;p>These include:&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;&amp;hellip; A stereo visual-inertial odometry (VIO) algorithm was used for state
estimation, a 2-D light detection and ranging (LiDAR) mounted on a nodding
gimbal was used for mapping and obstacle avoidance, and a search-based motion
planner was used for motion primitives plans collision-free and dynamically
feasible trajectories,&amp;rdquo; [1]&lt;/li>
&lt;li>A vision based solution that can navigate in both structured and moderately
unstructured environments (i.e., a collapsed building) [2]&lt;/li>
&lt;li>A system of using many UAVs that work together and coordinate through a
simultaneous localization and mapping (SLAM) scheme with loop closure that
utilized trees as landmarks was tested [3], but could fail when operating
within a dense forest&lt;/li>
&lt;/ul>
&lt;p>A limitation with all of these systems so far is that they are unable to work in
long range agricultural missions and they are unable to identify objects in real
time and at scale.&lt;/p>
&lt;h4 id="sensor-configuration">Sensor Configuration&lt;/h4>
&lt;p>Sensors on UAVs are used for both autonomy and for collecting mission specific
data. The most common sensors on UAVs are cameras, inertial measurement units
(IMUs), LiDARs, and global navigation satellite systems (GNSS). These sensors
must be lightweight because multi-rotor UAVs consume 100 - 200 W/kg.&lt;/p>
&lt;p>Cameras and IMUs are great for navigation and obstacle avoidance. However,
cameras are easily susceptible to changes in lightness and darkness. Thus
obstacle avoidance becomes difficult when lighting is patchy.&lt;/p>
&lt;p>LiDAR sensors can be used to reduce this issue, however, current LiDAR
technology is expensive, heavy, and still under much research.&lt;/p>
&lt;p>GNSS technologies (i.e., GPS) allow for geospatial positioning. However, if
there are obstacles in the way, the accuracy decreases. There do exists
optimizations to improve geospatial coordination and positioning, such as GPS
ground stations (DGPS) and real time kinematics (RTK). However, these solutions
must be both real time and reliable to resolve accuracy concerns.&lt;/p>
&lt;h4 id="challenges">Challenges&lt;/h4>
&lt;p>Detection of small obstacles is difficult with conventional camera systems. Thus
a forward facing, solid state LiDAR solution has been proposed to mitigate this.
However, the 360 degree view that LiDAR provides is lost because all of the
LiDAR beams are focused at the front of the device in order to gain resolution.&lt;/p>
&lt;p>Smaller UAVs can be more nimble, however, there is a weight to power and a
weight to flight time concern with these devices.&lt;/p>
&lt;p>Running deep neural network (DNN) algorithms on board a UAV is critical for low
latency, real time data collection, estimation, and understanding. However,
current DNN algorithms are computationally expensive to run. It is predicted
that more efficient algorithms, as well as the usage of AI accelerators, will
help mitigate this problem.&lt;/p>
&lt;h3 id="object-detection-and-segmentation">Object Detection and Segmentation&lt;/h3>
&lt;p>Object detection and segmentation are critical to precise agriculture as plant
or fruit specific data can be captured and acted upon.&lt;/p>
&lt;h4 id="image-based-2-d">Image-Based (2-D)&lt;/h4>
&lt;p>RGB, multi and hyper spectral imaging, thermal, and near-infrared imaging have
been used to perform object detection on plants. Previous methods involved using
K-Means algorithms and SVMs to solve detection and segmentation problems.
Recently, DNN based solutions are becoming more popular and additional sensor
data from the ground is also inputted into these algorithms to provide more
accurate results.&lt;/p>
&lt;h4 id="lidar-based-3-d">LiDAR-Based (3-D)&lt;/h4>
&lt;p>LiDAR based CV solutions are relatively new to the agriculture space. To
represent the problem domain, LiDAR captures data in the forms including a voxel
grid, point clouds, and multi-view and/or spherical images. It has been found
that voxel grid based convolutional neural networks (CNNs) are susceptible to
noise, whereas point clouds are not as affected. It is possible to join LiDAR
point cloud data and a spherical range image together and pass the union of this
data into a CNN to reduce information loss [4].&lt;/p>
&lt;h4 id="challenges-1">Challenges&lt;/h4>
&lt;p>It is difficult to acquire large, high-quality agriculture specific datasets to
train models on object detection and segmentation. Furthermore, occlusion
(e.g.,, not being able to see the plant or fruit) is still a problem that is
trying to be solved.&lt;/p>
&lt;h3 id="robot-localization-and-mapping">Robot Localization and Mapping&lt;/h3>
&lt;p>Mapping refers to the act of creating an understanding of an environment for an
autonomous robot to adhere to. Potential solutions require the input of
knowledge about the structure of the field.&lt;/p>
&lt;h4 id="semantic-localization-and-mapping">Semantic Localization and Mapping&lt;/h4>
&lt;p>Semantic features allows the robot to generate a meaningful map of the
environment and assist in pose estimation. The usage of locating and
representing trees as points of interests has been studied and found to be
useful for identifying local regions.&lt;/p>
&lt;h4 id="challenges-2">Challenges&lt;/h4>
&lt;p>SLAM is able to performing mapping quite well in man made environments. However,
new technologies must be developed to assist with the mapping of natural
environments. Active mapping (where an autonomous agent maps out its environment
in real time) is difficult to do in an agriculture context as fields can be
quite large and contain a dense information mapping.&lt;/p>
&lt;hr>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/challenges-and-opportunities-for-autonomous-micro-uavs-in-precision-agriculture/</description></item><item><title>A summary of Efficient Computer Vision for Embedded Systems by George K. Thiruvathukal and Yung-Hsiang Lu</title><link>https://nsynovic.dev/summaries/efficient-computer-vision-for-embedded-systems/</link><pubDate>Sat, 18 Feb 2023 10:41:18 -0600</pubDate><guid>https://nsynovic.dev/summaries/efficient-computer-vision-for-embedded-systems/</guid><description>&lt;h1 id="a-summary-of-a-summary-of-efficient-computer-vision-for-embedded-systems-0">A summary of &lt;em>A summary of Efficient Computer Vision for Embedded Systems&lt;/em> [0]&lt;/h1>
&lt;blockquote>
&lt;p>George K. Thiruvathukal and Yung-Hsiang Lu; IEEE Computing Edge, December
2022, 2022 &lt;a href="https://doi.org/10.1109/MC.2022.3145677">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-a-summary-of-efficient-computer-vision-for-embedded-systems-0">A summary of &lt;em>A summary of Efficient Computer Vision for Embedded Systems&lt;/em> [0]&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#lpcvc">LPCVC&lt;/a>&lt;/li>
&lt;li>&lt;a href="#lpcvc-research">LPCVC Research&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;blockquote>
&lt;p>Disclosure: Both George K. Thiruvathukal and Yung-Hsiang Lu are mentors,
colleagues, and friends of mine. Disclosure: I am a student organizer of the
2023 Low Power Computer Vision competition.&lt;/p>
&lt;/blockquote>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Computer Vision (CV) research and development is encouraged through competitions
that measure the accuracy of models for cash prizes. However, as more CV
technologies are being pushed towards the edge, power and computational
efficiency of these models become increasingly more important. Therefore, the
&lt;em>IEEE Low Power Computer Vision Challenge&lt;/em> (LPCVC) [1], formerly the &lt;em>Low
Power Image Recognition Challenge&lt;/em>, was created to encourage researchers to
develop efficient low power solutions.&lt;/p>
&lt;h3 id="lpcvc">LPCVC&lt;/h3>
&lt;blockquote>
&lt;p>&lt;em>Why did you participate in LPCVC?&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>Researchers participate to develop and formalize techniques for running machine
learning on the edge. This could be designing new hardware to accelerate CV
models and/or validating optimization and software techniques developing new CV
models. Additionally, this challenge encourages the discovery of new research
topics to pursue.&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>How is LPCVC relevant to activities in the IEEE Computer Society&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>The development and support of CV research is crucial the Computer Society&amp;rsquo;s
mission. The technical community, &lt;em>Technical Community on Pattern Analysis and
Machine Intelligence&lt;/em> [2], exists to promote the development of research and
solutions to problems surrounding and involving CV. Furthermore, the &lt;em>Computer
Vision and Pattern Recognition&lt;/em> (CVPR) conference [3] is currently IEEE&amp;rsquo;s most
influential conference as ranked by &lt;em>Guide2Research&lt;/em> [4].&lt;/p>
&lt;h3 id="lpcvc-research">LPCVC Research&lt;/h3>
&lt;blockquote>
&lt;p>&lt;em>Why is research in LPCV important?&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>As AI on the edge becomes more ubiquitous and desired by consumers, academic
research my provide industry with potential solutions to implement on the edge.
Furthermore, there exists a hardware challenge alongside the software challenge
of deploying LPCV solutions on edge. Thus hardware focused research must occur
to assist in LPCV optimizations. Hardware devices such as GPUs, CPUs, and Neural
Processing Units (NPUs) need to be designed and optimized (w.r.t hardware and
software) to support CV applications on the edge. Therefore, LPCV research
involves the union of power efficient designs and optimizations of both the
deployment hardware and solution software.&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>Can you describe one (or several) &amp;ldquo;grand challenges&amp;rdquo; using CV; the solutions
will significantly change the world, but are they far beyond today&amp;rsquo;s
technologies?&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>Better smart systems (e.g.,, smart homes, retail, factory, transportation,
farming, etc.) and prediction of natural disasters and phenomenon by utilizing
many data points can be possible through LPCV.&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>If you have unlimited resources, what would you like to see in the area of
LPCV?&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;p>Researchers in the space are looking for new datasets, challenges, and problem
specific competitions to advance research in LPCV.&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/efficient-computer-vision-for-embedded-systems/</description></item></channel></rss>