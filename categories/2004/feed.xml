<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2004 on Nicholas M. Synovic</title><link>https://nsynovic.dev/categories/2004/</link><description>Recent content in 2004 on Nicholas M. Synovic</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Nicholas M. Synovic</copyright><lastBuildDate>Fri, 18 Nov 2022 20:47:39 -0600</lastBuildDate><atom:link href="https://nsynovic.dev/categories/2004/feed.xml" rel="self" type="application/rss+xml"/><item><title>A summary of Distinctive Image Features from Scale-Invariant Keypoints by David G. Lowe</title><link>https://nsynovic.dev/summaries/distinctive-image-features-from-scale-invariant-keypoints/</link><pubDate>Fri, 18 Nov 2022 20:47:39 -0600</pubDate><guid>https://nsynovic.dev/summaries/distinctive-image-features-from-scale-invariant-keypoints/</guid><description>&lt;h1 id="a-summary-of-distinctive-image-features-from-scale-invariant-keypoints">A summary of &lt;em>Distinctive Image Features from Scale-Invariant Keypoints&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>David G. Lowe International Journal of Computer Vision, 2004
&lt;a href="https://doi.org/10.1023/B:VISI.0000029664.99615.94">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-distinctive-image-features-from-scale-invariant-keypoints">A summary of &lt;em>Distinctive Image Features from Scale-Invariant Keypoints&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>This paper aims to address image matching challenges by finding scale invariant
features from images. These features perform well against images that are
subject to blurring and noise.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this paper as it presents an efficient method for
generating image features that are invariant to scale and rotation changes. This
allows for images to be taken in arbitrary locations and at different locations
to then be matched with similar objects in a different image. In other words, it
presents an efficient way of generating image features that can be used to
compare how similar two images are regardless of scale and rotation differences.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is an algorithms paper focused on image matching and retrieval.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This work is most similar to work discussing image feature extraction and image
matching and retrieval techniques.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contributions were a methodology for extracting scale invariant image
features as well as methods for comparing features between images for the
purposes of image matching and retrieval.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done to develop image feature extractors. These extractors were
initially used for stereo and short range motion tracking. However, they are now
capable of more complex tasks. These include image recognition and retrieval.
All of these feature extractors produce a representation of the image that can
be used to compare one image against another.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All of the figures are clearly labeled. I do find the lines on the line charts
are a bit difficult to distinguish due to the usage of a tight dashed line. But
that&amp;rsquo;s on me, not the paper.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>The paper is well written, if a bit dense. There is an argument to be made that
this paper is two papers in one. One about a novel feature extraction technique,
and a second about image retrieval with the usage of feature extractors.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ol start="2">
&lt;li>Brown, M. and Lowe, D.G. 2002. Invariant features from interest point groups.
In British Machine Vision Conference, Cardiff, Wales, pp. 656–665.&lt;/li>
&lt;li>Carneiro, G. and Jepson, A.D. 2002. Phase-based local features. In European
Conference on Computer Vision (ECCV), Copenhagen, Denmark, pp. 282–296.&lt;/li>
&lt;li>Crowley,J.L.and Parker,A.C.1984.A representation for shapes based on peaks
and ridges in the difference of low-pass transform. IEEE Trans. on Pattern
Analysis and Machine Intelligence, 6(2):156–170.&lt;/li>
&lt;li>Fergus, R., Perona, P., and Zisserman, A. 2003. Object class recognition by
unsupervised scale-invariant learning. In IEEE Conference on Computer Vision
and Pattern Recognition, Madison, Wisconsin, pp. 264–271.&lt;/li>
&lt;li>Harris,C.and Stephens,M.1988.A combined corner and edge detector. In Fourth
Alvey Vision Conference, Manchester, UK, pp. 147–151.&lt;/li>
&lt;li>Koenderink, J.J. 1984. The structure of images. Biological Cybernetics,
50:363–396.&lt;/li>
&lt;/ol>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>For their feature extractor, they created a dataset of images and their
features, then performed transformations on images present in the dataset to
generate new features. With these variables, they were able to measure the
performance of their feature extractor and how well it was able to identify
invariant features. With respect to their testing on object recognition and
image retrieval, they utilized K Nearest Neighbor (KNN) algorithms to accomplish
this.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>This work doesn&amp;rsquo;t rely on the usage of Deep Neural Networks (DNNs) to learn the
representation of images. Because of this, their work relies on hand crafted
filters and algorithms to extract features. This could result in algorithmic
bias or generate results that are susceptible to the views of the author.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>Keeping in mind that this paper was published in 2004, this assumption seems
valid for the time. Due to the AI winter as well as the limited usage of GPUs
for the purposes of training DNNs, handcrafting feature extractors was a valid
usage.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>While I can&amp;rsquo;t say that image retrieval is of much interest to me, I would like
to explore how to perform object detection or image recognition using this
feature extractor. Additionally, it would be really cool to see if I could
utilize this feature extractor on low powered devices for the purposes of image
classification.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>The &lt;code>Background&lt;/code> section of this paper mentioned the usage of feature detectors
for motion capture. Is that possible with this feature extractor? What does that
space look like today vs 2004 vs 1990s? What would happen if I trained a Deep
Learning model on SIFT features? Could I get a comparable output to a CNN with
respect to image classification (for example)?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>This is a great paper that introduces a novel technique for performing feature
extraction. However, it is a bit dense and could&amp;rsquo;ve been split into two separate
papers. One being an algorithms paper presenting the feature extractor, and
another being a case study of feature extractor performance on many tasks.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Distinctive Image Features from Scale-Invariant Keypoints&lt;/em> by David
G. Lowe [1] presents a novel image feature extractor called Scale Invariant
Feature Transform (SIFT). SIFT is an algorithm to extract features from an image
that are invariant (do not change) to scale and rotation. These features can be
used to perform image retrieval and object recognition by utilizing nearest
neighbor algorithms such as KNN or ANN.&lt;/p>
&lt;p>This summary was kept short as I have been sitting on this summary for well over
two weeks now with no progress and just want to get something out. Sorry for the
brevity and weak summary.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/distinctive-image-features-from-scale-invariant-keypoints/</description></item><item><title>A summary of Robust Real Time-Face Detection by P. Viola and M.J. Jones</title><link>https://nsynovic.dev/summaries/robust-real-time-face-detection/</link><pubDate>Mon, 24 Oct 2022 19:29:57 -0500</pubDate><guid>https://nsynovic.dev/summaries/robust-real-time-face-detection/</guid><description>&lt;h1 id="a-summary-of-robust-real-time-face-detection">A summary of &lt;em>Robust Real-Time Face Detection&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>P. Viola and M.J. Jones;
&lt;a href="https://doi.org/10.1023/B:VISI.0000013087.49260.fb">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-robust-real-time-face-detection">A summary of &lt;em>Robust Real-Time Face Detection&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Discussion about the title, abstract, introduction, section and sub-section
headings, and conclusion&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Robust Real-Time Face Detection&lt;/em> by P. Viola and M.J. Jones [1]
presents a new methodology for efficiently performing face detection. They due
this through the usage of an integral image which is able to reduce the
computational complexity to constant time (O(1)) of analyzing an image as it
doesn&amp;rsquo;t rely on scale invariance and thus an image pyramid. Additional, the
classifier that they build is &amp;ldquo;simple and efficient&amp;rdquo; and allows for the engineer
to specify a large number of features to be analyzed without compromising on
performance as it relies upon the Ada Boost algorithm to select important
features. Furthermore, the authors propose a method for building a cascade of
classifiers which further reduces computation time as each classifier specifies
. Finally, they propose experiments that can be ran on face detection data sets
to conduct supervised learning.&lt;/p>
&lt;p>While this paper does propose many new and innovative ideas, the paper
originates from 2003.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is a research paper focusing on improving the Computer Vision task of face
detection without the reliance of CNNs.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is most closely related to non-CNN face detection papers.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>They create an integral image which is able to reduce the computational
complexity to constant time (O(1)) of analyzing an image as it doesn&amp;rsquo;t rely on
scale invariance and thus an image pyramid. Additionally, the classifier that
they build is &amp;ldquo;simple and efficient&amp;rdquo; and allows for the engineer to specify a
large number of features to be analyzed without compromising on performance as
it relies upon the Ada Boost algorithm to select important features.
Furthermore, the authors propose a method for building a cascade of classifiers
which further reduces computation time as each classifier specifies . Finally,
they propose experiments that can be ran on face detection data sets to conduct
supervised learning.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Prior work has been done in creating face detection systems. Prior work has been
done in creating the Ada Boost algorithm that is used to create a cascade of
classifiers. Prior work has been done in identifying methodologies to create
image features.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this paper as it presents a non-CNN methodology for
reliably identifying faces in images. Additionally, the authors also present a
methodology for doing this task efficiently on low end hardware.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The figures, diagrams, and graphs are well explained and designed.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>Yes, however a bit lengthy. Optimizations could have been made with respect to
reducing the amount of content describing the background to the Ada Boost
algorithm.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ul>
&lt;li>A Decision-Theoretic Generalization of On-Line Learning and an Application to
Boosting [2]&lt;/li>
&lt;/ul>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d like to implement their work on a low powered device and compare it to a
newer CNN model on ML metrics.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Robust Real-Time Face Detection&lt;/em> by P. Viola and M.J. Jones [1]
presents a new methodology for efficiently performing face detection. They due
this through the usage of an integral image which is able to reduce the
computational complexity to constant time (O(1)) of analyzing an image as it
doesn&amp;rsquo;t rely on scale invariance and thus an image pyramid. Additionally, the
classifier that they build is &amp;ldquo;simple and efficient&amp;rdquo; and allows for the engineer
to specify a large number of features to be analyzed without compromising on
performance as it relies upon the Ada Boost algorithm to select important
features. Furthermore, the authors propose a method for building a cascade of
classifiers which further reduces computation time as each classifier specifies
. Finally, they propose experiments that can be ran on face detection data sets
to conduct supervised learning.&lt;/p>
&lt;p>The main &amp;ldquo;wow&amp;rdquo; factor of this work is that it was built on a low powered system.
This same application could be more performant on modern smartphones in
comparison to the system that it was originally tested on.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/robust-real-time-face-detection/</description></item></channel></rss>