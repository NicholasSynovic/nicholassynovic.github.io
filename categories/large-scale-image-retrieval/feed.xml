<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>large-scale image retrieval on Nicholas M. Synovic</title><link>https://nsynovic.dev/categories/large-scale-image-retrieval/</link><description>Recent content in large-scale image retrieval on Nicholas M. Synovic</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Nicholas M. Synovic</copyright><lastBuildDate>Wed, 30 Nov 2022 16:14:49 -0600</lastBuildDate><atom:link href="https://nsynovic.dev/categories/large-scale-image-retrieval/feed.xml" rel="self" type="application/rss+xml"/><item><title>A summary of Large-Scale Image Retrieval with Attentive Deep Local Features by Hyeonwoo Noh et al.</title><link>https://nsynovic.dev/summaries/large-scale-image-retrieval-with-attentive-deep-local-features/</link><pubDate>Wed, 30 Nov 2022 16:14:49 -0600</pubDate><guid>https://nsynovic.dev/summaries/large-scale-image-retrieval-with-attentive-deep-local-features/</guid><description>&lt;h1 id="a-summary-of-large-scale-image-retrieval-with-attentive-deep-local-features">A summary of &lt;em>Large-Scale Image Retrieval with Attentive Deep Local Features&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Hyeonwoo Noh et al. ICCV, 2017 &lt;a href="https://doi.org/10.1109/ICCV.2017.374">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-large-scale-image-retrieval-with-attentive-deep-local-features">A summary of &lt;em>Large-Scale Image Retrieval with Attentive Deep Local Features&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>The authors intend to address the problem of image retrieval when images are
occuleded or have objects blocking the subject by taking a weakly supervised
Deep Learning (DL) approach. Additionally, they propose a large scale dataset
that would assist the image retrieval community in creating new SOTA models.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>We should care about this paper because it proposes a SOTA method for generating
robust image features using a DL approach. Additionally, it is the paper that
proposes the large scale Google-Landmarks dataset.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is a deep learnign computer vision paper as well as a datasets
release paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This work is related to image feature extraction, image retrieval, computer
vision, and deep learning computer vision.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>Their main contributions are a SOTA deep learning computer vision model for
image retireval as well as a large scale dataset for training similar image
retrieval models.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done to create both hand crafted and DL solutions to image
retrieval and image feature extraction.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>All figures are properly labeled and well explained.&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is well written.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ol start="2">
&lt;li>A. Gordo, J. Almazan, J. Revaud, and D. Larlus. Deep Image Retrieval:
Learning Global Representations for Image Search. In Proc. ECCV, 2016.&lt;/li>
&lt;li>F. Radenovi 패 c, G. Tolias, and O. Chum. CNN Image Retrieval Learns from BoW:
Unsupervised Fine-Tuning with Hard Examples. In Proc. ECCV, 2016.&lt;/li>
&lt;li>U. Buddemeier and H. Neven. Systems and Methods for Descriptor Vector
Computation, 2012. US Patent 8,098,938.&lt;/li>
&lt;li>H. Neven, G. Rose, and W. G. Macready. Image Recognition with an Adiabatic
Quantum Computer I. Mapping to Quadratic Unconstrained Binary Optimization.
arXiv:0804.4457, 2008.&lt;/li>
&lt;li>K. M. Yi, E. Trulls, V. Lepetit, and P. Fua. LIFT: Learned Invariant Feature
Transform. In Proc. ECCV, 2016.&lt;/li>
&lt;li>R. Arandjelovi 패 c, P. Gronat, A. Torii, T. Pajdla, and J. Sivic. NetVLAD:
CNN Architecture for Weakly Supervised Place Recognition. In Proc. CVPR,
2016.&lt;/li>
&lt;li>H. J 패 egou, M. Douze, C. Schmidt, and P. Perez. Aggregating Local
Descriptors into a Compact Image Representation. In Proc. CVPR, 2010.&lt;/li>
&lt;li>H. J 패 egou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and C. Schmid.
Aggregating Local Image Descriptors into Compact Codes. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 34(9), 2012.&lt;/li>
&lt;/ol>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The authors compared variations of DELF against SOTA image retrieval techniques
including CONGAS [4, 5], DIR [2], siaMAC [3] and LIFT [6] and graphed
precision vs recall on the Google Landmarks dataset and the accuracy of the same
methods on smaller datasets.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>That GPS coordinates were a useful feature to include in the Google Landmarks
dataset.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>I found it interesting that the authors included the GPS coordinates of images
selected for the Google Landmarks dataset. I&amp;rsquo;m not sure how useful this is for
image retrieval tasks as it relies upon image features, rather than resolving
GPS coordinates. While the authors used it for validating the ground truth
labels for the landmarks, actual usage for the purposes of image retrieval
alludes me.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d like to compare this result to DL models trained on both
&lt;a href="surf-speeded-up-robust-features.md">SURF&lt;/a> and
&lt;a href="distinctive-image-features-from-scale-invariant-keypoints.md">SIFT&lt;/a> feature
descriptions.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>Why was ResNet-50 choosen as the base model? The Google Landmarks dataset has
one million images, but only ~13,000 landmarks (~77 images per landmark). Is
there bias as to where the landmarks are choosne from? Is that enough landmarks
to train a DL model for the purposes of image feature description?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>A solid paper, however the discussion of the Google Landmarks dataset left me
wanting more that, in my opinion, could&amp;rsquo;ve been published in a blog post.&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Large-Scale Image Retrieval with Attentive Deep Local Features&lt;/em> by
Hyeonwoo Noh et al. discusses both a new image retrieval method that achieves
SOTA performance based off of ResNet-50 (DELF), and an image retrieval dataset
(Google Landmarks) to create similar models. DELF aims to work as an image
feature descriptor that is robust to occulusion and background clutter. DELF
achieves SOTA performance on the Google Landmarks dataset compared against
previous SOTA methods including CONGAS [4, 5], DIR [2], siaMAC [3] and
LIFT [6]. Additionally, DELF is more accurate the aforementioned methods on
smaller, traditional datasets.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/large-scale-image-retrieval-with-attentive-deep-local-features/</description></item></channel></rss>