<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2018 on Nicholas M. Synovic</title><link>https://nsynovic.dev/categories/2018/</link><description>Recent content in 2018 on Nicholas M. Synovic</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Nicholas M. Synovic</copyright><lastBuildDate>Sat, 03 Dec 2022 12:01:03 -0600</lastBuildDate><atom:link href="https://nsynovic.dev/categories/2018/feed.xml" rel="self" type="application/rss+xml"/><item><title>A summary of Focal Loss for Dense Object Detection by Tsung-Yi Lin et al.</title><link>https://nsynovic.dev/summaries/focal-loss-for-dense-object-detection/</link><pubDate>Sat, 03 Dec 2022 12:01:03 -0600</pubDate><guid>https://nsynovic.dev/summaries/focal-loss-for-dense-object-detection/</guid><description>&lt;h1 id="a-summary-of-focal-loss-for-dense-object-detection">A summary of &lt;em>Focal Loss for Dense Object Detection&lt;/em>&lt;/h1>
&lt;blockquote>
&lt;p>Tsung-yi Lin et al. arXiv, 2018 &lt;a href="http://arxiv.org/abs/1708.02002">DOI&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>For the summary of the paper, go to the &lt;a href="#summary">Summary&lt;/a> section of this
article.&lt;/p>
&lt;h2 id="table-of-contents">Table of Contents&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#a-summary-of-focal-loss-for-dense-object-detection">A summary of &lt;em>Focal Loss for Dense Object Detection&lt;/em>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#table-of-contents">Table of Contents&lt;/a>&lt;/li>
&lt;li>&lt;a href="#first-pass">First Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#problem">Problem&lt;/a>&lt;/li>
&lt;li>&lt;a href="#motivation">Motivation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#category">Category&lt;/a>&lt;/li>
&lt;li>&lt;a href="#context">Context&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributions">Contributions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#second-pass">Second Pass&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#background-work">Background Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#clarity">Clarity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#relevant-work">Relevant Work&lt;/a>&lt;/li>
&lt;li>&lt;a href="#methodology">Methodology&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-assumptions">Author Assumptions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#correctness">Correctness&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#future-directions">Future Directions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#open-questions">Open Questions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#author-feedback">Author Feedback&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summarization-technique">Summarization Technique&lt;/a>&lt;/li>
&lt;li>&lt;a href="#citations">Citations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="first-pass">First Pass&lt;/h2>
&lt;blockquote>
&lt;p>Read the title, abstract, introduction, section and sub-section headings, and
conclusion&lt;/p>
&lt;/blockquote>
&lt;h3 id="problem">Problem&lt;/h3>
&lt;blockquote>
&lt;p>What is the problem addressed in the paper?&lt;/p>
&lt;/blockquote>
&lt;p>This paper aims to address the problem that one stage object detectors (i.e.
YOLO, SSD) face when trying to match the performance of SOTA two stage object
detectors which is class imbalance.&lt;/p>
&lt;h3 id="motivation">Motivation&lt;/h3>
&lt;blockquote>
&lt;p>Why should we care about this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Because it introduces a new loss function that addresses the issue of class
imbalance when training dense, one stage object detectors. Additionally, the
authors released an example model implementing this loss known as Detectron/
RetinaNet.&lt;/p>
&lt;h3 id="category">Category&lt;/h3>
&lt;blockquote>
&lt;p>What type of paper is this work?&lt;/p>
&lt;/blockquote>
&lt;p>This is an algorithms and CV object detection paper.&lt;/p>
&lt;h3 id="context">Context&lt;/h3>
&lt;blockquote>
&lt;p>What other &lt;em>types&lt;/em> of papers is the work related to?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is related to papers demonstrating or working on one stage object
detection models.&lt;/p>
&lt;h3 id="contributions">Contributions&lt;/h3>
&lt;blockquote>
&lt;p>What are the author&amp;rsquo;s main contributions?&lt;/p>
&lt;/blockquote>
&lt;p>The author&amp;rsquo;s main contribution is a new loss function aimed at training one
stage object detection models that reduces the problem of class imbalance
between identifying objects in the foreground and background. Furthermore, the
authors have released an example model that was trained on this loss function
known as Detectron/RetinaNet.&lt;/p>
&lt;hr>
&lt;h2 id="second-pass">Second Pass&lt;/h2>
&lt;blockquote>
&lt;p>A proper read through of the paper is required to answer this&lt;/p>
&lt;/blockquote>
&lt;h3 id="background-work">Background Work&lt;/h3>
&lt;blockquote>
&lt;p>What has been done prior to this paper?&lt;/p>
&lt;/blockquote>
&lt;p>Work has been done in developing classic object detectors, one and two stage
detectors, reducing class imbalance, and robust estimation techniques.&lt;/p>
&lt;h3 id="figures-diagrams-illustrations-and-graphs">Figures, Diagrams, Illustrations, and Graphs&lt;/h3>
&lt;blockquote>
&lt;p>Are the axes properly labeled? Are results shown with error bars, so that
conclusions are statistically significant?&lt;/p>
&lt;/blockquote>
&lt;p>The figures are clear and understandable&lt;/p>
&lt;h3 id="clarity">Clarity&lt;/h3>
&lt;blockquote>
&lt;p>Is the paper well written?&lt;/p>
&lt;/blockquote>
&lt;p>This paper is well written and is dense with technical information.&lt;/p>
&lt;h3 id="relevant-work">Relevant Work&lt;/h3>
&lt;blockquote>
&lt;p>Mark relevant work for review&lt;/p>
&lt;/blockquote>
&lt;p>The following relevant work can be found in the &lt;a href="#citations">Citations&lt;/a> section
of this article.&lt;/p>
&lt;ol start="2">
&lt;li>P. Doll ́ ar, Z. Tu, P. Perona, and S. Belongie. Integral channel features.
In BMVC, 2009.&lt;/li>
&lt;li>P. F. Felzenszwalb, R. B. Girshick, and D. McAllester. Cascade object
detection with deformable part models. In CVPR, 2010.&lt;/li>
&lt;li>T. Hastie, R. Tibshirani, and J. Friedman. The elements of statistical
learning. Springer series in statistics Springer, Berlin, 2008.&lt;/li>
&lt;li>W. Liu, D. Anguelov, D. Erhan, C. Szegedy, and S. Reed. SSD: Single shot
multibox detector. In ECCV, 2016.&lt;/li>
&lt;/ol>
&lt;h3 id="methodology">Methodology&lt;/h3>
&lt;blockquote>
&lt;p>What methodology did the author&amp;rsquo;s use to validate their contributions?&lt;/p>
&lt;/blockquote>
&lt;p>They compared their RetinaNet model against other SOTA object detectors on the
COCO dataset. Additionally, they compare the performance of models trained using
their Focal Loss and the Online Hard Example Mining (OHEM) technique.&lt;/p>
&lt;h3 id="author-assumptions">Author Assumptions&lt;/h3>
&lt;blockquote>
&lt;p>What assumptions does the author(s) make? Are they justified assumptions?&lt;/p>
&lt;/blockquote>
&lt;p>Their assumption is that one stage object detectors are the future.&lt;/p>
&lt;h4 id="correctness">Correctness&lt;/h4>
&lt;blockquote>
&lt;p>Do the assumptions seem valid?&lt;/p>
&lt;/blockquote>
&lt;p>While having more options as to what type of object detector to choose from (one
or two stage), it is important to keep in mind that inference speed, accuracy,
recall, other metrics, and domain need all play an important role in what model
is selected for a particular task.&lt;/p>
&lt;h3 id="future-directions">Future Directions&lt;/h3>
&lt;blockquote>
&lt;p>My own proposed future directions for the work&lt;/p>
&lt;/blockquote>
&lt;p>I&amp;rsquo;d like to implement Focal Loss in both a traditional YOLO network and a YOLO
network following the MobileNet architecture.&lt;/p>
&lt;h3 id="open-questions">Open Questions&lt;/h3>
&lt;blockquote>
&lt;p>What open questions do I have about the work?&lt;/p>
&lt;/blockquote>
&lt;p>Why was the COCO dataset chosen and not the ImageNet or Pascal VOC dataset for
training?&lt;/p>
&lt;h3 id="author-feedback">Author Feedback&lt;/h3>
&lt;blockquote>
&lt;p>What feedback would I give to the authors?&lt;/p>
&lt;/blockquote>
&lt;p>This is a good paper. I would say that the size of the network is certainly
larger than previous one stage object detectors such as YOLO. Could it be
possible to reduce the size of the network to be comparable to these smaller
networks while maintaining the accuracy or achieving a better accuracy?&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;blockquote>
&lt;p>A summary of the paper&lt;/p>
&lt;/blockquote>
&lt;p>The paper &lt;em>Focal Loss for Dense Object Detection&lt;/em> by Tsung-Yi Lin et al. [1]
describes a new loss function aimed at improving the performance of one shot
object detection models that rely on region proposals. The problem that one shot
object detection models face compared against traditional two stage models that
utilize region proposals and object detection is that of class imbalance. Class
imbalance is simply that the region proposal network detects too many regions
where an object might be. This affects the performance of the object detection
component of the model as it might infer that an object is in a location that it
isn&amp;rsquo;t.&lt;/p>
&lt;p>To reduce this error, the authors of this paper propose the Focal Loss function,
a loss function aimed at reducing class imbalance. The function is
&lt;code>FL(pt) = −(1 − pt)^γ log(pt)&lt;/code> where &lt;code>γ &amp;gt;= 0&lt;/code> They then trained a model
(RetinaNet) with this loss function on the COCO dataset and found that it
performed better than other on stage methods with respect to average precision.&lt;/p>
&lt;hr>
&lt;h2 id="summarization-technique">Summarization Technique&lt;/h2>
&lt;p>This paper was summarized using a modified technique proposed by S. Keshav in
his work &lt;em>How to Read a Paper&lt;/em> [0].&lt;/p>
&lt;h2 id="citations">Citations&lt;/h2>
Citations availible at https://nsynovic.dev/summaries/focal-loss-for-dense-object-detection/</description></item></channel></rss>